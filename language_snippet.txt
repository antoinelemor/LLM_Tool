    def _detect_and_validate_language(
        self,
        df: pd.DataFrame,
        column_mapping: Dict[str, Any],
        models_or_plan: List[Dict[str, Any]],
    ) -> Optional[Dict]:
        """Detect dominant languages and confirm model compatibility."""
        working_df, _ = self._maybe_limit_to_annotated(
            df,
            context_key="language_detection",
            display_label="language detection",
        )
        text_column = column_mapping['text']
        model_infos: List[Dict[str, Any]] = [
            entry["info"] if isinstance(entry, dict) and "info" in entry else entry
            for entry in (models_or_plan or [])
        ]
        model_language_map: Dict[str, List[str]] = {}
        language_to_models: Dict[str, List[Dict[str, Any]]] = {}
        for model in model_infos:
            model_key = model.get("relative_name") or str(
                model.get("label_value")
                or model.get("base_model")
                or model.get("path")
                or "model"
            )
            raw_langs = model.get("confirmed_languages") or [model.get("language")]
            normalized_langs = sorted(
                {
                    (LanguageNormalizer.normalize_language(lang) or str(lang).strip() or "UNKNOWN").upper()
                    for lang in raw_langs
                    if lang is not None
                }
            )
            if not normalized_langs:
                fallback_lang = (
                    LanguageNormalizer.normalize_language(model.get("language"))
                    or model.get("language")
                    or "UNKNOWN"
                )
                normalized_langs = [str(fallback_lang).upper()]
            model_language_map[model_key] = normalized_langs
            for lang in normalized_langs:
                language_to_models.setdefault(lang, []).append(model)

        self.console.print("\n[cyan]We will analyze all texts in your dataset to detect their language.[/cyan]")
        self.console.print("[cyan]If you already track language codes, you can reuse that column to avoid auto-detection.[/cyan]\n")

        candidate_language_columns: List[Dict[str, Any]] = []
        for col in working_df.columns:
            if col in {text_column, column_mapping.get('id')}:
                continue
            if working_df[col].dtype != 'object':
                continue
            counts = LanguageNormalizer.detect_languages_in_column(working_df, col)
            if counts:
                candidate_language_columns.append({'name': col, 'counts': counts})

        language_column: Optional[str] = None
        detection_source = "detector"
        language_counts: Dict[str, int] = {}

        if candidate_language_columns:
            lang_table = Table(title="Detected Language Columns", box=box.ROUNDED)
            lang_table.add_column("#", style="cyan", width=4)
            lang_table.add_column("Column", style="green", width=30)
            lang_table.add_column("Languages", style="magenta", overflow="fold")
            for idx, candidate in enumerate(candidate_language_columns, 1):
                lang_summary = ", ".join(
                    f"{lang.upper()} ({count})" for lang, count in candidate['counts'].items()
                )
                lang_table.add_row(str(idx), candidate['name'], lang_summary)
            self._print_table(lang_table)

            if Confirm.ask("Use one of these columns for language detection?", default=True):
                lang_choice = Prompt.ask(
                    "\n[cyan]Select language column[/cyan]",
                    choices=[str(i) for i in range(1, len(candidate_language_columns) + 1)],
                    default="1",
                )
                selected = candidate_language_columns[int(lang_choice) - 1]
                language_column = selected['name']
                language_counts = selected['counts']
                detection_source = "column"

        analysis_results = self._analyze_languages(working_df, text_column)
        if language_counts:
            normalized_counts: Dict[str, int] = {}
            for lang, count in language_counts.items():
                norm = LanguageNormalizer.normalize_language(lang) or lang
                normalized_counts[norm.lower()] = normalized_counts.get(norm.lower(), 0) + count
            analysis_results['languages_detected'] = normalized_counts
            language_counts = normalized_counts
        else:
            language_counts = analysis_results.get('languages_detected', {})

        if not language_counts:
            language_counts = {'en': 1}
            analysis_results['languages_detected'] = language_counts
            self.console.print("[yellow]âš  Unable to confidently detect language, defaulting to English.[/yellow]")

        self._present_language_analysis(analysis_results)

        language_counts_upper: Dict[str, int] = {}
        for lang, count in language_counts.items():
            normalized = LanguageNormalizer.normalize_language(lang) or lang
            key = normalized.upper()
            language_counts_upper[key] = language_counts_upper.get(key, 0) + count
        language_counts = language_counts_upper

        sorted_langs = sorted(language_counts.items(), key=lambda item: item[1], reverse=True)
        primary_lang = sorted_langs[0][0]
        unique_languages = set(language_counts.keys())

        if detection_source == "column" and language_column:
            column_mapping['language'] = language_column
            normalized_series = working_df[language_column].map(
                lambda val: (LanguageNormalizer.normalize_language(val) or str(val).strip() or "UNKNOWN").upper()
            )
            if len(normalized_series) == len(df):
                self._language_assignments = normalized_series
        else:
            column_mapping['language'] = '__detected_language__'

        languages_supported = set(itertools.chain.from_iterable(model_language_map.values()))
        has_multilingual = any(len(langs) > 1 for langs in model_language_map.values())
        model_lang_str = ", ".join(sorted(languages_supported)) if languages_supported else "â€”"

        languages_detected_sorted = sorted(unique_languages)
        languages_without_model = sorted(lang for lang in unique_languages if lang not in languages_supported)
        languages_to_annotate = sorted(lang for lang in unique_languages if lang in languages_supported)

        language_series = self._language_assignments
        if language_series is None or len(language_series) != len(df):
            language_series = self._get_or_compute_row_languages(
                df,
                column_mapping,
                {
                    'language_column': language_column,
                    'detection_source': detection_source,
                },
            )
        language_series = language_series.astype(str).str.upper()
        self._language_assignments = language_series

        language_mask = (
            language_series.isin(languages_to_annotate)
            if languages_to_annotate
            else pd.Series([True] * len(language_series), index=language_series.index)
        )
        eligible_count = int(language_mask.sum())
        skipped_count = len(language_series) - eligible_count

        if languages_detected_sorted:
            self.console.print("\n[bold]ðŸ“Š Language Distribution & Model Coverage[/bold]")
            self.console.print("[dim]Each model will only annotate rows in its supported language(s).[/dim]\n")

            for lang in languages_detected_sorted:
                lang_count = language_counts.get(lang.lower(), 0)
                models_for_lang = language_to_models.get(lang, [])
                if models_for_lang:
                    formatted_models: List[str] = []
                    for mdl in models_for_lang:
                        display_name = self._condense_relative_name(
                            mdl.get("relative_name")
                            or mdl.get("label_value")
                            or str(mdl.get("base_model") or "model")
                        )
                        per_metrics = mdl.get("metrics_per_language") or mdl.get("metrics", {}).get("per_language", {})
                        score = per_metrics.get(lang) if isinstance(per_metrics, dict) else None
                        if isinstance(score, (int, float)):
                            formatted_models.append(f"{display_name} (F1: {score:.3f})")
                        else:
                            formatted_models.append(display_name)
                    self.console.print(f"  [green]âœ“ {lang}[/green]: {lang_count:,} rows â†’ will be annotated")
                    self.console.print(f"    [dim]Model: {', '.join(formatted_models)}[/dim]")
                else:
                    self.console.print(f"  [yellow]âŠ˜ {lang}[/yellow]: {lang_count:,} rows â†’ will be skipped (no compatible model)")

        self.console.print()
        if skipped_count > 0:
            self.console.print(
                f"[bold cyan]Summary:[/bold cyan] {eligible_count:,} rows will be annotated, "
                f"{skipped_count:,} rows will be skipped (no compatible model)."
            )
            self.console.print(
                f"[dim]This is expected when your dataset contains multiple languages but your models only support some of them.[/dim]"
            )
        else:
            self.console.print(f"[green]âœ“ All {eligible_count:,} rows will be annotated (full language coverage).[/green]")

        if eligible_count == 0:
            self.console.print(
                "[red]âœ— No rows remain after applying language compatibility filters. "
                "Select another model or adjust your dataset.[/red]"
            )
            return None

        self._language_annotation_mask = language_mask
        self._allowed_annotation_languages = languages_to_annotate

        try:
            self._display_annotation_examples(
                df,
                column_mapping,
                language_series,
                language_mask,
                languages_to_annotate,
            )
        except Exception as exc:
            self.logger.debug("Unable to display annotation examples: %s", exc)

        # Show language compatibility summary
        if len(unique_languages) > 1:
            if has_multilingual:
                self.console.print(f"\n[green]âœ“ Dataset contains {len(unique_languages)} languages. Your model(s) support multiple languages.[/green]")
            else:
                covered_pct = (eligible_count / len(df)) * 100 if len(df) > 0 else 0
                self.console.print(f"\n[cyan]â„¹ Dataset contains {len(unique_languages)} languages (Primary: {primary_lang}).[/cyan]")
                self.console.print(f"[cyan]  Model language(s): {model_lang_str}[/cyan]")
                self.console.print(f"[cyan]  Coverage: {covered_pct:.1f}% of rows will be annotated ({eligible_count:,}/{len(df):,}).[/cyan]")

                if primary_lang not in languages_supported:
                    self.console.print(
                        f"\n[yellow]âš  Note: Your primary language ({primary_lang}) doesn't match the model language(s) ({model_lang_str}).[/yellow]"
                    )
                    self.console.print(
                        f"[yellow]  Only rows in {model_lang_str} will receive annotations. This is normal for multilingual datasets.[/yellow]"
                    )
                    if language_column and len(unique_languages) > 1:
                        self.console.print(
                            f"[dim]  Tip: You can filter your dataset by the '{language_column}' column before annotation if you prefer.[/dim]"
                        )
                    if not Confirm.ask("\nProceed with annotation?", default=True):
                        return None
        else:
            # Single language case
            is_compatible = primary_lang in languages_supported or has_multilingual
            if is_compatible:
                self.console.print(f"\n[green]âœ“ Language compatibility confirmed ({primary_lang}). All rows will be annotated.[/green]")
            else:
                self.console.print(
                    f"\n[red]âœ— Language mismatch: Dataset is in {primary_lang}, but model only supports {model_lang_str}.[/red]"
                )
                if not Confirm.ask("Proceed anyway?", default=False):
                    return None

        if column_mapping.get('language') == '__detected_language__':
            try:
                df['__detected_language__'] = language_series.reindex(df.index)
            except Exception:
                df['__detected_language__'] = language_series.values

        language_info = {
            'primary_language': primary_lang,
            'languages': languages_detected_sorted,
            'counts': {lang: int(language_counts.get(lang, 0)) for lang in languages_detected_sorted},
            'language_column': language_column,
            'detection_source': detection_source,
            'model_languages': model_language_map,
            'language_to_models': {
                lang: [
                    mdl.get("relative_name")
                    or str(mdl.get("label_value") or mdl.get("base_model") or "model")
                    for mdl in language_to_models.get(lang, [])
                ]
                for lang in languages_detected_sorted
            },
            'languages_supported': sorted(languages_supported),
            'languages_to_annotate': languages_to_annotate,
            'languages_without_model': languages_without_model,
            'eligible_row_count': eligible_count,
            'skipped_row_count': skipped_count,
            'total_row_count': int(len(df)),
        }
        return language_info

