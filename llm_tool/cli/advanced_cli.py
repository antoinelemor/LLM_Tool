#!/usr/bin/env python3
"""
PROJECT:
-------
LLMTool

TITLE:
------
advanced_cli.py

MAIN OBJECTIVE:
---------------
This script provides a sophisticated, professional-grade Command Line Interface
for the LLMTool package with auto-detection, intelligent suggestions, and
guided interactive workflows inspired by state-of-the-art CLI design patterns.

Dependencies:
-------------
- sys
- os
- subprocess
- pathlib
- rich (optional but highly recommended)
- inquirer
- pandas
- psutil

MAIN FEATURES:
--------------
1) Auto-detection of available models (Ollama, API models, local files)
2) Intelligent suggestions based on context and history
3) Interactive guided wizards for complex workflows
4) Professional validation with helpful error recovery
5) Rich visual interface with graceful fallback
6) Configuration profiles and execution history
7) Real-time progress tracking with detailed statistics
8) Smart defaults based on detected environment

Author:
-------
Antoine Lemor
"""

import sys
import os
import subprocess
import contextlib
import json
import time
import logging
import glob
import shutil
import numpy as np
from pathlib import Path
from typing import Optional, Dict, Any, List, Tuple, Set
from dataclasses import dataclass, field
from datetime import datetime
from collections import defaultdict, Counter
import re

# Rich is mandatory for this CLI
HAS_RICH = False
try:
    from rich.console import Console
    from rich.panel import Panel
    from rich.table import Table
    from rich.prompt import Prompt, Confirm, IntPrompt, FloatPrompt
    from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn, TimeElapsedColumn
    from rich.text import Text
    from rich.tree import Tree
    from rich.layout import Layout
    from rich.live import Live
    from rich.columns import Columns
    from rich.syntax import Syntax
    from rich.markdown import Markdown
    from rich import print as rprint
    from rich import box
    console = Console()
    HAS_RICH = True
except ImportError as e:
    print("\n❌ Error: Rich library is required but not installed.")

# Requests is optional (only needed for Label Studio direct export)
HAS_REQUESTS = False
try:
    import requests
    HAS_REQUESTS = True
except ImportError:
    pass  # Will be handled gracefully when needed

# Try importing pandas for data preview
try:
    import pandas as pd
    HAS_PANDAS = True
except ImportError:
    HAS_PANDAS = False

# Try importing numpy for numerical operations
try:
    import numpy as np
    HAS_NUMPY = True
except ImportError:
    HAS_NUMPY = False

# Try importing psutil for system monitoring
try:
    import psutil
    HAS_PSUTIL = True
except ImportError:
    HAS_PSUTIL = False

# Try importing transformers for tokenizer
try:
    from transformers import AutoTokenizer
    HAS_TRANSFORMERS = True
except ImportError:
    HAS_TRANSFORMERS = False

# Try importing tqdm for progress bars
try:
    from tqdm import tqdm
    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False

# Import internal modules
from ..config.settings import Settings
from ..pipelines.pipeline_controller import PipelineController
from ..utils.language_detector import LanguageDetector
from ..utils.data_filter_logger import get_filter_logger
from ..utils.system_resources import detect_resources, SystemResourceDetector
from ..utils.resource_display import (
    display_resources,
    create_resource_table,
    create_recommendations_table,
    create_compact_resource_panel,
    display_resource_header,
    get_resource_summary_text,
    create_visual_resource_panel,
    create_mode_resource_banner,
    create_detailed_mode_panel
)
from ..annotators.json_cleaner import extract_expected_keys
from ..annotators.prompt_wizard import SocialSciencePromptWizard, create_llm_client_for_wizard
from ..trainers.model_trainer import ModelTrainer, BenchmarkConfig
from ..trainers.multi_label_trainer import (
    MultiLabelTrainer,
    TrainingConfig as MultiLabelTrainingConfig,
    ModelInfo as MultiLabelModelInfo,
)
from ..trainers.training_data_builder import (
    TrainingDatasetBuilder,
    TrainingDataRequest,
    TrainingDataBundle,
)


# ============================================================================
# MODEL DESCRIPTIONS - Factual information about popular LLM models
# ============================================================================
MODEL_DESCRIPTIONS = {
    # Meta Llama models
    'llama3.3': 'Meta Llama 3.3 70B - Multilingual model with 128K context, strong reasoning',
    'llama3.2': 'Meta Llama 3.2 - Small efficient models (1B/3B) for edge deployment',
    'llama3.1': 'Meta Llama 3.1 - Multimodal model with tool use, 128K context (8B/70B/405B)',
    'llama3': 'Meta Llama 3 - Improved instruction following, 8K context (8B/70B)',
    'llama2': 'Meta Llama 2 - Chat-optimized open model, 4K context (7B/13B/70B)',
    'codellama': 'Meta Code Llama - Code generation specialist, supports Python/C++/Java (7B/13B/34B)',

    # Google Gemma models
    'gemma3': 'Google Gemma 3 - Latest efficient model for everyday devices (2B/9B/27B)',
    'gemma2': 'Google Gemma 2 - Lightweight deployment across consumer devices (2B/9B/27B)',
    'gemma': 'Google Gemma - Efficient open model by Google DeepMind (2B/7B)',

    # Alibaba Qwen models
    'qwen3': 'Qwen 3 - Latest generation with dense and MoE variants (8B-235B), 128K context',
    'qwen2.5': 'Qwen 2.5 - Pretrained on 18T tokens, multilingual support, 128K context',
    'qwen2': 'Qwen 2 - Multilingual model with strong coding abilities (0.5B-72B)',
    'qwen': 'Qwen - Alibaba large language model series',

    # DeepSeek models
    'deepseek-r1': 'DeepSeek-R1 - Reasoning model, strong in math/coding/logic (8B/671B)',
    'deepseek-coder': 'DeepSeek Coder - Specialized coding model with 16K context',
    'deepseek': 'DeepSeek - General purpose model series',

    # Mistral AI models
    'mixtral': 'Mistral Mixtral - Mixture of Experts (MoE) model with open weights (8x7B/8x22B)',
    'mistral': 'Mistral 7B - Efficient 7B model, approaches CodeLlama on code tasks',
    'mistral-nemo': 'Mistral Nemo - 12B model with 128K context window',
    'codestral': 'Codestral - Mistral AI code generation model, supports 80+ languages',

    # NVIDIA Nemotron models
    'nemotron': 'NVIDIA Nemotron-70B - Customized Llama 3.1 for helpful responses via RLHF',
    'nemotron-mini': 'NVIDIA Nemotron Mini - Small language model for RAG/function calling, 4K context',

    # Microsoft Phi models
    'phi4': 'Microsoft Phi-4 - 14B reasoning model rivaling larger models',
    'phi3': 'Microsoft Phi-3 - Lightweight state-of-the-art models (3B Mini/14B Medium)',
    'phi': 'Microsoft Phi-2 - 2.7B model with strong reasoning/language understanding',

    # Cohere models
    'command-r': 'Cohere Command R - Optimized for conversational interaction and long context',
    'command-r-plus': 'Cohere Command R+ - Enhanced version with stronger capabilities',

    # Specialized models
    'yi': 'Yi - Bilingual (English/Chinese) model with strong performance (6B/34B)',
    'solar': 'Solar - Upstage Solar, depth-upscaled Llama 2 with 10.7B parameters',
    'orca': 'Orca - Microsoft Orca, reasoning specialist trained on GPT-4 outputs',
    'vicuna': 'Vicuna - Open-source chatbot fine-tuned from LLaMA',
    'wizardcoder': 'WizardCoder - Code generation with Evol-Instruct method',
    'starcoder': 'StarCoder - Code generation model trained on The Stack dataset',
    'falcon': 'Falcon - TII open-source model, trained on refined web data (7B/40B)',
    'stable-lm': 'StabilityAI Stable LM - Efficient language model series',
    'bloom': 'BLOOM - Multilingual model supporting 46 languages (176B)',
    'gpt4all': 'GPT4All - Ecosystem of open-source chatbots',

    # OpenAI models
    'gpt-5': 'OpenAI GPT-5 - Latest flagship model (2025)',
    'gpt-4o': 'OpenAI GPT-4o - Multimodal (text/image), matches GPT-4 Turbo performance',
    'gpt-4-turbo': 'OpenAI GPT-4 Turbo - Large multimodal model, optimized for chat/completions',
    'gpt-4': 'OpenAI GPT-4 - Advanced reasoning, multimodal capabilities',
    'gpt-3.5-turbo': 'OpenAI GPT-3.5 Turbo - Fast, cost-effective for most tasks',
    'o1': 'OpenAI o1 - Reasoning model for science/coding/math',
    'o3': 'OpenAI o3 - Latest reasoning model with enhanced performance',
    'o4': 'OpenAI o4 - Advanced reasoning model (2025)',

    # Anthropic Claude models
    'claude-sonnet-4.5': 'Claude Sonnet 4.5 - Best for coding/computer use, autonomous for 30hrs (200K)',
    'claude-3.7-sonnet': 'Claude 3.7 Sonnet - Hybrid reasoning, extended thinking for complex problems',
    'claude-3.5-sonnet': 'Claude 3.5 Sonnet - Strong vision, 64% agentic coding eval (200K)',
    'claude-opus-4.1': 'Claude Opus 4.1 - Most intelligent, frontier in coding/search/writing (200K)',
    'claude-3-opus': 'Claude 3 Opus - Largest Claude 3 model, 200K-1M context window',
    'claude-3.5-haiku': 'Claude 3.5 Haiku - Fast, surpasses Claude 3 Opus on benchmarks',
    'claude-3-sonnet': 'Claude 3 Sonnet - Balanced performance and speed',
    'claude-3-haiku': 'Claude 3 Haiku - Fastest Claude 3 model',
}


@dataclass
class ModelInfo:
    """Information about an available model"""
    name: str
    provider: str  # ollama, openai, anthropic, local
    size: Optional[str] = None
    quantization: Optional[str] = None
    context_length: Optional[int] = None
    is_available: bool = True
    requires_api_key: bool = False
    supports_json: bool = True
    supports_streaming: bool = True
    max_tokens: Optional[int] = None
    cost_per_1k_tokens: Optional[float] = None


@dataclass
class DatasetInfo:
    """Information about a dataset"""
    path: Path
    format: str
    rows: Optional[int] = None
    columns: List[str] = field(default_factory=list)
    size_mb: Optional[float] = None
    detected_language: Optional[str] = None
    has_labels: bool = False
    label_column: Optional[str] = None
    column_types: Dict[str, str] = field(default_factory=dict)
    text_scores: Dict[str, float] = field(default_factory=dict)


@dataclass
class ExecutionProfile:
    """Saved execution profile for quick re-runs"""
    name: str
    created_at: datetime
    last_used: datetime
    configuration: Dict[str, Any]
    performance_metrics: Dict[str, Any] = field(default_factory=dict)
    notes: str = ""


class LLMDetector:
    """Auto-detect available LLMs for annotation from various sources"""

    @staticmethod
    def detect_ollama_models() -> List[ModelInfo]:
        """Detect locally available Ollama LLMs for annotation"""
        models = []
        try:
            result = subprocess.run(
                ["ollama", "list"],
                capture_output=True,
                text=True,
                timeout=5
            )
            if result.returncode == 0:
                lines = result.stdout.strip().split('\n')
                if len(lines) > 1:  # Has header and content
                    # Parse the table format from ollama list
                    # Format: NAME                    ID              SIZE      MODIFIED
                    for line in lines[1:]:  # Skip header
                        if line.strip():
                            parts = line.split()
                            if len(parts) >= 1:
                                name = parts[0]

                                # Extract size - look for GB/MB/KB in any part
                                size = None
                                for part in parts[1:]:
                                    part_upper = part.upper()
                                    if 'GB' in part_upper or 'MB' in part_upper or 'KB' in part_upper:
                                        # Format nicely: "27GB" -> "27 GB", "1.5GB" -> "1.5 GB"
                                        import re
                                        match = re.match(r'([\d.]+)\s*([KMGT]B)', part_upper)
                                        if match:
                                            size = f"{match.group(1)} {match.group(2)}"
                                        else:
                                            size = part
                                        break

                                models.append(ModelInfo(
                                    name=name,
                                    provider="ollama",
                                    size=size,
                                    is_available=True,
                                    supports_json=True,
                                    supports_streaming=True,
                                    context_length=LLMDetector._estimate_context_length(name),
                                    max_tokens=LLMDetector._suggest_local_max_tokens(name)
                                ))
        except (subprocess.SubprocessError, FileNotFoundError):
            pass
        return models

    @staticmethod
    def _estimate_context_length(model_name: str) -> int:
        """Estimate context length based on model name"""
        name_lower = model_name.lower()
        if 'llama3' in name_lower or 'llama-3' in name_lower:
            return 8192
        elif 'mixtral' in name_lower:
            return 32768
        elif 'gemma2' in name_lower:
            return 8192
        elif 'gemma3' in name_lower:
            return 8192
        elif 'qwen' in name_lower:
            return 32768
        elif 'phi' in name_lower:
            return 4096
        elif 'mistral' in name_lower:
            return 8192
        else:
            return 4096  # Default

    @staticmethod
    def _suggest_local_max_tokens(model_name: str) -> int:
        """Heuristic for local model generation budget"""
        context = LLMDetector._estimate_context_length(model_name)
        # Keep generous default while leaving room for prompt context
        return max(512, min(2048, context // 2))

    @staticmethod
    def detect_openai_models() -> List[ModelInfo]:
        """List available OpenAI models"""
        models = [
            # ✅ Tested models (fully supported in pipeline)
            ModelInfo("gpt-5-nano-2025-08-07", "openai", context_length=200000, requires_api_key=True,
                     cost_per_1k_tokens=0.001, supports_json=True, supports_streaming=True, max_tokens=4000),
            ModelInfo("gpt-5-mini-2025-08-07", "openai", context_length=200000, requires_api_key=True,
                     cost_per_1k_tokens=0.001, supports_json=True, supports_streaming=True, max_tokens=4000),
        ]
        return models

    @staticmethod
    def detect_anthropic_models() -> List[ModelInfo]:
        """List available Anthropic models"""
        models = [
            # ⚠️ Not yet tested in pipeline
        ]
        return models

    @staticmethod
    def detect_all_llms() -> Dict[str, List[ModelInfo]]:
        """Detect all available LLMs for annotation from all sources"""
        return {
            "local": LLMDetector.detect_ollama_models(),
            "openai": LLMDetector.detect_openai_models(),
            "anthropic": LLMDetector.detect_anthropic_models(),
        }


class TrainerModelDetector:
    """Detect available models for training (BERT variants, etc.)"""

    @staticmethod
    def get_available_models() -> Dict[str, List[Dict[str, Any]]]:
        """Get all available models for training organized by category"""
        return {
            "Multilingual Models": [
                {"name": "xlm-roberta-base", "params": "278M", "type": "XLM-R", "languages": "100+", "performance": "★★★★"},
                {"name": "xlm-roberta-large", "params": "560M", "type": "XLM-R", "languages": "100+", "performance": "★★★★★"},
                {"name": "microsoft/mdeberta-v3-base", "params": "280M", "type": "mDeBERTa", "languages": "100+", "performance": "★★★★★"},
                {"name": "bert-base-multilingual-cased", "params": "177M", "type": "mBERT", "languages": "104", "performance": "★★★"},
            ],
            "Long Document Models (Multilingual)": [
                {"name": "markussagen/xlm-roberta-longformer-base-4096", "params": "278M", "type": "XLM-R Longformer", "max_length": "4096", "languages": "100+", "performance": "★★★★★"},
                {"name": "allenai/led-base-16384", "params": "406M", "type": "LED", "max_length": "16384", "language": "English", "performance": "★★★★★"},
                {"name": "allenai/led-large-16384", "params": "406M", "type": "LED", "max_length": "16384", "language": "English", "performance": "★★★★★"},
                {"name": "allenai/longformer-base-4096", "params": "149M", "type": "Longformer", "max_length": "4096", "language": "English", "performance": "★★★★"},
                {"name": "allenai/longformer-large-4096", "params": "435M", "type": "Longformer", "max_length": "4096", "language": "English", "performance": "★★★★★"},
                {"name": "google/bigbird-roberta-base", "params": "128M", "type": "BigBird", "max_length": "4096", "language": "English", "performance": "★★★★"},
                {"name": "google/bigbird-roberta-large", "params": "340M", "type": "BigBird", "max_length": "4096", "language": "English", "performance": "★★★★★"},
            ],
            "Long Document Models (Language-Specific)": [
                # French
                {"name": "cmarkea/distilcamembert-base-nli", "params": "68M", "type": "DistilCamemBERT", "max_length": "512", "language": "French", "performance": "★★★★"},
                {"name": "gilf/french-camembert-postag-model", "params": "110M", "type": "CamemBERT", "max_length": "512", "language": "French", "performance": "★★★★"},
                # Spanish
                {"name": "PlanTL-GOB-ES/roberta-base-bne", "params": "125M", "type": "RoBERTa-BNE", "max_length": "512", "language": "Spanish", "performance": "★★★★"},
                {"name": "dccuchile/bert-base-spanish-wwm-cased", "params": "110M", "type": "BETO", "max_length": "512", "language": "Spanish", "performance": "★★★★"},
                # German
                {"name": "deepset/gbert-base", "params": "110M", "type": "GBERT", "max_length": "512", "language": "German", "performance": "★★★★"},
                {"name": "bert-base-german-cased", "params": "110M", "type": "German BERT", "max_length": "512", "language": "German", "performance": "★★★★"},
                # Italian
                {"name": "dbmdz/bert-base-italian-cased", "params": "110M", "type": "Italian BERT", "max_length": "512", "language": "Italian", "performance": "★★★★"},
                # Portuguese
                {"name": "neuralmind/bert-base-portuguese-cased", "params": "110M", "type": "BERTimbau", "max_length": "512", "language": "Portuguese", "performance": "★★★★"},
                # Dutch
                {"name": "GroNLP/bert-base-dutch-cased", "params": "110M", "type": "BERTje", "max_length": "512", "language": "Dutch", "performance": "★★★★"},
                {"name": "wietsedv/bert-base-dutch-cased", "params": "110M", "type": "Dutch BERT", "max_length": "512", "language": "Dutch", "performance": "★★★★"},
                # Polish
                {"name": "allegro/herbert-base-cased", "params": "124M", "type": "HerBERT", "max_length": "514", "language": "Polish", "performance": "★★★★"},
                # Chinese
                {"name": "hfl/chinese-roberta-wwm-ext", "params": "102M", "type": "Chinese RoBERTa", "max_length": "512", "language": "Chinese", "performance": "★★★★"},
                {"name": "bert-base-chinese", "params": "110M", "type": "Chinese BERT", "max_length": "512", "language": "Chinese", "performance": "★★★★"},
                # Japanese
                {"name": "cl-tohoku/bert-base-japanese-whole-word-masking", "params": "111M", "type": "Japanese BERT WWM", "max_length": "512", "language": "Japanese", "performance": "★★★★"},
                {"name": "cl-tohoku/bert-base-japanese", "params": "111M", "type": "Japanese BERT", "max_length": "512", "language": "Japanese", "performance": "★★★★"},
                # Arabic
                {"name": "aubmindlab/bert-base-arabert", "params": "135M", "type": "AraBERT", "max_length": "512", "language": "Arabic", "performance": "★★★★"},
                {"name": "asafaya/bert-base-arabic", "params": "110M", "type": "Arabic BERT", "max_length": "512", "language": "Arabic", "performance": "★★★★"},
                # Russian
                {"name": "DeepPavlov/rubert-base-cased", "params": "178M", "type": "RuBERT", "max_length": "512", "language": "Russian", "performance": "★★★★"},
            ],
            "Efficient Models": [
                {"name": "distilbert-base", "params": "66M", "type": "DistilBERT", "speed": "2x faster", "performance": "★★★"},
                {"name": "distilroberta-base", "params": "82M", "type": "DistilRoBERTa", "speed": "2x faster", "performance": "★★★"},
                {"name": "albert-base-v2", "params": "12M", "type": "ALBERT", "speed": "4x faster", "performance": "★★★"},
                {"name": "albert-large-v2", "params": "18M", "type": "ALBERT", "speed": "3x faster", "performance": "★★★★"},
                {"name": "deberta-v3-xsmall", "params": "22M", "type": "DeBERTa", "speed": "5x faster", "performance": "★★★"},
                {"name": "electra-small", "params": "14M", "type": "ELECTRA", "speed": "4x faster", "performance": "★★★"},
            ],
            "English Models": [
                {"name": "bert-base-uncased", "params": "110M", "type": "BERT", "performance": "★★★"},
                {"name": "bert-large-uncased", "params": "340M", "type": "BERT", "performance": "★★★★"},
                {"name": "roberta-base", "params": "125M", "type": "RoBERTa", "performance": "★★★★"},
                {"name": "roberta-large", "params": "355M", "type": "RoBERTa", "performance": "★★★★★"},
                {"name": "deberta-v3-base", "params": "184M", "type": "DeBERTa", "performance": "★★★★★"},
                {"name": "deberta-v3-large", "params": "435M", "type": "DeBERTa", "performance": "★★★★★"},
                {"name": "electra-base", "params": "110M", "type": "ELECTRA", "performance": "★★★★"},
                {"name": "electra-large", "params": "335M", "type": "ELECTRA", "performance": "★★★★★"},
                {"name": "albert-xlarge-v2", "params": "60M", "type": "ALBERT", "performance": "★★★★"},
            ],
            "French Models": [
                {"name": "camembert-base", "params": "110M", "type": "CamemBERT", "performance": "★★★★"},
                {"name": "camembert-large", "params": "335M", "type": "CamemBERT", "performance": "★★★★★"},
                {"name": "camemberta-base", "params": "110M", "type": "CamemBERTa-v2", "performance": "★★★★"},
                {"name": "flaubert-base", "params": "137M", "type": "FlauBERT", "performance": "★★★★"},
                {"name": "flaubert-large", "params": "373M", "type": "FlauBERT", "performance": "★★★★★"},
                {"name": "distilcamembert", "params": "68M", "type": "DistilCamemBERT", "performance": "★★★"},
                {"name": "barthez", "params": "165M", "type": "BARThez", "performance": "★★★★"},
                {"name": "fralbert", "params": "12M", "type": "FrALBERT", "performance": "★★★"},
                {"name": "fr-electra", "params": "110M", "type": "FrELECTRA", "performance": "★★★★"},
            ],
            "Other Language Models": [
                {"name": "asafaya/bert-base-arabic", "params": "110M", "type": "AraBERT", "language": "Arabic", "performance": "★★★★"},
                {"name": "bert-base-chinese", "params": "110M", "type": "Chinese BERT", "language": "Chinese", "performance": "★★★★"},
                {"name": "bert-base-german-cased", "params": "110M", "type": "German BERT", "language": "German", "performance": "★★★★"},
                {"name": "ai4bharat/indic-bert", "params": "110M", "type": "Hindi BERT", "language": "Hindi", "performance": "★★★"},
                {"name": "dbmdz/bert-base-italian-cased", "params": "110M", "type": "Italian BERT", "language": "Italian", "performance": "★★★★"},
            ]
        }


class LanguageNormalizer:
    """Intelligent language normalization and mapping system"""

    # Comprehensive language mapping dictionary
    LANGUAGE_MAPPINGS = {
        'en': ['en', 'eng', 'english', 'anglais'],
        'fr': ['fr', 'fra', 'fre', 'french', 'français', 'francais'],
        'de': ['de', 'deu', 'ger', 'german', 'deutsch', 'allemand'],
        'es': ['es', 'spa', 'spanish', 'español', 'espagnol'],
        'it': ['it', 'ita', 'italian', 'italiano', 'italien'],
        'pt': ['pt', 'por', 'portuguese', 'português', 'portugais'],
        'nl': ['nl', 'nld', 'dut', 'dutch', 'nederlands', 'néerlandais'],
        'ru': ['ru', 'rus', 'russian', 'русский', 'russe'],
        'zh': ['zh', 'chi', 'zho', 'chinese', '中文', 'chinois'],
        'ja': ['ja', 'jpn', 'japanese', '日本語', 'japonais'],
        'ar': ['ar', 'ara', 'arabic', 'العربية', 'arabe'],
        'hi': ['hi', 'hin', 'hindi', 'हिन्दी'],
        'ko': ['ko', 'kor', 'korean', '한국어', 'coréen'],
        'pl': ['pl', 'pol', 'polish', 'polski', 'polonais'],
        'tr': ['tr', 'tur', 'turkish', 'türkçe', 'turc'],
        'sv': ['sv', 'swe', 'swedish', 'svenska', 'suédois'],
        'da': ['da', 'dan', 'danish', 'dansk', 'danois'],
        'no': ['no', 'nor', 'norwegian', 'norsk', 'norvégien'],
        'fi': ['fi', 'fin', 'finnish', 'suomi', 'finnois'],
        'cs': ['cs', 'ces', 'cze', 'czech', 'čeština', 'tchèque'],
        'ro': ['ro', 'ron', 'rum', 'romanian', 'română', 'roumain'],
        'hu': ['hu', 'hun', 'hungarian', 'magyar', 'hongrois'],
        'el': ['el', 'ell', 'gre', 'greek', 'ελληνικά', 'grec'],
        'he': ['he', 'heb', 'hebrew', 'עברית', 'hébreu'],
        'th': ['th', 'tha', 'thai', 'ไทย', 'thaï'],
        'vi': ['vi', 'vie', 'vietnamese', 'tiếng việt', 'vietnamien'],
        'id': ['id', 'ind', 'indonesian', 'bahasa indonesia', 'indonésien'],
        'uk': ['uk', 'ukr', 'ukrainian', 'українська', 'ukrainien'],
    }

    # Reverse mapping for quick lookup
    _REVERSE_MAPPING = None

    @classmethod
    def _build_reverse_mapping(cls):
        """Build reverse mapping from variant to standard code"""
        if cls._REVERSE_MAPPING is None:
            cls._REVERSE_MAPPING = {}
            for standard_code, variants in cls.LANGUAGE_MAPPINGS.items():
                for variant in variants:
                    cls._REVERSE_MAPPING[variant.lower()] = standard_code

    @classmethod
    def normalize_language(cls, lang_value: str) -> Optional[str]:
        """Normalize a language value to standard 2-letter code"""
        if not lang_value:
            return None

        cls._build_reverse_mapping()
        lang_lower = str(lang_value).strip().lower()
        return cls._REVERSE_MAPPING.get(lang_lower)

    @staticmethod
    def detect_languages_in_column(df, column_name: str) -> Dict[str, int]:
        """Detect and count languages in a dataframe column"""
        if column_name not in df.columns:
            return {}

        lang_counts = {}
        for value in df[column_name].dropna():
            normalized = LanguageNormalizer.normalize_language(value)
            if normalized:
                lang_counts[normalized] = lang_counts.get(normalized, 0) + 1

        return lang_counts

    @staticmethod
    def recommend_models(languages: Set[str], all_models: Dict[str, List[Dict]]) -> List[Dict[str, Any]]:
        """Recommend training models based on detected languages - supports 11+ languages"""
        recommendations = []

        # Language-specific model mappings (comprehensive list)
        LANGUAGE_SPECIFIC_MODELS = {
            'en': ('English Models', ['bert-base-uncased', 'roberta-base', 'deberta-v3-base']),
            'fr': ('French Models', ['camembert-base', 'flaubert-base', 'distilcamembert']),
            'es': ('Spanish Models', ['dccuchile/bert-base-spanish-wwm-cased', 'PlanTL-GOB-ES/roberta-base-bne']),
            'de': ('German Models', ['bert-base-german-cased', 'deepset/gbert-base']),
            'it': ('Italian Models', ['dbmdz/bert-base-italian-cased', 'idb-ita/gilberto-uncased-from-camembert']),
            'pt': ('Portuguese Models', ['neuralmind/bert-base-portuguese-cased', 'portuguese-bert-base']),
            'nl': ('Dutch Models', ['GroNLP/bert-base-dutch-cased', 'wietsedv/bert-base-dutch-cased']),
            'ru': ('Russian Models', ['DeepPavlov/rubert-base-cased', 'sberbank-ai/ruBert-base']),
            'zh': ('Chinese Models', ['bert-base-chinese', 'hfl/chinese-bert-wwm-ext']),
            'ja': ('Japanese Models', ['cl-tohoku/bert-base-japanese', 'nlp-waseda/roberta-base-japanese']),
            'ar': ('Arabic Models', ['asafaya/bert-base-arabic', 'CAMeL-Lab/bert-base-arabic-camelbert-ca']),
        }

        if not languages:
            # No language info - recommend multilingual as safe default
            recommendations.append({
                'model': 'xlm-roberta-base',
                'category': 'Multilingual Models',
                'reason': 'No language detected - multilingual model as safe default',
                'priority': 3
            })
            return recommendations

        # Single language recommendations
        if len(languages) == 1:
            lang = list(languages)[0]

            if lang in LANGUAGE_SPECIFIC_MODELS:
                category, model_names = LANGUAGE_SPECIFIC_MODELS[lang]

                # Add language-specific models from all_models if available
                if category in all_models:
                    for model in all_models[category]:
                        recommendations.append({
                            'model': model['name'],
                            'category': category,
                            'reason': f"Optimized for {lang.upper()} ({model.get('performance', 'N/A')} performance)",
                            'priority': 1,
                            'details': model
                        })
                else:
                    # Fallback: use hardcoded models
                    for model_name in model_names[:3]:  # Top 3 models
                        recommendations.append({
                            'model': model_name,
                            'category': category,
                            'reason': f"Specialized for {lang.upper()}",
                            'priority': 1
                        })

                # Also suggest multilingual as fallback
                recommendations.append({
                    'model': 'xlm-roberta-base',
                    'category': 'Multilingual Models',
                    'reason': f'Multilingual fallback (supports {lang.upper()} + 100 languages)',
                    'priority': 2
                })

            else:
                # Language not in specific models - recommend multilingual
                for model in all_models.get('Multilingual Models', []):
                    recommendations.append({
                        'model': model['name'],
                        'category': 'Multilingual Models',
                        'reason': f"Supports {lang.upper()} + {model.get('languages', '100+')} languages",
                        'priority': 1,
                        'details': model
                    })

        # Multiple languages - strongly recommend multilingual
        else:
            lang_str = ', '.join([l.upper() for l in sorted(languages)])
            for model in all_models.get('Multilingual Models', []):
                recommendations.append({
                    'model': model['name'],
                    'category': 'Multilingual Models',
                    'reason': f"Handles multiple languages ({lang_str}) - {model.get('languages', '100+')} supported",
                    'priority': 1,
                    'details': model
                })

            # Also suggest separate models per language
            recommendations.append({
                'model': 'separate_per_language',
                'category': 'Multi-Model Strategy',
                'reason': f"Train separate specialized models for each language ({lang_str})",
                'priority': 2
            })

        # Sort by priority
        recommendations.sort(key=lambda x: x['priority'])
        return recommendations


class DataDetector:
    """Auto-detect and analyze available datasets"""

    @staticmethod
    def scan_directory(directory: Path = Path.cwd()) -> List[DatasetInfo]:
        """Scan directory and subdirectories for potential datasets"""
        datasets = []

        # If directory doesn't exist, return empty list
        if not directory.exists():
            return datasets

        # Only include formats fully supported by the annotation pipeline
        patterns = [
            '**/*.csv',  # CSV - fully supported
            '**/*.json', '**/*.jsonl',  # JSON formats - fully supported
            '**/*.xlsx', '**/*.xls',  # Excel formats - fully supported
            '**/*.parquet',  # Parquet - fully supported
            '**/*.RData', '**/*.rdata',  # R format - fully supported (requires pyreadr)
        ]

        for pattern in patterns:
            for file_path in directory.glob(pattern):
                if file_path.is_file():
                    dataset_info = DataDetector.analyze_file(file_path)
                    if dataset_info:
                        datasets.append(dataset_info)

        return datasets

    @staticmethod
    def analyze_file(file_path: Path) -> Optional[DatasetInfo]:
        """Analyze a single file to extract dataset information"""
        if not file_path.exists():
            return None

        info = DatasetInfo(
            path=file_path,
            format=file_path.suffix[1:],
            size_mb=file_path.stat().st_size / (1024 * 1024)
        )

        # Try to read and analyze the file
        if HAS_PANDAS:
            try:
                # CSV format
                if info.format == 'csv':
                    df = pd.read_csv(file_path, nrows=100)

                # JSON formats
                elif info.format == 'json':
                    df = pd.read_json(file_path, lines=False, nrows=100)
                elif info.format == 'jsonl':
                    df = pd.read_json(file_path, lines=True, nrows=100)

                # Excel formats
                elif info.format in ['xlsx', 'xls']:
                    df = pd.read_excel(file_path, nrows=100)

                # Parquet format
                elif info.format == 'parquet':
                    df = pd.read_parquet(file_path).head(100)

                # R format
                elif info.format.lower() in ['rdata']:
                    try:
                        import pyreadr
                        result = pyreadr.read_r(str(file_path))
                        if result:
                            df = list(result.values())[0].head(100)
                        else:
                            return info
                    except ImportError:
                        # If pyreadr not available, return basic info without columns
                        return info

                else:
                    # Unsupported format - return basic info only
                    return info

                info.rows = len(df)
                info.columns = list(df.columns)
                info.column_types = {col: str(df[col].dtype) for col in df.columns}
                info.text_scores = {}

                for col in df.columns:
                    if pd.api.types.is_string_dtype(df[col]) or str(df[col].dtype) == 'object':
                        sample_series = df[col].dropna().astype(str)
                        if not sample_series.empty:
                            avg_len = float(sample_series.str.len().mean())
                            info.text_scores[col] = avg_len

                # Detect if there's a label column
                label_candidates = ['label', 'labels', 'class', 'category', 'target', 'y']
                for col in info.columns:
                    if col.lower() in label_candidates:
                        info.has_labels = True
                        info.label_column = col
                        break

            except Exception:
                pass

        return info

    @staticmethod
    def analyze_file_intelligently(file_path: Path) -> Dict[str, Any]:
        """
        Comprehensive intelligent analysis of any supported file format.
        Returns detailed information about columns, languages, annotations, etc.
        """
        result = {
            'file_path': file_path,
            'format': file_path.suffix[1:].lower(),
            'columns': [],
            'all_columns': [],  # Add this for complete column list
            'text_column_candidates': [],
            'annotation_column_candidates': [],
            'id_column_candidates': [],
            'language_column_candidates': [],
            'languages_detected': {},
            'has_valid_annotations': False,
            'annotation_stats': {},
            'row_count': 0,
            'issues': [],
            'sample_data': {},  # Add sample data for displaying examples
            'annotation_keys_found': set()  # For JSON annotations
        }

        if not HAS_PANDAS:
            result['issues'].append("pandas not available - limited analysis")
            return result

        try:
            # Read file based on format
            df = None
            file_format = result['format']

            if file_format == 'csv':
                df = pd.read_csv(file_path, nrows=1000)
            elif file_format == 'tsv':
                df = pd.read_csv(file_path, sep='\t', nrows=1000)
            elif file_format == 'json':
                df = pd.read_json(file_path, lines=False)
                if len(df) > 1000:
                    df = df.head(1000)
            elif file_format == 'jsonl':
                df = pd.read_json(file_path, lines=True, nrows=1000)
            elif file_format in ['xlsx', 'xls']:
                df = pd.read_excel(file_path, nrows=1000)
            elif file_format == 'parquet':
                df = pd.read_parquet(file_path)
                if len(df) > 1000:
                    df = df.head(1000)
            else:
                result['issues'].append(f"Unsupported format: {file_format}")
                return result

            if df is None or df.empty:
                result['issues'].append("File is empty or could not be read")
                return result

            result['row_count'] = len(df)
            result['columns'] = list(df.columns)
            result['all_columns'] = list(df.columns)  # Store complete list

            # Extract sample data for each column (first 3 non-null values)
            for col in df.columns:
                non_null_values = df[col].dropna().head(3).tolist()
                result['sample_data'][col] = non_null_values

            # For JSON annotations, detect keys within the annotation column
            for col in df.columns:
                col_lower = col.lower()
                if 'annotation' in col_lower or 'label' in col_lower:
                    # Try to parse first few entries as JSON to find keys
                    for idx in range(min(10, len(df))):
                        val = df[col].iloc[idx]
                        if pd.notna(val):
                            try:
                                if isinstance(val, dict):
                                    result['annotation_keys_found'].update(val.keys())
                                elif isinstance(val, str):
                                    parsed = json.loads(val)
                                    if isinstance(parsed, dict):
                                        result['annotation_keys_found'].update(parsed.keys())
                            except:
                                pass

            # Detect column candidates
            text_candidates = ['text', 'content', 'message', 'sentence', 'paragraph',
                             'document', 'body', 'description', 'comment', 'review']
            annotation_candidates = ['annotation', 'annotations', 'label', 'labels',
                                    'category', 'categories', 'class', 'classification']
            id_candidates = ['id', 'identifier', '_id', 'uuid', 'key']
            lang_candidates = ['lang', 'language', 'langue', 'idioma', 'sprache', 'lingua']

            for col in df.columns:
                col_lower = col.lower()

                # Check for text columns
                if any(candidate in col_lower for candidate in text_candidates):
                    if pd.api.types.is_string_dtype(df[col]) or df[col].dtype == 'object':
                        avg_len = df[col].dropna().astype(str).str.len().mean()
                        if avg_len > 20:  # Likely text if average length > 20
                            result['text_column_candidates'].append({
                                'name': col,
                                'avg_length': float(avg_len),
                                'match_type': 'name_pattern'
                            })

                # Check for annotation columns
                if any(candidate in col_lower for candidate in annotation_candidates):
                    # CRITICAL: Exclude ID columns even if they match annotation pattern
                    # e.g., 'llm_annotation_id' is NOT an annotation, it's an ID
                    is_id_column = (
                        col_lower.endswith('_id') or
                        col_lower.endswith('id') or
                        col_lower.startswith('id_') or
                        'identifier' in col_lower or
                        col_lower in ['id']
                    )

                    if is_id_column:
                        # This is an ID column, not actual annotations - skip it
                        continue

                    # Check if this column contains JSON data (real annotations)
                    is_json_column = False
                    sample_values = df[col].dropna().head(5)
                    for val in sample_values:
                        if isinstance(val, str) and (val.strip().startswith('{') or val.strip().startswith('[')):
                            is_json_column = True
                            break
                        elif isinstance(val, dict) or isinstance(val, list):
                            is_json_column = True
                            break

                    # Check if annotations are valid (not empty)
                    non_empty = df[col].notna().sum()
                    empty = df[col].isna().sum()

                    result['annotation_column_candidates'].append({
                        'name': col,
                        'match_type': 'name_pattern',
                        'is_json': is_json_column,
                        'priority': 2 if is_json_column else 1  # JSON columns get higher priority
                    })

                    result['annotation_stats'][col] = {
                        'non_empty': int(non_empty),
                        'empty': int(empty),
                        'fill_rate': float(non_empty / len(df)) if len(df) > 0 else 0,
                        'is_json': is_json_column
                    }
                    if non_empty > 0:
                        result['has_valid_annotations'] = True

                # Check for ID columns
                if any(candidate in col_lower for candidate in id_candidates) or col_lower.endswith('_id') or col_lower.endswith('id'):
                    result['id_column_candidates'].append(col)

                # Check for language columns
                if any(candidate in col_lower for candidate in lang_candidates):
                    result['language_column_candidates'].append(col)
                    # Detect languages
                    lang_counts = LanguageNormalizer.detect_languages_in_column(df, col)
                    if lang_counts:
                        result['languages_detected'] = lang_counts

            # IMPROVED: Detect JSON annotation columns by content (even if name doesn't match patterns)
            # This catches columns like "gemma3" that contain JSON annotations
            detected_annotation_names = {c['name'] for c in result['annotation_column_candidates']}
            for col in df.columns:
                # Skip if already detected as annotation column
                if col in detected_annotation_names:
                    continue

                # Skip text columns (likely not annotations)
                if col in [c['name'] for c in result['text_column_candidates']]:
                    continue

                # Skip ID columns
                col_lower = col.lower()
                is_id_column = (
                    col_lower.endswith('_id') or
                    col_lower.endswith('id') or
                    col_lower.startswith('id_') or
                    'identifier' in col_lower or
                    col_lower in ['id']
                )
                if is_id_column:
                    continue

                # Check if this column contains JSON data
                is_json_column = False
                sample_values = df[col].dropna().head(10)  # Check more samples
                json_count = 0

                for val in sample_values:
                    if isinstance(val, str) and (val.strip().startswith('{') or val.strip().startswith('[')):
                        try:
                            json.loads(val)
                            json_count += 1
                        except:
                            pass
                    elif isinstance(val, dict) or isinstance(val, list):
                        json_count += 1

                # If majority of samples are JSON, this is likely an annotation column
                if json_count >= len(sample_values) * 0.7 and len(sample_values) > 0:  # 70% threshold
                    is_json_column = True

                    # Add as annotation candidate with high priority
                    non_empty = df[col].notna().sum()
                    empty = df[col].isna().sum()

                    result['annotation_column_candidates'].append({
                        'name': col,
                        'match_type': 'json_content',  # Different match type
                        'is_json': True,
                        'priority': 3  # Highest priority for JSON detected by content
                    })

                    result['annotation_stats'][col] = {
                        'non_empty': int(non_empty),
                        'empty': int(empty),
                        'fill_rate': float(non_empty / len(df)) if len(df) > 0 else 0,
                        'is_json': True
                    }
                    if non_empty > 0:
                        result['has_valid_annotations'] = True

            # If no text candidates found by name, find by heuristics
            if not result['text_column_candidates']:
                for col in df.columns:
                    if pd.api.types.is_string_dtype(df[col]) or df[col].dtype == 'object':
                        avg_len = df[col].dropna().astype(str).str.len().mean()
                        if avg_len > 50:  # Longer text
                            result['text_column_candidates'].append({
                                'name': col,
                                'avg_length': float(avg_len),
                                'match_type': 'heuristic'
                            })

            # Sort text candidates by length (longer is likely main text)
            result['text_column_candidates'].sort(key=lambda x: x['avg_length'], reverse=True)

            # Sort annotation candidates by priority (JSON columns first)
            result['annotation_column_candidates'].sort(key=lambda x: x.get('priority', 0), reverse=True)

            # Validation checks
            if not result['text_column_candidates']:
                result['issues'].append("⚠️  No text column detected - manual selection required")

            if result['annotation_column_candidates'] and not result['has_valid_annotations']:
                result['issues'].append("❌ Annotation columns found but they are EMPTY - cannot train!")

            if not result['language_column_candidates'] and len(result['text_column_candidates']) > 0:
                result['issues'].append("ℹ️  No language column detected - language detection can be applied")

        except Exception as e:
            result['issues'].append(f"Analysis error: {str(e)}")

        return result

    @staticmethod
    def suggest_text_column(dataset: DatasetInfo) -> Optional[str]:
        """Suggest the most likely text column from a dataset"""
        text_candidates = ['text', 'content', 'message', 'comment', 'review',
                          'description', 'body', 'document', 'sentence', 'paragraph']
        column_types = dataset.column_types or {}

        def is_probably_identifier(name: str) -> bool:
            name_lower = name.lower()
            if name_lower in {'id', 'identifier'}:
                return True
            if name_lower.endswith('_id') or name_lower.endswith('id'):
                return True
            return False

        def candidate_score(name: str) -> float:
            return dataset.text_scores.get(name, 0.0)

        # Prioritise columns whose dtype looks textual
        textual_columns = []
        for col in dataset.columns:
            dtype = column_types.get(col, '').lower()
            if 'object' in dtype or 'string' in dtype:
                textual_columns.append(col)
        if not textual_columns:
            textual_columns = list(dataset.columns)

        # Exact matches first
        exact_matches = [
            col for col in textual_columns
            if col.lower() in text_candidates and not is_probably_identifier(col)
        ]
        if exact_matches:
            exact_matches.sort(key=candidate_score, reverse=True)
            return exact_matches[0]

        # Partial matches (avoid *_id)
        partial_matches = []
        for col in textual_columns:
            col_lower = col.lower()
            if is_probably_identifier(col):
                continue
            for candidate in text_candidates:
                if candidate in col_lower:
                    partial_matches.append(col)
                    break

        if partial_matches:
            partial_matches.sort(key=candidate_score, reverse=True)
            return partial_matches[0]

        # Fall back to column with largest average length
        if dataset.text_scores:
            for col, _ in sorted(dataset.text_scores.items(), key=lambda item: item[1], reverse=True):
                if col in textual_columns and not is_probably_identifier(col):
                    return col

        # Final fallback: first non-identifier textual column
        for col in textual_columns:
            if not is_probably_identifier(col):
                return col

        return dataset.columns[0] if dataset.columns else None


class ProfileManager:
    """Manage execution profiles for quick re-runs"""

    def __init__(self, profile_dir: Path = None):
        self.profile_dir = profile_dir or Path.home() / '.llmtool' / 'profiles'
        self.profile_dir.mkdir(parents=True, exist_ok=True)
        self.history_file = self.profile_dir / 'history.json'

    def save_profile(self, profile: ExecutionProfile):
        """Save an execution profile"""
        profile_file = self.profile_dir / f"{profile.name}.json"
        data = {
            'name': profile.name,
            'created_at': profile.created_at.isoformat(),
            'last_used': profile.last_used.isoformat(),
            'configuration': profile.configuration,
            'performance_metrics': profile.performance_metrics,
            'notes': profile.notes
        }
        with open(profile_file, 'w') as f:
            json.dump(data, f, indent=2)

    def load_profile(self, name: str) -> Optional[ExecutionProfile]:
        """Load an execution profile by name"""
        profile_file = self.profile_dir / f"{name}.json"
        if not profile_file.exists():
            return None

        with open(profile_file, 'r') as f:
            data = json.load(f)

        return ExecutionProfile(
            name=data['name'],
            created_at=datetime.fromisoformat(data['created_at']),
            last_used=datetime.fromisoformat(data['last_used']),
            configuration=data['configuration'],
            performance_metrics=data.get('performance_metrics', {}),
            notes=data.get('notes', '')
        )

    def list_profiles(self) -> List[ExecutionProfile]:
        """List all available profiles"""
        profiles = []
        for profile_file in self.profile_dir.glob('*.json'):
            if profile_file.name != 'history.json':
                profile = self.load_profile(profile_file.stem)
                if profile:
                    profiles.append(profile)
        return sorted(profiles, key=lambda p: p.last_used, reverse=True)

    def save_to_history(self, config: Dict[str, Any]):
        """Save configuration to execution history"""
        history = []
        if self.history_file.exists():
            with open(self.history_file, 'r') as f:
                history = json.load(f)

        history.append({
            'timestamp': datetime.now().isoformat(),
            'configuration': config
        })

        # Keep only last 100 executions
        history = history[-100:]

        with open(self.history_file, 'w') as f:
            json.dump(history, f, indent=2)

    def get_recent_configs(self, limit: int = 5) -> List[Dict[str, Any]]:
        """Get recent configurations from history"""
        if not self.history_file.exists():
            return []

        with open(self.history_file, 'r') as f:
            history = json.load(f)

        return [h['configuration'] for h in history[-limit:]]


class AdvancedCLI:
    """Professional-grade CLI for LLMTool with sophisticated features"""

    def __init__(self):
        """Initialize the advanced CLI"""
        self.console = Console() if HAS_RICH else None
        self.settings = Settings()
        self.pipeline_controller = PipelineController()
        self.language_detector = LanguageDetector()
        self.llm_detector = LLMDetector()
        self.trainer_model_detector = TrainerModelDetector()
        self.data_detector = DataDetector()
        self.profile_manager = ProfileManager()
        self.resource_detector = SystemResourceDetector()
        self.system_resources = None  # Will be populated on first detection

        # Import and initialize PromptManager
        from ..annotators.prompt_manager import PromptManager
        self.prompt_manager = PromptManager()

        # Cache for detected models
        self.detected_llms: Optional[Dict[str, List[ModelInfo]]] = None
        self.available_trainer_models: Optional[Dict[str, List[Dict]]] = None
        self.detected_datasets: Optional[List[DatasetInfo]] = None

        # Session state
        self.current_session = {
            'start_time': datetime.now(),
            'operations_count': 0,
            'last_operation': None
        }

        # Setup logging
        self._setup_logging()

    def analyze_text_lengths(
        self,
        data_path: Path = None,
        df: Any = None,
        text_column: str = None,
        display_results: bool = True,
        step_label: str = "Text Length Analysis"
    ) -> Dict[str, Any]:
        """
        CRITICAL: Universal text length analysis method.

        This method MUST be used by ALL training modes:
        - Benchmark (single-label and multi-label)
        - Custom training
        - Model selector
        - Training studio
        - Quick training

        Args:
            data_path: Path to dataset file (CSV, JSON, JSONL, Excel, Parquet)
            df: Pre-loaded DataFrame (if already loaded)
            text_column: Name of text column to analyze
            display_results: Whether to display rich tables with results
            step_label: Label for display step (e.g., "Step 3b: Text Length Analysis")

        Returns:
            Dict with text length statistics and distribution
        """
        if display_results and self.console:
            self.console.print(f"\n[bold cyan]{step_label}[/bold cyan]\n")

        text_length_stats = {}
        requires_long_document_model = False

        try:
            # Verify required libraries are available
            if not HAS_PANDAS:
                raise ImportError("pandas not available")
            if not HAS_NUMPY:
                raise ImportError("numpy not available")

            # Import locally to avoid UnboundLocalError
            import pandas as pd
            import numpy as np

            # Load dataset if not provided
            if df is None and data_path is not None:
                if data_path.suffix == '.csv':
                    df = pd.read_csv(data_path)
                elif data_path.suffix == '.json':
                    df = pd.read_json(data_path)
                elif data_path.suffix == '.jsonl':
                    df = pd.read_json(data_path, lines=True)
                elif data_path.suffix in ['.xlsx', '.xls']:
                    df = pd.read_excel(data_path)
                elif data_path.suffix == '.parquet':
                    df = pd.read_parquet(data_path)

            if df is not None and text_column and text_column in df.columns:
                if display_results and self.console:
                    self.console.print("[dim]Analyzing text lengths for all documents...[/dim]\n")

                # Get all texts
                all_texts = df[text_column].dropna().astype(str).tolist()

                # Load tokenizer for accurate token counting
                try:
                    if not HAS_TRANSFORMERS:
                        raise ImportError("transformers library not available")

                    # Import tqdm locally to avoid UnboundLocalError
                    if HAS_TQDM:
                        from tqdm import tqdm

                    # Try to load tokenizer with fallback
                    tokenizer = None
                    tokenizer_models = [
                        "bert-base-multilingual-cased",  # Best for multilingual
                        "bert-base-uncased",              # Fallback
                        "distilbert-base-uncased"         # Lightweight fallback
                    ]

                    for model_name in tokenizer_models:
                        try:
                            tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=False)
                            break
                        except Exception as model_error:
                            self.logger.debug(f"Could not load {model_name}: {model_error}")
                            continue

                    if tokenizer is None:
                        raise RuntimeError("Could not load any tokenizer model")

                    # Analyze lengths in both characters and tokens
                    char_lengths = []
                    token_lengths = []

                    # Use tqdm only if available
                    text_iterator = tqdm(all_texts, desc="Measuring text lengths", disable=not HAS_TQDM) if HAS_TQDM else all_texts
                    for text in text_iterator:
                        char_lengths.append(len(text))
                        tokens = tokenizer.encode(text, truncation=False, add_special_tokens=True)
                        token_lengths.append(len(tokens))

                    char_lengths = np.array(char_lengths)
                    token_lengths = np.array(token_lengths)

                    # Calculate comprehensive statistics
                    text_length_stats = {
                        'char_min': int(np.min(char_lengths)),
                        'char_max': int(np.max(char_lengths)),
                        'char_mean': float(np.mean(char_lengths)),
                        'char_median': float(np.median(char_lengths)),
                        'char_std': float(np.std(char_lengths)),
                        'char_p25': float(np.percentile(char_lengths, 25)),
                        'char_p75': float(np.percentile(char_lengths, 75)),
                        'char_p95': float(np.percentile(char_lengths, 95)),
                        'token_min': int(np.min(token_lengths)),
                        'token_max': int(np.max(token_lengths)),
                        'token_mean': float(np.mean(token_lengths)),
                        'token_median': float(np.median(token_lengths)),
                        'token_std': float(np.std(token_lengths)),
                        'token_p25': float(np.percentile(token_lengths, 25)),
                        'token_p75': float(np.percentile(token_lengths, 75)),
                        'token_p95': float(np.percentile(token_lengths, 95)),
                        'avg_chars': float(np.mean(char_lengths)),  # For compatibility
                    }

                    # Classify documents by length
                    short_docs = np.sum(token_lengths < 128)
                    medium_docs = np.sum((token_lengths >= 128) & (token_lengths < 512))
                    long_docs = np.sum((token_lengths >= 512) & (token_lengths < 1024))
                    very_long_docs = np.sum(token_lengths >= 1024)
                    total_docs = len(token_lengths)

                    text_length_stats['distribution'] = {
                        'short': {'count': int(short_docs), 'percentage': float(short_docs / total_docs * 100)},
                        'medium': {'count': int(medium_docs), 'percentage': float(medium_docs / total_docs * 100)},
                        'long': {'count': int(long_docs), 'percentage': float(long_docs / total_docs * 100)},
                        'very_long': {'count': int(very_long_docs), 'percentage': float(very_long_docs / total_docs * 100)},
                    }

                    # Display results if requested
                    if display_results and self.console:
                        # Statistics table
                        stats_table = Table(title="Text Length Statistics", border_style="cyan", show_header=True, header_style="bold")
                        stats_table.add_column("Metric", style="cyan", width=20)
                        stats_table.add_column("Characters", style="yellow", justify="right", width=15)
                        stats_table.add_column("Tokens", style="green", justify="right", width=15)

                        stats_table.add_row("Minimum", f"{text_length_stats['char_min']:,}", f"{text_length_stats['token_min']:,}")
                        stats_table.add_row("25th Percentile", f"{text_length_stats['char_p25']:,.0f}", f"{text_length_stats['token_p25']:,.0f}")
                        stats_table.add_row("Median", f"{text_length_stats['char_median']:,.0f}", f"{text_length_stats['token_median']:,.0f}")
                        stats_table.add_row("Mean", f"{text_length_stats['char_mean']:,.0f}", f"{text_length_stats['token_mean']:,.0f}")
                        stats_table.add_row("75th Percentile", f"{text_length_stats['char_p75']:,.0f}", f"{text_length_stats['token_p75']:,.0f}")
                        stats_table.add_row("95th Percentile", f"{text_length_stats['char_p95']:,.0f}", f"{text_length_stats['token_p95']:,.0f}")
                        stats_table.add_row("Maximum", f"{text_length_stats['char_max']:,}", f"{text_length_stats['token_max']:,}")
                        stats_table.add_row("Std Deviation", f"{text_length_stats['char_std']:,.0f}", f"{text_length_stats['token_std']:,.0f}")

                        self.console.print(stats_table)

                        # Distribution table
                        self.console.print("\n")
                        dist_table = Table(title="Document Length Distribution", border_style="cyan", show_header=True, header_style="bold")
                        dist_table.add_column("Category", style="cyan", width=20)
                        dist_table.add_column("Token Range", style="white", width=20)
                        dist_table.add_column("Count", style="yellow", justify="right", width=12)
                        dist_table.add_column("Percentage", style="green", justify="right", width=12)

                        dist_table.add_row("Short", "< 128 tokens", f"{short_docs:,}", f"{short_docs / total_docs * 100:.1f}%")
                        dist_table.add_row("Medium", "128-511 tokens", f"{medium_docs:,}", f"{medium_docs / total_docs * 100:.1f}%")
                        dist_table.add_row("Long", "512-1023 tokens", f"{long_docs:,}", f"{long_docs / total_docs * 100:.1f}%",
                                         style="bold yellow" if long_docs > 0 else None)
                        dist_table.add_row("Very Long", "≥ 1024 tokens", f"{very_long_docs:,}", f"{very_long_docs / total_docs * 100:.1f}%",
                                         style="bold red" if very_long_docs > 0 else None)

                        self.console.print(dist_table)

                        # Long document warning
                        long_document_percentage = (long_docs + very_long_docs) / total_docs * 100

                        if long_document_percentage > 20:
                            requires_long_document_model = True
                            self.console.print(f"\n[bold yellow]⚠ Warning: {long_document_percentage:.1f}% of documents exceed 512 tokens[/bold yellow]")
                            self.console.print("[dim]Standard BERT models truncate at 512 tokens, which may lose important information.[/dim]")
                            self.console.print("[dim]Long-document models (Longformer, BigBird) can handle up to 4096 tokens.[/dim]\n")
                        else:
                            self.console.print(f"\n[green]✓ Most documents ({100 - long_document_percentage:.1f}%) fit within standard BERT limits (512 tokens)[/green]")

                    text_length_stats['requires_long_model'] = requires_long_document_model

                except Exception as tokenizer_error:
                    self.logger.debug(f"Tokenizer-based analysis failed: {tokenizer_error}")
                    if display_results and self.console:
                        self.console.print(f"[yellow]Could not load tokenizer for precise token counting[/yellow]")
                        self.console.print(f"[dim]Error: {str(tokenizer_error)}[/dim]")

                    # Fallback: character-based analysis only
                    char_lengths = [len(str(text)) for text in all_texts]
                    char_lengths = np.array(char_lengths)

                    text_length_stats = {
                        'char_min': int(np.min(char_lengths)),
                        'char_max': int(np.max(char_lengths)),
                        'char_mean': float(np.mean(char_lengths)),
                        'char_median': float(np.median(char_lengths)),
                        'char_p95': float(np.percentile(char_lengths, 95)),
                        'avg_chars': float(np.mean(char_lengths)),
                    }

                    if display_results and self.console:
                        self.console.print(f"[dim]Average text length: {text_length_stats['char_mean']:.0f} characters[/dim]")

        except Exception as e:
            self.logger.debug(f"Text length analysis failed: {e}")
            if display_results and self.console:
                self.console.print("[yellow]Could not perform text length analysis[/yellow]")
                self.console.print(f"[dim]Error: {str(e)}[/dim]")

        return text_length_stats

        # Setup logging
        self._setup_logging()

    def _setup_logging(self):
        """Configure professional logging"""
        # Use application logs subdirectory for general application logs
        log_dir = self.settings.paths.logs_dir / "application"
        log_dir.mkdir(parents=True, exist_ok=True)

        log_file = log_dir / f"llmtool_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"

        # Force reconfiguration by clearing existing handlers
        root_logger = logging.getLogger()
        for handler in root_logger.handlers[:]:
            root_logger.removeHandler(handler)

        # Configure logging: DEBUG to file, WARNING to console
        logging.basicConfig(
            level=logging.DEBUG,  # Capture everything
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, mode='a', encoding='utf-8')
            ],
            force=True
        )

        # Add console handler with WARNING level only (hides INFO and DEBUG)
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(logging.WARNING)
        console_handler.setFormatter(logging.Formatter('%(levelname)s: %(message)s'))
        root_logger.addHandler(console_handler)

        self.logger = logging.getLogger(__name__)

        # Store log file path for reference
        self.current_log_file = log_file

    def _int_prompt_with_validation(self, prompt: str, default: int = 1, min_value: int = None, max_value: int = None) -> int:
        """IntPrompt.ask with validation since min_value/max_value not supported in older Rich versions"""
        while True:
            try:
                value = IntPrompt.ask(prompt, default=default)
                if min_value is not None and value < min_value:
                    self.console.print(f"[red]Value must be at least {min_value}[/red]")
                    continue
                if max_value is not None and value > max_value:
                    self.console.print(f"[red]Value must be at most {max_value}[/red]")
                    continue
                return value
            except ValueError:
                self.console.print("[red]Please enter a valid number[/red]")

    def _float_prompt_with_validation(self, prompt: str, default: float, min_value: float = None, max_value: float = None) -> float:
        """Prompt for a floating point value with optional bounds."""
        while True:
            raw_value = Prompt.ask(prompt, default=f"{default}")
            try:
                value = float(raw_value)
            except ValueError:
                if HAS_RICH and self.console:
                    self.console.print("[red]Please enter a valid number[/red]")
                else:
                    print("Please enter a valid number")
                continue

            if min_value is not None and value < min_value:
                message = f"Value must be at least {min_value}"
                if HAS_RICH and self.console:
                    self.console.print(f"[red]{message}[/red]")
                else:
                    print(message)
                continue

            if max_value is not None and value > max_value:
                message = f"Value must be at most {max_value}"
                if HAS_RICH and self.console:
                    self.console.print(f"[red]{message}[/red]")
                else:
                    print(message)
                continue

            return value

    def _check_cuda_available(self) -> bool:
        """Check if CUDA is available for training"""
        try:
            import torch
            return torch.cuda.is_available()
        except:
            return False

    def display_banner(self):
        """Display professional welcome banner with system info"""
        if HAS_RICH and self.console:
            # Spectacular multicolor full-width ASCII art banner
            from rich.align import Align
            from rich.text import Text

            # Get terminal width
            width = self.console.width

            self.console.print()
            self.console.print("[on bright_blue]" + " " * width + "[/on bright_blue]")
            self.console.print("[on bright_magenta]" + " " * width + "[/on bright_magenta]")
            self.console.print()

            # Giant LLM TOOL text with each letter in different color
            self.console.print(Align.center("[bright_magenta]██╗     [bright_yellow]██╗     [bright_green]███╗   ███╗    [bright_cyan]████████╗ [bright_red]██████╗  [bright_blue]██████╗ [bright_white]██╗     "))
            self.console.print(Align.center("[bright_magenta]██║     [bright_yellow]██║     [bright_green]████╗ ████║    [bright_cyan]╚══██╔══╝[bright_red]██╔═══██╗[bright_blue]██╔═══██╗[bright_white]██║     "))
            self.console.print(Align.center("[bright_magenta]██║     [bright_yellow]██║     [bright_green]██╔████╔██║       [bright_cyan]██║   [bright_red]██║   ██║[bright_blue]██║   ██║[bright_white]██║     "))
            self.console.print(Align.center("[bright_magenta]██║     [bright_yellow]██║     [bright_green]██║╚██╔╝██║       [bright_cyan]██║   [bright_red]██║   ██║[bright_blue]██║   ██║[bright_white]██║     "))
            self.console.print(Align.center("[bright_magenta]███████╗[bright_yellow]███████╗[bright_green]██║ ╚═╝ ██║       [bright_cyan]██║   [bright_red]╚██████╔╝[bright_blue]╚██████╔╝[bright_white]███████╗"))
            self.console.print(Align.center("[bright_magenta]╚══════╝[bright_yellow]╚══════╝[bright_green]╚═╝     ╚═╝       [bright_cyan]╚═╝    [bright_red]╚═════╝  [bright_blue]╚═════╝ [bright_white]╚══════╝"))

            self.console.print()
            self.console.print(Align.center("[bold bright_yellow on blue]  🚀 LLM-powered Intelligent Annotation & Training Pipeline 🚀  [/bold bright_yellow on blue]"))
            self.console.print()

            # Colorful pipeline with emojis
            pipeline_text = Text()
            pipeline_text.append("📊 Data ", style="bold bright_yellow on black")
            pipeline_text.append("→ ", style="bold white")
            pipeline_text.append("🤖 LLM Annotation ", style="bold bright_green on black")
            pipeline_text.append("→ ", style="bold white")
            pipeline_text.append("🧹 Clean ", style="bold bright_cyan on black")
            pipeline_text.append("→ ", style="bold white")
            pipeline_text.append("🎯 Label ", style="bold bright_magenta on black")
            pipeline_text.append("→ ", style="bold white")
            pipeline_text.append("🧠 Train ", style="bold bright_red on black")
            pipeline_text.append("→ ", style="bold white")
            pipeline_text.append("📈 Deploy ", style="bold bright_blue on black")

            self.console.print(Align.center(pipeline_text))
            self.console.print()
            self.console.print("[on bright_magenta]" + " " * width + "[/on bright_magenta]")
            self.console.print("[on bright_blue]" + " " * width + "[/on bright_blue]")
            self.console.print()

            # Information table with system info
            info_table = Table(show_header=False, box=None, padding=(0, 2))
            info_table.add_row("📚 Version:", "[bright_green]1.0[/bright_green]")
            info_table.add_row("👨‍💻 Author:", "[bright_yellow]Antoine Lemor[/bright_yellow]")
            info_table.add_row("🚀 Features:", "[cyan]50+ BERT Models, Multi-LLM (Ollama/OpenAI/Claude), Parallel GPU/CPU, Reinforcement Learning[/cyan]")
            info_table.add_row("🎯 Capabilities:", "[magenta]Multi-Label Classification, 100+ Languages, SQL/File I/O, Doccano/Label Studio Export[/magenta]")

            self.console.print(Panel(
                info_table,
                title="[bold bright_cyan]✨ Welcome to LLM Tool ✨[/bold bright_cyan]",
                border_style="bright_blue",
                padding=(1, 2)
            ))
            self.console.print()

            # Auto-detect models and system resources in background
            with self.console.status("[bold green]🔍 Scanning environment...", spinner="dots"):
                self.detected_llms = self.llm_detector.detect_all_llms()
                self.available_trainer_models = self.trainer_model_detector.get_available_models()
                # Scan only in data/ directory
                data_dir = self.settings.paths.data_dir
                self.detected_datasets = self.data_detector.scan_directory(data_dir)
                # Detect system resources
                self.system_resources = self.resource_detector.detect_all()

            # Show detection results
            self._display_detection_results()

        else:
            print("\n" + "="*80)
            print(" " * 28 + "LLM TOOL")
            print(" " * 15 + "LLM-powered Intelligent Annotation & Training Pipeline")
            print("="*80)
            print("\n📚 Version: 1.0")
            print("👨‍💻 Author: Antoine Lemor")
            print("\n📊 Data → 🤖 LLM Annotation → 🧹 Clean → 🎯 Label → 🧠 Train → 📈 Deploy")
            print("\nScanning environment...")

            self.detected_llms = self.llm_detector.detect_all_llms()
            self.available_trainer_models = self.trainer_model_detector.get_available_models()
            # Scan only in data/ directory
            data_dir = self.settings.paths.data_dir
            self.detected_datasets = self.data_detector.scan_directory(data_dir)

            # Count LLMs and trainer models
            llm_count = sum(len(m) for m in self.detected_llms.values())
            trainer_count = sum(len(m) for m in self.available_trainer_models.values())

            print(f"✓ Found {llm_count} annotation LLMs")
            print(f"✓ {trainer_count} trainable models available")
            print(f"✓ Found {len(self.detected_datasets)} datasets")
            print()

    def _display_detection_results(self):
        """Display auto-detection results in a professional format"""
        if not HAS_RICH or not self.console:
            return

        # === ANNOTATION LLMs SECTION ===
        llms_table = Table(title="🤖 Available LLMs for Annotation", border_style="cyan", show_lines=True, expand=False, width=75)
        llms_table.add_column("Provider", style="cyan", width=10)
        llms_table.add_column("Model", style="white", width=22)
        llms_table.add_column("Size", style="yellow", width=8)
        llms_table.add_column("Context", style="green", width=11)
        llms_table.add_column("Status", style="green", width=12)

        # Show Ollama models (all of them if available)
        local_llms = self.detected_llms.get('local', [])
        if local_llms:
            # Show all local models, not just first 3
            for model in local_llms:
                llms_table.add_row(
                    "Ollama",
                    model.name,
                    model.size or "N/A",
                    f"{model.context_length:,}" if model.context_length else "N/A",
                    "✓ Ready"
                )

        # Show API models (top ones)
        for provider in ['openai', 'anthropic']:
            api_models = self.detected_llms.get(provider, [])
            for model in api_models[:2]:  # Show top 2 per API provider
                llms_table.add_row(
                    provider.title(),
                    model.name,
                    "API",
                    f"{model.context_length:,}" if model.context_length else "N/A",
                    "🔑 API Key" if model.requires_api_key else "✓ Ready"
                )

        # === TRAINABLE MODELS SECTION ===
        trainer_table = Table(title="🏋️ Available Models for Training", border_style="magenta", show_lines=False, expand=False, width=85)
        trainer_table.add_column("Category", style="magenta bold", width=20)
        trainer_table.add_column("Models", style="white", width=56)

        # Define the desired order
        desired_order = [
            "Multilingual Models",
            "Long Document Models",
            "Long Document Models - French",
            "Long Document Models - Spanish",
            "Long Document Models - German",
            "Long Document Models - Italian",
            "Long Document Models - Portuguese",
            "Long Document Models - Dutch",
            "Long Document Models - Polish",
            "Long Document Models - Chinese",
            "Long Document Models - Japanese",
            "Long Document Models - Arabic",
            "Long Document Models - Russian",
            "Efficient Models",
            "English Models",
            "French Models",
            "Other Language Models"
        ]

        # Display models in the specified order
        for category in desired_order:
            if category in self.available_trainer_models:
                models = self.available_trainer_models[category]
                # Format model names compactly
                model_names = [m['name'] for m in models[:4]]  # Show first 4
                if len(models) > 4:
                    model_names.append(f"(+{len(models)-4} more)")
                trainer_table.add_row(
                    category,
                    ", ".join(model_names)
                )

        # Add any remaining categories not in the desired order
        for category, models in self.available_trainer_models.items():
            if category not in desired_order:
                model_names = [m['name'] for m in models[:4]]
                if len(models) > 4:
                    model_names.append(f"(+{len(models)-4} more)")
                trainer_table.add_row(
                    category,
                    ", ".join(model_names)
                )

        # === DISPLAY SIDE BY SIDE ===
        self.console.print(Columns([llms_table, trainer_table], equal=False, expand=True))
        self.console.print()

        # === DATASETS SECTION ===
        datasets_table = Table(title="📊 Detected Datasets", border_style="yellow", show_lines=False, expand=True)
        datasets_table.add_column("File", style="cyan", no_wrap=True, width=30)
        datasets_table.add_column("Format", style="white bold", width=12, justify="center")
        datasets_table.add_column("Size", style="green", width=10, justify="right")
        datasets_table.add_column("Folder", style="yellow", width=20)
        datasets_table.add_column("Columns", style="dim", width=35)

        if self.detected_datasets:
            for dataset in self.detected_datasets:  # Show ALL datasets
                columns_preview = ", ".join(dataset.columns[:3]) if dataset.columns else "N/A"
                if len(dataset.columns) > 3:
                    columns_preview += f" (+{len(dataset.columns)-3} more)"

                # Color format based on type
                format_style = {
                    'CSV': 'cyan bold',
                    'JSON': 'green bold',
                    'JSONL': 'blue bold',
                    'EXCEL': 'magenta bold',
                    'PARQUET': 'red bold',
                    'TSV': 'yellow bold'
                }.get(dataset.format.upper(), 'white')

                # Get folder name (parent directory name)
                folder_name = dataset.path.parent.name if dataset.path.parent.name else "data"

                datasets_table.add_row(
                    dataset.path.name,
                    f"[{format_style}]{dataset.format.upper()}[/{format_style}]",
                    f"{dataset.size_mb:.1f} MB" if dataset.size_mb else "Unknown",
                    folder_name,
                    columns_preview
                )
        else:
            datasets_table.add_row(
                "No datasets found",
                "-",
                "-",
                "-",
                "Place CSV/JSON files in current directory"
            )

        # Print datasets table
        self.console.print(datasets_table)
        self.console.print()

        # === ALL SUPPORTED FORMATS SECTION ===
        # Create a centered panel showing all supported formats
        all_formats_text = Text(justify="center")
        all_formats_text.append("📦 Supported Formats: ", style="bold cyan")

        # File formats
        all_formats_text.append("CSV", style="cyan bold")
        all_formats_text.append(" • ", style="dim")
        all_formats_text.append("JSON/JSONL", style="green bold")
        all_formats_text.append(" • ", style="dim")
        all_formats_text.append("Excel", style="magenta bold")
        all_formats_text.append(" • ", style="dim")
        all_formats_text.append("Parquet", style="red bold")
        all_formats_text.append(" • ", style="dim")
        all_formats_text.append("RData", style="yellow bold")
        all_formats_text.append(" • ", style="dim")
        all_formats_text.append("TSV", style="blue bold")

        # Databases
        all_formats_text.append("\n💾 Databases: ", style="bold cyan")
        all_formats_text.append("PostgreSQL", style="blue bold")
        all_formats_text.append(" • ", style="dim")
        all_formats_text.append("MySQL", style="yellow bold")
        all_formats_text.append(" • ", style="dim")
        all_formats_text.append("SQLite", style="green bold")
        all_formats_text.append(" • ", style="dim")
        all_formats_text.append("MongoDB", style="magenta bold")

        formats_panel = Panel(
            all_formats_text,
            border_style="cyan",
            padding=(0, 2)
        )

        self.console.print(formats_panel)
        self.console.print()

        # === SYSTEM RESOURCES SECTION ===
        if self.system_resources:
            # Create and display the visual resource panel
            resource_panel = create_visual_resource_panel(
                self.system_resources,
                show_recommendations=True
            )
            self.console.print(resource_panel)
            self.console.print()

    def get_main_menu_choice(self) -> str:
        """Display sophisticated main menu with smart suggestions"""
        if HAS_RICH and self.console:
            # Create menu table
            menu_table = Table.grid(padding=0)
            menu_table.add_column(width=3)
            menu_table.add_column()

            options = [
                ("1", "🎨 The Annotator - Zero-Shot LLM Annotation (Ollama/OpenAI/Claude) → Label Studio/Doccano Export"),
                ("2", "🏭 The Annotator Factory - LLM Annotations → Training Data → Fine-Tuned BERT Models"),
                ("3", "🎮 Training Arena - Train 50+ Models (BERT/RoBERTa/DeBERTa) with Multi-Label & Benchmarking"),
                ("4", "🤖 BERT Annotation Studio - High-Throughput Inference (Parallel GPU/CPU, 100+ Languages)"),
                ("5", "🔍 Validation Lab - Quality Scoring, Stratified Sampling, Inter-Annotator Agreement [⚠️ IN DEVELOPMENT]"),
                ("6", "💾 Profile Manager - Save & Load Configurations"),
                ("7", "📚 Documentation & Help"),
                ("0", "❌ Exit")
            ]

            for num, desc in options:
                menu_table.add_row(
                    f"[bold cyan]{num}[/bold cyan]",
                    desc
                )

            # Suggestions based on context
            suggestions = self._get_smart_suggestions()

            panel = Panel(
                menu_table,
                title="[bold]Main Menu[/bold]",
                subtitle=f"[dim]{suggestions}[/dim]" if suggestions else None,
                border_style="cyan"
            )

            self.console.print(panel)

            # Smart prompt with validation (now 0-7 since we have 8 options)
            choice = Prompt.ask(
                "\n[bold yellow]Select option[/bold yellow]",
                choices=["0", "1", "2", "3", "4", "5", "6", "7"],
                default="1"
            )

        else:
            print("\n" + "="*50)
            print("Main Menu")
            print("="*50)
            print("1. The Annotator - Zero-Shot LLM Annotation (Ollama/OpenAI/Claude) → Export")
            print("2. The Annotator Factory - LLM Annotations → Training Data → BERT Models")
            print("3. Training Arena - Train 50+ Models (Multi-Label & Benchmarking)")
            print("4. BERT Annotation Studio - High-Throughput Inference (Parallel GPU/CPU)")
            print("5. Validation Lab - Quality Scoring & Sampling [⚠️ IN DEVELOPMENT]")
            print("6. Profile Manager - Save & Load Configurations")
            print("7. Documentation & Help")
            print("0. Exit")
            print("-"*50)

            suggestions = self._get_smart_suggestions()
            if suggestions:
                print(f"💡 {suggestions}")

            choice = input("\nSelect option (0-7): ").strip()

        return choice

    def _get_smart_suggestions(self) -> str:
        """Generate intelligent suggestions based on context"""
        suggestions = []

        # Check for available LLMs
        if self.detected_llms:
            local_llms = self.detected_llms.get('local', [])
            if local_llms:
                # Show count of local LLMs
                suggestions.append(f"{len(local_llms)} local LLMs available")
            else:
                suggestions.append("No local LLMs - run 'ollama pull llama3.2'")

        # Check for datasets
        if self.detected_datasets:
            suggestions.append(f"{len(self.detected_datasets)} dataset{'s' if len(self.detected_datasets) != 1 else ''} found")
        else:
            suggestions.append("No datasets in current directory")

        # Check for recent profiles
        recent_profiles = self.profile_manager.list_profiles()
        if recent_profiles:
            suggestions.append(f"Last: {recent_profiles[0].name}")

        return " | ".join(suggestions) if suggestions else ""

    def _get_or_prompt_api_key(self, provider: str, model_name: Optional[str] = None) -> Optional[str]:
        """
        Get API key from secure storage or prompt user.

        Parameters
        ----------
        provider : str
            Provider name (openai, anthropic, google)
        model_name : str, optional
            Model name to save with the key

        Returns
        -------
        str or None
            The API key
        """
        # Check if key exists in storage
        existing_key = self.settings.get_api_key(provider)

        if existing_key:
            if HAS_RICH and self.console:
                use_existing = Confirm.ask(
                    f"[dim]Found saved API key for {provider}. Use it?[/dim]",
                    default=True
                )
            else:
                use_existing = input(f"Found saved API key for {provider}. Use it? [Y/n]: ").strip().lower() != 'n'

            if use_existing:
                return existing_key

        # Prompt for new key
        if HAS_RICH and self.console:
            self.console.print(f"\n[bold cyan]🔑 API Key Required for {provider}[/bold cyan]")
            if self.settings.key_manager:
                self.console.print("[dim]Your key will be stored securely using encryption[/dim]")
            else:
                self.console.print("[yellow]⚠️  Install 'cryptography' for secure key storage: pip install cryptography[/yellow]")

            api_key = Prompt.ask("API Key", password=True)

            # Ask if user wants to save the key
            if api_key:
                save_key = Confirm.ask(
                    "[dim]Save this API key for future use?[/dim]",
                    default=True
                )

                if save_key:
                    self.settings.set_api_key(provider, api_key, model_name)
                    self.console.print("[green]✓ API key saved securely[/green]")
        else:
            print(f"\nAPI Key Required for {provider}")
            if self.settings.key_manager:
                print("(Will be stored securely using encryption)")
            else:
                print("⚠️  Install 'cryptography' for secure key storage")

            api_key = input("API Key: ").strip()

            if api_key:
                save = input("Save this API key for future use? [Y/n]: ").strip().lower() != 'n'
                if save:
                    self.settings.set_api_key(provider, api_key, model_name)
                    print("✓ API key saved")

        return api_key

    @staticmethod
    def _estimate_model_size_billion(model: ModelInfo) -> Optional[float]:
        """Estimate model parameter count (billions) from its metadata."""
        patterns = [
            r'(\d+(?:\.\d+)?)\s*b',
            r'(\d+(?:\.\d+)?)\s*bn',
        ]

        lower_name = model.name.lower()
        for pattern in patterns:
            match = re.search(pattern, lower_name)
            if match:
                try:
                    return float(match.group(1))
                except ValueError:
                    continue

        if model.size and model.size.lower() != 'n/a':
            match = re.search(r'(\d+(?:\.\d+)?)\s*(?:b|bn)', model.size.lower())
            if match:
                try:
                    return float(match.group(1))
                except ValueError:
                    return None

        return None

    def _prompt_for_text_column(self, columns: List[str], suggested: Optional[str]) -> str:
        """Prompt the user to choose the text column."""
        if not columns:
            if HAS_RICH and self.console:
                return Prompt.ask("Text column name", default="text")
            return input("Text column name [text]: ").strip() or "text"

        choices = [str(col) for col in columns]
        default_choice = suggested if suggested in choices else choices[0]

        if HAS_RICH and self.console:
            return Prompt.ask(
                "Which column contains the text to annotate?",
                choices=choices,
                default=default_choice
            )

        print("Available columns:")
        for col in choices:
            marker = " (suggested)" if col == default_choice else ""
            print(f"  - {col}{marker}")
        response = input(f"Text column [{default_choice}]: ").strip()
        return response or default_choice

    def _detect_id_columns(self, columns: List[str]) -> List[str]:
        """Detect all columns that could serve as IDs"""
        id_columns = []
        for col in columns:
            col_lower = col.lower()
            # Check if column name suggests it's an ID
            if (col_lower == 'id' or
                col_lower.endswith('_id') or
                col_lower.startswith('id_') or
                'identifier' in col_lower or
                col_lower in ['promesse_id', 'sentence_id', 'doc_id', 'item_id', 'record_id']):
                id_columns.append(col)
        return id_columns

    def _prompt_for_identifier_column(
        self,
        columns: List[str],
        suggested: Optional[str]
    ) -> Optional[str]:
        """Ask which column should serve as unique identifier, if any."""
        if not columns:
            if HAS_RICH and self.console:
                self.console.print("[dim]No columns detected; will create 'llm_annotation_id'.[/dim]")
            else:
                print("No columns detected; will create 'llm_annotation_id'.")
            return None

        # Detect all ID columns
        id_columns = self._detect_id_columns(columns)

        # If multiple ID columns found, offer to combine them
        if len(id_columns) > 1:
            if HAS_RICH and self.console:
                self.console.print(f"\n[bold cyan]📋 Found {len(id_columns)} ID columns:[/bold cyan]")
                for i, col in enumerate(id_columns, 1):
                    self.console.print(f"  {i}. [cyan]{col}[/cyan]")

                # Ask if user wants to use single or combined ID
                self.console.print("\n[bold]ID Strategy:[/bold]")
                self.console.print("[dim]IDs are used to track which texts have been annotated and link results to your original data.[/dim]")
                self.console.print("• [cyan]single[/cyan]: Use one column as ID")
                self.console.print("• [cyan]combine[/cyan]: Combine multiple columns (e.g., 'promesse_id+sentence_id')")
                self.console.print("• [cyan]none[/cyan]: Generate automatic IDs")

                id_strategy = Prompt.ask(
                    "ID strategy",
                    choices=["single", "combine", "none"],
                    default="single"
                )

                if id_strategy == "none":
                    self.console.print("[dim]An 'llm_annotation_id' column will be created automatically.[/dim]")
                    return None
                elif id_strategy == "combine":
                    # Ask which columns to combine
                    self.console.print("\n[bold]Select columns to combine:[/bold]")
                    self.console.print("[dim]Enter column numbers separated by commas (e.g., '1,2')[/dim]")

                    while True:
                        selection = Prompt.ask("Columns to combine")
                        try:
                            indices = [int(x.strip()) - 1 for x in selection.split(',')]
                            if all(0 <= i < len(id_columns) for i in indices):
                                selected_cols = [id_columns[i] for i in indices]
                                combined_id = "+".join(selected_cols)
                                self.console.print(f"[green]✓ Will combine: {' + '.join(selected_cols)}[/green]")
                                self.console.print(f"[dim]Example ID format: {' _ '.join(['123' for _ in selected_cols])}[/dim]")
                                return combined_id
                            else:
                                self.console.print("[red]Invalid column numbers. Try again.[/red]")
                        except (ValueError, IndexError):
                            self.console.print("[red]Invalid format. Use comma-separated numbers (e.g., '1,2')[/red]")
                else:  # single
                    # Select single ID column
                    default_choice = id_columns[0]
                    return Prompt.ask(
                        "Which ID column to use?",
                        choices=id_columns,
                        default=default_choice
                    )
            else:
                # Non-Rich fallback
                print(f"\nFound {len(id_columns)} ID columns:")
                for i, col in enumerate(id_columns, 1):
                    print(f"  {i}. {col}")

                choice = input("Use single ID (s), combine IDs (c), or generate new (n)? [s/c/n]: ").strip().lower()

                if choice == 'n':
                    print("Will create 'llm_annotation_id' automatically.")
                    return None
                elif choice == 'c':
                    selection = input("Enter column numbers to combine (comma-separated): ").strip()
                    try:
                        indices = [int(x.strip()) - 1 for x in selection.split(',')]
                        selected_cols = [id_columns[i] for i in indices]
                        return "+".join(selected_cols)
                    except (ValueError, IndexError):
                        print("Invalid selection. Using first ID column.")
                        return id_columns[0]
                else:
                    idx = int(input(f"Select ID column [1-{len(id_columns)}]: ").strip() or "1") - 1
                    return id_columns[idx] if 0 <= idx < len(id_columns) else id_columns[0]

        # Single or no ID column found - use original logic
        default_has_id = suggested is not None or len(id_columns) == 1

        if HAS_RICH and self.console:
            has_id = Confirm.ask(
                "Does the dataset already contain a unique ID column?",
                default=default_has_id
            )
            if not has_id:
                self.console.print("[dim]An 'llm_annotation_id' column will be created automatically.[/dim]")
                return None

            choices = [str(col) for col in columns]
            default_choice = (id_columns[0] if id_columns else
                            (suggested if suggested in choices else choices[0]))
            return Prompt.ask(
                "Which column should be used as the identifier?",
                choices=choices,
                default=default_choice
            )

        prompt_default = "y" if default_has_id else "n"
        raw = input(f"Dataset has an ID column? [y/n] ({prompt_default}): ").strip().lower()
        has_id = raw or prompt_default
        if has_id.startswith('n'):
            print("We'll create 'llm_annotation_id' automatically.")
            return None

        print("Available columns:")
        for col in columns:
            marker = " (suggested)" if suggested and col == suggested else ""
            print(f"  - {col}{marker}")
        default_choice = suggested or columns[0]
        response = input(f"Identifier column [{default_choice}]: ").strip()
        return response or default_choice

    def _display_ascii_logo(self):
        """Display only the ASCII logo, tagline, and workflow (without info panel)"""
        if HAS_RICH and self.console:
            from rich.align import Align
            from rich.text import Text

            # Get terminal width
            width = self.console.width

            self.console.print()
            self.console.print("[on bright_blue]" + " " * width + "[/on bright_blue]")
            self.console.print("[on bright_magenta]" + " " * width + "[/on bright_magenta]")
            self.console.print()

            # Giant LLM TOOL text with each letter in different color
            self.console.print(Align.center("[bright_magenta]██╗     [bright_yellow]██╗     [bright_green]███╗   ███╗    [bright_cyan]████████╗ [bright_red]██████╗  [bright_blue]██████╗ [bright_white]██╗     "))
            self.console.print(Align.center("[bright_magenta]██║     [bright_yellow]██║     [bright_green]████╗ ████║    [bright_cyan]╚══██╔══╝[bright_red]██╔═══██╗[bright_blue]██╔═══██╗[bright_white]██║     "))
            self.console.print(Align.center("[bright_magenta]██║     [bright_yellow]██║     [bright_green]██╔████╔██║       [bright_cyan]██║   [bright_red]██║   ██║[bright_blue]██║   ██║[bright_white]██║     "))
            self.console.print(Align.center("[bright_magenta]██║     [bright_yellow]██║     [bright_green]██║╚██╔╝██║       [bright_cyan]██║   [bright_red]██║   ██║[bright_blue]██║   ██║[bright_white]██║     "))
            self.console.print(Align.center("[bright_magenta]███████╗[bright_yellow]███████╗[bright_green]██║ ╚═╝ ██║       [bright_cyan]██║   [bright_red]╚██████╔╝[bright_blue]╚██████╔╝[bright_white]███████╗"))
            self.console.print(Align.center("[bright_magenta]╚══════╝[bright_yellow]╚══════╝[bright_green]╚═╝     ╚═╝       [bright_cyan]╚═╝    [bright_red]╚═════╝  [bright_blue]╚═════╝ [bright_white]╚══════╝"))

            self.console.print()
            self.console.print(Align.center("[bold bright_yellow on blue]  🚀 LLM-powered Intelligent Annotation & Training Pipeline 🚀  [/bold bright_yellow on blue]"))
            self.console.print()

            # Colorful pipeline with emojis
            pipeline_text = Text()
            pipeline_text.append("📊 Data ", style="bold bright_yellow on black")
            pipeline_text.append("→ ", style="bold white")
            pipeline_text.append("🤖 LLM Annotation ", style="bold bright_green on black")
            pipeline_text.append("→ ", style="bold white")
            pipeline_text.append("🧹 Clean ", style="bold bright_cyan on black")
            pipeline_text.append("→ ", style="bold white")
            pipeline_text.append("🎯 Label ", style="bold bright_magenta on black")
            pipeline_text.append("→ ", style="bold white")
            pipeline_text.append("🧠 Train ", style="bold bright_red on black")
            pipeline_text.append("→ ", style="bold white")
            pipeline_text.append("📈 Deploy ", style="bold bright_blue on black")

            self.console.print(Align.center(pipeline_text))
            self.console.print()
            self.console.print("[on bright_magenta]" + " " * width + "[/on bright_magenta]")
            self.console.print("[on bright_blue]" + " " * width + "[/on bright_blue]")
            self.console.print()
        else:
            print("="*80)
            print(" " * 28 + "LLM TOOL")
            print(" " * 18 + "Intelligent Annotation & Training Pipeline")
            print("="*80)
            print("\n  🤖 -> 📝 -> 🧹 -> 🎯 -> 🧠 -> 📊 -> ✨")
            print("  AI   Annotate Clean Label Train Test Deploy\n")
            print("="*80 + "\n")

    def _display_mode_banner(self, mode: str):
        """Display mode-specific ASCII banner"""
        if not (HAS_RICH and self.console):
            return

        from rich.align import Align
        from .banners import BANNERS

        if mode not in BANNERS:
            return

        banner_data = BANNERS[mode]
        color = banner_data['color']

        self.console.print()
        for line in banner_data['ascii'].split('\n'):
            self.console.print(Align.center(f"[bold {color}]{line}[/bold {color}]"))

        self.console.print()
        self.console.print(Align.center(f"[bold {color}]{banner_data['tagline']}[/bold {color}]"))
        self.console.print()

    def _display_section_header(self, title: str, description: str, mode_info: Optional[Dict[str, Any]] = None):
        """Display a personalized section header with mode-specific information"""
        if HAS_RICH and self.console:
            # If mode_info provided, create a detailed panel
            if mode_info:
                from rich.table import Table

                info_table = Table(show_header=False, box=None, padding=(0, 2))

                # Always add author first
                info_table.add_row("👨‍💻 Author:", "[bright_yellow]Antoine Lemor[/bright_yellow]")

                # Add mode-specific rows
                if 'workflow' in mode_info:
                    info_table.add_row("📊 Workflow:", f"[cyan]{mode_info['workflow']}[/cyan]")

                if 'capabilities' in mode_info:
                    caps = ' • '.join(mode_info['capabilities'])
                    info_table.add_row("🎯 Capabilities:", f"[yellow]{caps}[/yellow]")

                if 'input' in mode_info:
                    info_table.add_row("📥 Input:", f"[green]{mode_info['input']}[/green]")

                if 'output' in mode_info:
                    info_table.add_row("📤 Output:", f"[magenta]{mode_info['output']}[/magenta]")

                if 'best_for' in mode_info:
                    info_table.add_row("✨ Best For:", f"[bright_blue]{mode_info['best_for']}[/bright_blue]")

                if 'duration' in mode_info:
                    info_table.add_row("⏱️  Duration:", f"[dim]{mode_info['duration']}[/dim]")

                self.console.print(Panel(
                    info_table,
                    title=f"[bold cyan]{title}[/bold cyan]",
                    subtitle=f"[dim]{description}[/dim]",
                    border_style="cyan",
                    padding=(1, 2)
                ))

                # Display horizontal resource banner below the info panel
                if self.system_resources:
                    self.console.print()
                    resource_banner = create_mode_resource_banner(self.system_resources)
                    banner_panel = Panel(
                        resource_banner,
                        title="[bold bright_blue]⚙️  System Resources[/bold bright_blue]",
                        border_style="blue",
                        padding=(0, 1)
                    )
                    self.console.print(banner_panel)

            else:
                # Fallback to simple panel
                self.console.print(Panel.fit(
                    f"[bold cyan]{title}[/bold cyan]\n{description}",
                    border_style="cyan"
                ))

                # Display horizontal resource banner
                if self.system_resources:
                    self.console.print()
                    resource_banner = create_mode_resource_banner(self.system_resources)
                    banner_panel = Panel(
                        resource_banner,
                        title="[bold bright_blue]⚙️  System Resources[/bold bright_blue]",
                        border_style="blue",
                        padding=(0, 1)
                    )
                    self.console.print(banner_panel)

        else:
            print(f"\n{title}")
            print(description)

    def _display_welcome_banner(self):
        """Display a beautiful welcome banner"""
        if HAS_RICH and self.console:
            # Spectacular multicolor full-width ASCII art banner
            from rich.align import Align
            from rich.text import Text

            # Get terminal width
            width = self.console.width

            self.console.print()
            self.console.print("[on bright_blue]" + " " * width + "[/on bright_blue]")
            self.console.print("[on bright_magenta]" + " " * width + "[/on bright_magenta]")
            self.console.print()

            # Giant LLM TOOL text with each letter in different color
            self.console.print(Align.center("[bright_magenta]██╗     [bright_yellow]██╗     [bright_green]███╗   ███╗    [bright_cyan]████████╗ [bright_red]██████╗  [bright_blue]██████╗ [bright_white]██╗     "))
            self.console.print(Align.center("[bright_magenta]██║     [bright_yellow]██║     [bright_green]████╗ ████║    [bright_cyan]╚══██╔══╝[bright_red]██╔═══██╗[bright_blue]██╔═══██╗[bright_white]██║     "))
            self.console.print(Align.center("[bright_magenta]██║     [bright_yellow]██║     [bright_green]██╔████╔██║       [bright_cyan]██║   [bright_red]██║   ██║[bright_blue]██║   ██║[bright_white]██║     "))
            self.console.print(Align.center("[bright_magenta]██║     [bright_yellow]██║     [bright_green]██║╚██╔╝██║       [bright_cyan]██║   [bright_red]██║   ██║[bright_blue]██║   ██║[bright_white]██║     "))
            self.console.print(Align.center("[bright_magenta]███████╗[bright_yellow]███████╗[bright_green]██║ ╚═╝ ██║       [bright_cyan]██║   [bright_red]╚██████╔╝[bright_blue]╚██████╔╝[bright_white]███████╗"))
            self.console.print(Align.center("[bright_magenta]╚══════╝[bright_yellow]╚══════╝[bright_green]╚═╝     ╚═╝       [bright_cyan]╚═╝    [bright_red]╚═════╝  [bright_blue]╚═════╝ [bright_white]╚══════╝"))

            self.console.print()
            self.console.print(Align.center("[bold bright_yellow on blue]  🚀 LLM-powered Intelligent Annotation & Training Pipeline 🚀  [/bold bright_yellow on blue]"))
            self.console.print()

            # Colorful pipeline with emojis
            pipeline_text = Text()
            pipeline_text.append("📊 Data ", style="bold bright_yellow on black")
            pipeline_text.append("→ ", style="bold white")
            pipeline_text.append("🤖 LLM Annotation ", style="bold bright_green on black")
            pipeline_text.append("→ ", style="bold white")
            pipeline_text.append("🧹 Clean ", style="bold bright_cyan on black")
            pipeline_text.append("→ ", style="bold white")
            pipeline_text.append("🎯 Label ", style="bold bright_magenta on black")
            pipeline_text.append("→ ", style="bold white")
            pipeline_text.append("🧠 Train ", style="bold bright_red on black")
            pipeline_text.append("→ ", style="bold white")
            pipeline_text.append("📈 Deploy ", style="bold bright_blue on black")

            self.console.print(Align.center(pipeline_text))
            self.console.print()
            self.console.print("[on bright_magenta]" + " " * width + "[/on bright_magenta]")
            self.console.print("[on bright_blue]" + " " * width + "[/on bright_blue]")
            self.console.print()

            # Information table
            info_table = Table(show_header=False, box=None, padding=(0, 2))
            info_table.add_row("📚 Version:", "[bright_green]1.0[/bright_green]")
            info_table.add_row("👨‍💻 Author:", "[bright_yellow]Antoine Lemor[/bright_yellow]")
            info_table.add_row("🚀 Features:", "[cyan]Ollama/OpenAI/Claude, Prompt Wizard, Auto JSON Repair, 200K Context Support[/cyan]")
            info_table.add_row("🎯 Capabilities:", "[magenta]Multi-Label Categories, NER, Hierarchical Schemas, Pydantic Validation[/magenta]")
            info_table.add_row("⚡ Performance:", "[green]Parallel Processing, Incremental Save, Resume, Label Studio/Doccano Export[/green]")

            self.console.print(Panel(
                info_table,
                title="[bold bright_cyan]✨ Welcome to LLM Tool ✨[/bold bright_cyan]",
                border_style="bright_blue",
                padding=(1, 2)
            ))
            self.console.print()
        else:
            # Fallback for non-Rich environments (should never happen due to import check)
            print("="*80)
            print(" " * 28 + "LLM TOOL")
            print(" " * 18 + "Intelligent Annotation & Training Pipeline")
            print("="*80)
            print("\n📚 Version: 1.0")
            print("👨‍💻 Author: Antoine Lemor")
            print("🚀 Features: Ollama/OpenAI/Claude, Prompt Wizard, 200K Context, JSON Repair")
            print("🎯 Capabilities: Multi-Label Categories, NER, Hierarchical Schemas")
            print("⚡ Performance: Parallel Processing, Incremental Save, Resume Support")
            print("\n  🤖 -> 📝 -> 🧹 -> 🎯 -> 🧠 -> 📊 -> ✨")
            print("  AI   Annotate Clean Label Train Test Deploy\n")
            print("="*80 + "\n")

    def quick_start_wizard(self):
        """Complete workflow: LLM annotation followed by intelligent model training"""
        # Display ASCII logo only
        self._display_ascii_logo()

        # Display mode-specific banner
        self._display_mode_banner('factory')

        # Display personalized mode info
        self._display_section_header(
            "🏭 The Annotator Factory - LLM Annotations → Training Data → Fine-Tuned BERT Models",
            "End-to-end pipeline: Ollama/OpenAI/Claude annotation → Automatic training data conversion → Model training",
            mode_info={
                'workflow': 'Data → LLM Annotate (Parallel) → Language Detection → Auto-Convert → Train 50+ Models',
                'capabilities': ['Multi-LLM Support (Ollama/OpenAI/Claude)', '100+ Languages', 'Multi-Label Classification', 'Reinforcement Learning'],
                'input': 'CSV/Excel/JSON/SQL with text column',
                'output': 'Annotated data + Trained BERT models + Benchmarking metrics + Training summaries',
                'best_for': 'Complete zero-shot annotation to supervised learning pipeline with automatic optimization',
                'duration': '~10-60 min (annotation + training + benchmarking)'
            }
        )

        if HAS_RICH and self.console:
            # Get smart suggestions
            suggestions = self._get_smart_suggestions()

            # Create workflow menu table
            from rich.table import Table
            workflow_table = Table(show_header=False, box=None, padding=(0, 2))
            workflow_table.add_column("Option", style="cyan", width=8)
            workflow_table.add_column("Description")

            workflows = [
                ("1", "🔄 Resume/Relaunch Workflow (Use saved parameters or resume incomplete)"),
                ("2", "🎯 Complete Workflow (New annotation → training pipeline)"),
                ("3", "🗑️  Clean Old Metadata (Delete saved parameters)"),
                ("0", "⬅️  Back to main menu")
            ]

            for option, desc in workflows:
                workflow_table.add_row(f"[bold cyan]{option}[/bold cyan]", desc)

            # Display panel with suggestions
            panel = Panel(
                workflow_table,
                title="[bold]🏭 The Annotator Factory[/bold]",
                subtitle=f"[dim]{suggestions}[/dim]" if suggestions else None,
                border_style="cyan"
            )

            self.console.print("\n")
            self.console.print(panel)

            workflow = Prompt.ask(
                "\n[bold yellow]Select workflow[/bold yellow]",
                choices=["0", "1", "2", "3"],
                default="2"
            )

            if workflow == "0":
                return
            elif workflow == "1":
                self._resume_mode2()
            elif workflow == "2":
                # CRITICAL: Ask user for session name first (like Training Arena and Mode 1)
                from datetime import datetime

                self.console.print("\n[bold cyan]═══════════════════════════════════════════════════════════════[/bold cyan]")
                self.console.print("[bold cyan]           📝 Session Name Configuration                       [/bold cyan]")
                self.console.print("[bold cyan]═══════════════════════════════════════════════════════════════[/bold cyan]\n")

                self.console.print("[bold]Why session names matter:[/bold]")
                self.console.print("  • [green]Organization:[/green] Easily identify pipelines (e.g., 'sentiment_twitter', 'legal_classifier')")
                self.console.print("  • [green]Traceability:[/green] Track annotations, training, and models in one place")
                self.console.print("  • [green]Collaboration:[/green] Team members understand each pipeline's purpose")
                self.console.print("  • [green]Audit trail:[/green] Timestamp ensures uniqueness\n")

                self.console.print("[dim]Format: {session_name}_{yyyymmdd_hhmmss}[/dim]")
                self.console.print("[dim]Example: sentiment_pipeline_20251008_143022[/dim]\n")

                # Ask for user-defined session name
                user_session_name = Prompt.ask(
                    "[bold yellow]Enter a descriptive name for this annotation+training pipeline[/bold yellow]",
                    default="factory_session"
                ).strip()

                # Sanitize the user input
                user_session_name = user_session_name.replace(' ', '_').replace('/', '_').replace('\\', '_')
                user_session_name = ''.join(c for c in user_session_name if c.isalnum() or c in ['_', '-'])

                # Create full session ID with timestamp
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                session_id = f"{user_session_name}_{timestamp}"

                self.console.print(f"\n[bold green]✓ Session ID:[/bold green] [cyan]{session_id}[/cyan]")
                self.console.print(f"[dim]This ID will be used for annotations, training, and all outputs[/dim]\n")

                # Pass session_id to _complete_workflow_mode2
                self._complete_workflow_mode2(session_id=session_id)
            elif workflow == "3":
                self._clean_metadata()
        else:
            print("\n=== The Annotator Factory ===")
            print("Clone The Annotator into ML Models\n")
            print("1. Resume/Relaunch Workflow")
            print("2. Complete Workflow (Recommended)")
            print("3. Clean Old Metadata")
            print("0. Back")
            choice = input("\nSelect workflow: ").strip()

            if choice == "1":
                self._resume_mode2()
            elif choice == "2":
                self._complete_workflow_mode2()
            elif choice == "3":
                self._clean_metadata()

    def _complete_workflow_mode2(self, session_id: str = None):
        """Execute complete annotation → training workflow

        Parameters
        ----------
        session_id : str, optional
            Session identifier for organizing outputs. If None, a timestamp-based ID is generated.
        """
        import pandas as pd
        from datetime import datetime
        from pathlib import Path

        # Generate session_id if not provided (for backward compatibility)
        if session_id is None:
            session_id = f"factory_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        # Create session directories
        session_dirs = self._create_annotator_factory_session_directories(session_id)

        # Step 1: Data Source Selection
        self.console.print("[bold]Step 1/7: Data Source Selection[/bold]\n")

        # Ask user to choose between files and SQL database
        self.console.print("[yellow]Choose data source:[/yellow]")
        self.console.print("  1. 📁 Files (CSV/Excel/JSON/etc.) - Auto-detected or manual")
        self.console.print("  2. 🗄️  SQL Database (PostgreSQL/MySQL/SQLite/SQL Server)\n")

        data_source_choice = Prompt.ask(
            "Data source",
            choices=["1", "2"],
            default="1"
        )

        use_sql_database = (data_source_choice == "2")

        if use_sql_database:
            # SQL DATABASE WORKFLOW
            self.console.print("\n[bold cyan]🗄️  SQL Database (Training Sample)[/bold cyan]\n")
            self.console.print("[yellow]Note: For training, you'll select a representative sample from your database[/yellow]\n")

            # Database type selection
            db_choices = ["PostgreSQL", "MySQL", "SQLite", "Microsoft SQL Server"]
            db_table = Table(title="Database Types", border_style="cyan")
            db_table.add_column("#", style="cyan", width=6)
            db_table.add_column("Database Type", style="white")
            for i, choice in enumerate(db_choices, 1):
                db_table.add_row(str(i), choice)
            self.console.print(db_table)

            db_choice = self._int_prompt_with_validation("Select database type", 1, 1, len(db_choices))
            db_type_name = db_choices[db_choice - 1]

            # Connection details
            if db_type_name == "SQLite":
                db_file = Prompt.ask("SQLite database file path")
                connection_string = f"sqlite:///{db_file}"
            else:
                host = Prompt.ask("Database host", default="localhost")
                default_ports = {"PostgreSQL": "5432", "MySQL": "3306", "Microsoft SQL Server": "1433"}
                port = Prompt.ask("Port", default=default_ports.get(db_type_name, "5432"))
                username = Prompt.ask("Username", default="postgres" if db_type_name == "PostgreSQL" else "root")
                password = Prompt.ask("Password", password=True)
                database = Prompt.ask("Database name")

                if db_type_name == "PostgreSQL":
                    connection_string = f"postgresql://{username}:{password}@{host}:{port}/{database}"
                elif db_type_name == "MySQL":
                    connection_string = f"mysql+pymysql://{username}:{password}@{host}:{port}/{database}"
                else:
                    connection_string = f"mssql+pyodbc://{username}:{password}@{host}:{port}/{database}?driver=ODBC+Driver+17+for+SQL+Server"

            # Test connection
            self.console.print("\nTesting connection...")
            try:
                from sqlalchemy import create_engine, inspect, text
                import pandas as pd
                engine = create_engine(connection_string)
                with engine.connect() as conn:
                    conn.execute(text("SELECT 1"))
                self.console.print("[green]✓ Connected successfully![/green]\n")
            except Exception as e:
                self.console.print(f"[red]✗ Connection failed: {str(e)}[/red]")
                input("\nPress Enter to continue...")
                return

            # Table selection
            inspector = inspect(engine)
            tables = inspector.get_table_names()
            if not tables:
                self.console.print("[red]No tables found[/red]")
                input("\nPress Enter to continue...")
                return

            # Get row counts
            table_info = []
            for table in tables:
                try:
                    with engine.connect() as conn:
                        result = conn.execute(text(f"SELECT COUNT(*) FROM {table}"))
                        table_info.append((table, result.scalar()))
                except:
                    table_info.append((table, None))

            tables_table = Table(title="Available Tables", border_style="cyan")
            tables_table.add_column("#", style="cyan", width=3)
            tables_table.add_column("Table Name", style="white")
            tables_table.add_column("Rows", style="green", justify="right")
            for i, (table, rows) in enumerate(table_info, 1):
                tables_table.add_row(str(i), table, f"{rows:,}" if rows else "?")
            self.console.print(tables_table)

            table_choice = self._int_prompt_with_validation("Select table", 1, 1, len(table_info))
            selected_table, total_rows = table_info[table_choice - 1]
            self.console.print(f"\n[green]✓ Selected: {selected_table} ({total_rows:,} rows)[/green]\n")

            # Load ALL data to temporary CSV (will use SAME workflow as files)
            from datetime import datetime
            import pandas as pd

            df = pd.read_sql(f"SELECT * FROM {selected_table}", engine)

            # Save to CSV in data/annotations
            annotations_dir = self.settings.paths.data_dir / 'annotations'
            annotations_dir.mkdir(parents=True, exist_ok=True)
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            data_path = annotations_dir / f"quickstart_sql_{selected_table}_{timestamp}.csv"
            df.to_csv(data_path, index=False)
            data_format = 'csv'

            self.console.print(f"[green]✓ Loaded {len(df):,} rows from {selected_table}[/green]")
            self.console.print(f"[dim]Saved to: {data_path}[/dim]")

        else:
            # FILE-BASED WORKFLOW (original code)
            if not self.detected_datasets:
                self.console.print("[yellow]No datasets auto-detected.[/yellow]")
                data_path = Path(self._prompt_file_path("Dataset path"))
            else:
                self.console.print(f"\n[bold cyan]📊 Found {len(self.detected_datasets)} dataset(s):[/bold cyan]\n")

                # Create table for datasets
                datasets_table = Table(border_style="cyan", show_header=True)
                datasets_table.add_column("#", style="bold yellow", width=4)
                datasets_table.add_column("Filename", style="white")
                datasets_table.add_column("Format", style="green", width=10)
                datasets_table.add_column("Size", style="magenta", width=10)
                datasets_table.add_column("Rows", style="cyan", width=10)
                datasets_table.add_column("Columns", style="blue", width=10)

                for i, ds in enumerate(self.detected_datasets[:20], 1):
                    # Format size
                    if ds.size_mb < 0.1:
                        size_str = f"{ds.size_mb * 1024:.1f} KB"
                    else:
                        size_str = f"{ds.size_mb:.1f} MB"

                    # Format rows and columns
                    rows_str = f"{ds.rows:,}" if ds.rows else "?"
                    cols_str = str(len(ds.columns)) if ds.columns else "?"

                    datasets_table.add_row(
                        str(i),
                        ds.path.name,
                        ds.format.upper(),
                        size_str,
                        rows_str,
                        cols_str
                    )

                self.console.print(datasets_table)
                self.console.print()

                use_detected = Confirm.ask("[bold yellow]Use detected dataset?[/bold yellow]", default=True)
                if use_detected:
                    choice = self._int_prompt_with_validation("Select dataset", 1, 1, len(self.detected_datasets))
                    data_path = self.detected_datasets[choice - 1].path
                else:
                    data_path = Path(self._prompt_file_path("Dataset path"))

            # Detect format
            data_format = data_path.suffix[1:].lower()
            if data_format == 'xlsx':
                data_format = 'excel'

            self.console.print(f"[green]✓ Selected: {data_path.name} ({data_format})[/green]")

        # Step 2: Column Selection with Intelligent Detection (SAME AS MODE 1)
        self.console.print("\n[bold]Step 2/7: Column Selection with Intelligent Detection[/bold]")
        self.console.print("[dim]Analyzing columns...[/dim]\n")

        # Load sample data for intelligent detection (SAME AS MODE 1 DATABASE ANNOTATOR)
        import pandas as pd
        df_sample = pd.read_csv(data_path, nrows=100) if data_path.suffix == '.csv' else pd.read_excel(data_path, nrows=100)

        # Detect text columns intelligently (SAME LOGIC AS MODE 1)
        text_candidates = []
        id_candidates = []

        for col_name in df_sample.columns:
            # Check if it's a potential ID column
            col_lower = col_name.lower()
            if any(id_keyword in col_lower for id_keyword in ['id', 'key', 'index', 'number', 'num', 'pk']):
                # Check if values are unique
                is_unique = df_sample[col_name].nunique() == len(df_sample[col_name].dropna())
                if is_unique:
                    id_candidates.append({
                        'name': col_name,
                        'type': str(df_sample[col_name].dtype),
                        'confidence': 'high' if 'id' in col_lower else 'medium'
                    })

            # Check if it's a text column (object/string type)
            if df_sample[col_name].dtype == 'object':
                # Get non-null samples
                non_null = df_sample[col_name].dropna()
                if len(non_null) == 0:
                    continue

                # Calculate average length
                avg_length = non_null.astype(str).str.len().mean()
                sample_value = str(non_null.iloc[0])[:80] if len(non_null) > 0 else ""

                # Determine confidence based on average length
                if avg_length > 100:
                    confidence = "high"
                elif avg_length > 50:
                    confidence = "medium"
                elif avg_length > 20:
                    confidence = "low"
                else:
                    continue  # Skip very short text

                text_candidates.append({
                    'name': col_name,
                    'confidence': confidence,
                    'avg_length': avg_length,
                    'sample': sample_value
                })

        # Sort candidates (SAME AS MODE 1)
        confidence_order = {"high": 0, "medium": 1, "low": 2}
        text_candidates.sort(key=lambda x: (confidence_order[x['confidence']], -x['avg_length']))
        id_candidates.sort(key=lambda x: (confidence_order.get(x['confidence'], 3)))

        # Display columns with intelligent suggestions (SAME TABLE AS MODE 1)
        col_table = Table(title=f"Columns in Dataset", box=box.ROUNDED)
        col_table.add_column("#", style="cyan", justify="right", width=4)
        col_table.add_column("Column Name", style="green", width=25)
        col_table.add_column("Type", style="yellow", width=20)
        col_table.add_column("Detection", style="magenta", width=30)

        detected_text_col = None
        detected_id_col = None
        columns_list = list(df_sample.columns)

        for idx, col_name in enumerate(columns_list, 1):
            col_type = str(df_sample[col_name].dtype)
            detection = ""

            # Check if it's a suggested text column
            text_match = next((tc for tc in text_candidates if tc['name'] == col_name), None)
            if text_match:
                if text_match['confidence'] == 'high':
                    detection = "📝 Text (High confidence)"
                    if detected_text_col is None:
                        detected_text_col = idx
                elif text_match['confidence'] == 'medium':
                    detection = "📝 Text (Medium)"
                else:
                    detection = "📝 Text (Low)"

            # Check if it's a suggested ID column
            id_match = next((ic for ic in id_candidates if ic['name'] == col_name), None)
            if id_match:
                if id_match['confidence'] == 'high':
                    detection = "🔑 ID (Recommended)"
                    if detected_id_col is None:
                        detected_id_col = idx
                else:
                    detection = "🔑 ID (Possible)"

            col_table.add_row(str(idx), col_name, col_type, detection)

        self.console.print(col_table)

        # Select text column with intelligent default (SAME AS MODE 1)
        if detected_text_col:
            self.console.print(f"\n[cyan]💡 Suggested text column: '{columns_list[detected_text_col-1]}' (detected automatically)[/cyan]")

        text_col_choice = Prompt.ask(
            "\n[cyan]Select TEXT column (to annotate)[/cyan]",
            choices=[str(i) for i in range(1, len(columns_list) + 1)],
            default=str(detected_text_col) if detected_text_col else "1"
        )
        text_column = columns_list[int(text_col_choice) - 1]

        # Select ID column with intelligent default (SAME AS MODE 1)
        identifier_column = None
        if Confirm.ask("\n[cyan]Do you want to select an ID column?[/cyan]", default=True):
            if detected_id_col:
                self.console.print(f"\n[cyan]💡 Suggested ID column: '{columns_list[detected_id_col-1]}' (unique values detected)[/cyan]")

            id_col_choice = Prompt.ask(
                "\n[cyan]Select ID column[/cyan]",
                choices=[str(i) for i in range(1, len(columns_list) + 1)],
                default=str(detected_id_col) if detected_id_col else "1"
            )
            identifier_column = columns_list[int(id_col_choice) - 1]

        self.console.print(f"\n[green]✓ Text column: {text_column}[/green]")
        if identifier_column:
            self.console.print(f"[green]✓ ID column: {identifier_column}[/green]")

        # Store column info for later use
        column_info = {
            'all_columns': columns_list,
            'text_candidates': text_candidates,
            'df': df_sample
        }
    
        # Step 3: Model Selection
        self.console.print("\n[bold]Step 3/7: Model Selection[/bold]")
        self.console.print("[dim]Tested API models: OpenAI & Anthropic[/dim]\n")
    
        selected_llm = self._select_llm_interactive()
        provider = selected_llm.provider
        model_name = selected_llm.name
    
        # Get API key if needed
        api_key = None
        if selected_llm.requires_api_key:
            api_key = self._get_or_prompt_api_key(provider, model_name)
    
        # Step 4: Prompt Configuration
        self.console.print("\n[bold]Step 4/7: Prompt Configuration[/bold]")
    
        # Auto-detect prompts
        detected_prompts = self._detect_prompts_in_folder()
    
        if detected_prompts:
            self.console.print(f"\n[green]✓ Found {len(detected_prompts)} prompts in prompts/ folder:[/green]")
            for i, p in enumerate(detected_prompts, 1):
                # Display ALL keys, not truncated
                keys_str = ', '.join(p['keys'])
                self.console.print(f"  {i}. [cyan]{p['name']}[/cyan]")
                self.console.print(f"     Keys ({len(p['keys'])}): {keys_str}")
    
            # Explain the options clearly
            self.console.print("\n[bold]Prompt Selection Options:[/bold]")
            self.console.print("  [cyan]all[/cyan]     - Use ALL detected prompts (multi-prompt mode)")
            self.console.print("           → Each text will be annotated with all prompts")
            self.console.print("           → Useful when you want complete annotations from all perspectives")
            self.console.print("\n  [cyan]select[/cyan]  - Choose SPECIFIC prompts by number (e.g., 1,3,5)")
            self.console.print("           → Only selected prompts will be used")
            self.console.print("           → Useful when testing or when you need only certain annotations")
            self.console.print("\n  [cyan]wizard[/cyan]  - 🧙‍♂️ Create NEW prompt using Social Science Wizard")
            self.console.print("           → Interactive guided prompt creation")
            self.console.print("           → Optional AI assistance for definitions")
            self.console.print("           → [bold green]Recommended for new research projects![/bold green]")
            self.console.print("\n  [cyan]custom[/cyan]  - Provide path to a prompt file NOT in prompts/ folder")
            self.console.print("           → Use a prompt from another location")
            self.console.print("           → Useful for testing new prompts or one-off annotations")
    
            prompt_choice = Prompt.ask(
                "\n[bold yellow]Prompt selection[/bold yellow]",
                choices=["all", "select", "wizard", "custom"],
                default="all"
            )
    
            selected_prompts = []
            if prompt_choice == "all":
                selected_prompts = detected_prompts
                self.console.print(f"[green]✓ Using all {len(selected_prompts)} prompts[/green]")
            elif prompt_choice == "select":
                indices = Prompt.ask("Enter prompt numbers (comma-separated, e.g., 1,3,5)")
                if indices.strip():  # Only process if not empty
                    for idx_str in indices.split(','):
                        idx_str = idx_str.strip()
                        if idx_str:  # Skip empty strings
                            try:
                                idx = int(idx_str) - 1
                                if 0 <= idx < len(detected_prompts):
                                    selected_prompts.append(detected_prompts[idx])
                            except ValueError:
                                self.console.print(f"[yellow]⚠️  Skipping invalid number: '{idx_str}'[/yellow]")
                if not selected_prompts:
                    self.console.print("[yellow]No valid prompts selected. Using all prompts.[/yellow]")
                    selected_prompts = detected_prompts
                else:
                    self.console.print(f"[green]✓ Selected {len(selected_prompts)} prompts[/green]")
            elif prompt_choice == "wizard":
                # Launch Social Science Wizard
                wizard_prompt = self._run_social_science_wizard()
                from ..annotators.json_cleaner import extract_expected_keys
                keys = extract_expected_keys(wizard_prompt)
                selected_prompts = [{
                    'path': None,  # Wizard-generated, not from file
                    'name': 'wizard_generated',
                    'keys': keys,
                    'content': wizard_prompt
                }]
                self.console.print(f"[green]✓ Using wizard-generated prompt with {len(keys)} keys[/green]")
            else:
                # Custom path
                custom_path = Path(self._prompt_file_path("Prompt file path (.txt)"))
                content = custom_path.read_text(encoding='utf-8')
                from ..annotators.json_cleaner import extract_expected_keys
                keys = extract_expected_keys(content)
                selected_prompts = [{
                    'path': custom_path,
                    'name': custom_path.stem,
                    'keys': keys,
                    'content': content
                }]
        else:
            self.console.print("[yellow]No prompts found in prompts/ folder[/yellow]")
    
            # Offer wizard or custom path
            self.console.print("\n[bold]Prompt Options:[/bold]")
            self.console.print("  [cyan]wizard[/cyan] - 🧙‍♂️ Create prompt using Social Science Wizard (Recommended)")
            self.console.print("  [cyan]custom[/cyan] - Provide path to existing prompt file")
    
            choice = Prompt.ask(
                "\n[bold yellow]Select option[/bold yellow]",
                choices=["wizard", "custom"],
                default="wizard"
            )
    
            if choice == "wizard":
                # Launch Social Science Wizard
                wizard_prompt = self._run_social_science_wizard()
                from ..annotators.json_cleaner import extract_expected_keys
                keys = extract_expected_keys(wizard_prompt)
                selected_prompts = [{
                    'path': None,  # Wizard-generated, not from file
                    'name': 'wizard_generated',
                    'keys': keys,
                    'content': wizard_prompt
                }]
                self.console.print(f"[green]✓ Using wizard-generated prompt with {len(keys)} keys[/green]")
            else:
                custom_path = Path(self._prompt_file_path("Prompt file path (.txt)"))
                content = custom_path.read_text(encoding='utf-8')
                from ..annotators.json_cleaner import extract_expected_keys
                keys = extract_expected_keys(content)
                selected_prompts = [{
                    'path': custom_path,
                    'name': custom_path.stem,
                    'keys': keys,
                    'content': content
                }]
    
    
        # Step 4b: Language Column Detection (FROM QUICK START)
        self.console.print("\n[bold]Step 4b/7: Language Column Detection[/bold]")

        lang_column = None
        available_columns = column_info.get('all_columns', []) if column_info else []
        if available_columns:
            # Detect potential language columns
            potential_lang_cols = [col for col in available_columns
                                  if col.lower() in ['lang', 'language', 'langue', 'lng', 'iso_lang']]
    
            if potential_lang_cols:
                self.console.print(f"\n[bold cyan]🌍 Found language column(s):[/bold cyan]")
                for col in potential_lang_cols:
                    self.console.print(f"  • [cyan]{col}[/cyan]")
    
                use_lang_col = Confirm.ask("Use a language column for training metadata?", default=True)
                if use_lang_col:
                    if len(potential_lang_cols) == 1:
                        lang_column = potential_lang_cols[0]
                        self.console.print(f"[green]✓ Using language column: {lang_column}[/green]")
                    else:
                        lang_column = Prompt.ask(
                            "Which language column to use?",
                            choices=potential_lang_cols,
                            default=potential_lang_cols[0]
                        )
                else:
                    # Ask if automatic language detection is needed
                    auto_detect = Confirm.ask(
                        "[yellow]⚠️  Language information is needed for training. Enable automatic language detection?[/yellow]",
                        default=True
                    )
                    if auto_detect:
                        self.console.print("[dim]Language will be automatically detected for each text during annotation.[/dim]")
                        lang_column = None  # Will trigger auto-detection later
                    else:
                        self.console.print("[yellow]⚠️  Warning: Proceeding without language information may affect training quality.[/yellow]")
                        lang_column = None
            else:
                # No language column detected
                has_lang = Confirm.ask("Does your dataset have a language column?", default=False)
                if has_lang:
                    lang_column = Prompt.ask(
                        "Language column name",
                        choices=available_columns,
                        default=available_columns[0] if available_columns else "language"
                    )
        # Step 5: Multi-prompt prefix configuration
        prompt_configs = []
        if len(selected_prompts) > 1:
            self.console.print("\n[bold]Multi-Prompt Mode:[/bold] Configure key prefixes")
            self.console.print("[dim]Prefixes help identify which prompt generated which keys[/dim]\n")
    
            for i, prompt in enumerate(selected_prompts, 1):
                self.console.print(f"\n[cyan]Prompt {i}: {prompt['name']}[/cyan]")
                self.console.print(f"  Keys: {', '.join(prompt['keys'])}")
    
                add_prefix = Confirm.ask(f"Add prefix to keys for this prompt?", default=True)
                prefix = ""
                if add_prefix:
                    default_prefix = prompt['name'].lower().replace(' ', '_')
                    prefix = Prompt.ask("Prefix", default=default_prefix)
                    self.console.print(f"  [green]Keys will become: {', '.join([f'{prefix}_{k}' for k in prompt['keys'][:3]])}[/green]")
    
                prompt_configs.append({
                    'prompt': prompt,
                    'prefix': prefix
                })
        else:
            # Single prompt - no prefix needed
            prompt_configs = [{'prompt': selected_prompts[0], 'prefix': ''}]
    
        # Step 6: Advanced Options
        self.console.print("\n[bold]Step 5/7: Advanced Options[/bold]")
    
        # ============================================================
        # DATASET SCOPE
        # ============================================================
        self.console.print("\n[bold cyan]📊 Dataset Scope[/bold cyan]")
        self.console.print("[dim]Determine how many rows to annotate from your dataset[/dim]\n")
    
        # Get total rows if possible
        total_rows = None
        if column_info.get('df') is not None:
            # We have a sample, extrapolate
            total_rows = len(pd.read_csv(data_path)) if data_format == 'csv' else None
    
        if total_rows:
            self.console.print(f"[green]✓ Dataset contains {total_rows:,} rows[/green]\n")
    
        # Option 1: Annotate all or limited
        self.console.print("[yellow]Option 1:[/yellow] Annotate ALL rows vs LIMIT to specific number")
        self.console.print("  • [cyan]all[/cyan]   - Annotate the entire dataset")
        self.console.print("           [dim]Use this for production annotations[/dim]")
        self.console.print("  • [cyan]limit[/cyan] - Specify exact number of rows to annotate")
        self.console.print("           [dim]Use this for testing or partial annotation[/dim]")

        scope_choice = Prompt.ask(
            "\nAnnotate entire dataset or limit rows?",
            choices=["all", "limit"],
            default="all"
        )

        annotation_limit = None
        use_sample = False
        sample_strategy = "head"
        recommended_sample = None

        if scope_choice == "limit":
            # Option 2: FIRST ask about representative sample calculation (before asking for number)
            if total_rows and total_rows > 1000:
                self.console.print("\n[yellow]Option 2:[/yellow] Representative Sample Calculation")
                self.console.print("  Calculate statistically representative sample size (95% confidence interval)")
                self.console.print("  [dim]This helps determine the minimum sample needed for statistical validity[/dim]")

                calculate_sample = Confirm.ask("Calculate representative sample size?", default=True)

                if calculate_sample:
                    # Formula: n = (Z² × p × (1-p)) / E²
                    # For 95% CI: Z=1.96, p=0.5 (max variance), E=0.05 (5% margin)
                    import math
                    z = 1.96
                    p = 0.5
                    e = 0.05
                    n_infinite = (z**2 * p * (1-p)) / (e**2)
                    n_adjusted = n_infinite / (1 + ((n_infinite - 1) / total_rows))
                    recommended_sample = int(math.ceil(n_adjusted))

                    self.console.print(f"\n[green]📈 Recommended sample size: {recommended_sample} rows[/green]")
                    self.console.print(f"[dim]   (95% confidence level, 5% margin of error)[/dim]")
                    self.console.print(f"[dim]   Population: {total_rows:,} rows[/dim]\n")

            # THEN ask for specific number (with recommendation as default if calculated)
            default_limit = recommended_sample if recommended_sample else 100
            annotation_limit = self._int_prompt_with_validation(
                f"How many rows to annotate?",
                default=default_limit,
                min_value=1,
                max_value=total_rows if total_rows else 1000000
            )

            # Check if user chose the recommended sample
            if recommended_sample and annotation_limit == recommended_sample:
                use_sample = True

            # Option 3: Random sampling
            self.console.print("\n[yellow]Option 3:[/yellow] Sampling Strategy")
            self.console.print("  Choose how to select the rows to annotate")
            self.console.print("  • [cyan]head[/cyan]   - Take first N rows (faster, sequential)")
            self.console.print("           [dim]Good for testing, preserves order[/dim]")
            self.console.print("  • [cyan]random[/cyan] - Random sample of N rows (representative)")
            self.console.print("           [dim]Better for statistical validity, unbiased[/dim]")
    
            sample_strategy = Prompt.ask(
                "\nSampling strategy",
                choices=["head", "random"],
                default="random" if use_sample else "head"
            )
    
        # ============================================================
        # PARALLEL PROCESSING
        # ============================================================
        self.console.print("\n[bold cyan]⚙️  Parallel Processing[/bold cyan]")
        self.console.print("[dim]Configure how many processes run simultaneously[/dim]\n")
    
        self.console.print("[yellow]Parallel Workers:[/yellow]")
        self.console.print("  Number of simultaneous annotation processes")
        self.console.print("\n  [red]⚠️  IMPORTANT:[/red]")
        self.console.print("  [dim]Most local machines can only handle 1 worker for LLM inference[/dim]")
        self.console.print("  [dim]Parallel processing is mainly useful for API models[/dim]")
        self.console.print("\n  • [cyan]1 worker[/cyan]  - Sequential processing")
        self.console.print("           [dim]Recommended for: Local models (Ollama), first time users, debugging[/dim]")
        self.console.print("  • [cyan]2-4 workers[/cyan] - Moderate parallelism")
        self.console.print("           [dim]Recommended for: API models (OpenAI, Claude) - avoid rate limits[/dim]")
        self.console.print("  • [cyan]4-8 workers[/cyan] - High parallelism")
        self.console.print("           [dim]Recommended for: API models only - requires high rate limits[/dim]")
    
        num_processes = self._int_prompt_with_validation("Parallel workers", 1, 1, 16)
    
        # ============================================================
        # INCREMENTAL SAVE
        # ============================================================
        self.console.print("\n[bold cyan]💾 Incremental Save[/bold cyan]")
        self.console.print("[dim]Configure how often results are saved during annotation[/dim]\n")
    
        self.console.print("[yellow]Enable incremental save?[/yellow]")
        self.console.print("  • [green]Yes[/green] - Save progress regularly during annotation (recommended)")
        self.console.print("           [dim]Protects against crashes, allows resuming, safer for long runs[/dim]")
        self.console.print("  • [red]No[/red]  - Save only at the end")
        self.console.print("           [dim]Faster but risky - you lose everything if process crashes[/dim]")
    
        save_incrementally = Confirm.ask("\n💿 Enable incremental save?", default=True)
    
        # Only ask for batch size if incremental save is enabled
        if save_incrementally:
            self.console.print("\n[yellow]Batch Size:[/yellow]")
            self.console.print("  Number of rows processed between each save")
            self.console.print("  • [cyan]Smaller (1-10)[/cyan]   - Very frequent saves, maximum safety")
            self.console.print("           [dim]Use for: Unstable systems, expensive APIs, testing[/dim]")
            self.console.print("  • [cyan]Medium (10-50)[/cyan]   - Balanced safety and performance")
            self.console.print("           [dim]Use for: Most production cases[/dim]")
            self.console.print("  • [cyan]Larger (50-200)[/cyan]  - Less frequent saves, better performance")
            self.console.print("           [dim]Use for: Stable systems, large datasets, local models[/dim]")
    
            batch_size = self._int_prompt_with_validation("Batch size", 1, 1, 1000)
        else:
            batch_size = None  # Not used when incremental save is disabled
    
        # ============================================================
        # MODEL PARAMETERS
        # ============================================================
        self.console.print("\n[bold cyan]🎛️  Model Parameters[/bold cyan]")
        self.console.print("[dim]Configure advanced model generation parameters[/dim]\n")
    
        # Check if model supports parameter tuning
        model_name_lower = model_name.lower()
        is_o_series = any(x in model_name_lower for x in ['o1', 'o3', 'o4'])
        supports_params = not is_o_series
    
        if not supports_params:
            self.console.print(f"[yellow]⚠️  Model '{model_name}' uses fixed parameters (reasoning model)[/yellow]")
            self.console.print("[dim]   Temperature and top_p are automatically set to 1.0[/dim]")
            configure_params = False
        else:
            self.console.print("[yellow]Configure model parameters?[/yellow]")
            self.console.print("  Adjust how the model generates responses")
            self.console.print("  [dim]• Default values work well for most cases[/dim]")
            self.console.print("  [dim]• Advanced users can fine-tune for specific needs[/dim]")
            configure_params = Confirm.ask("\nConfigure model parameters?", default=False)
    
        # Default values
        temperature = 0.7
        max_tokens = 1000
        top_p = 1.0
        top_k = 40
    
        if configure_params:
            self.console.print("\n[bold]Parameter Explanations:[/bold]\n")
    
            # Temperature
            self.console.print("[cyan]🌡️  Temperature (0.0 - 2.0):[/cyan]")
            self.console.print("  Controls randomness in responses")
            self.console.print("  • [green]Low (0.0-0.3)[/green]  - Deterministic, focused, consistent")
            self.console.print("           [dim]Use for: Structured tasks, factual extraction, classification[/dim]")
            self.console.print("  • [yellow]Medium (0.4-0.9)[/yellow] - Balanced creativity and consistency")
            self.console.print("           [dim]Use for: General annotation, most use cases[/dim]")
            self.console.print("  • [red]High (1.0-2.0)[/red]  - Creative, varied, unpredictable")
            self.console.print("           [dim]Use for: Brainstorming, diverse perspectives[/dim]")
            temperature = FloatPrompt.ask("Temperature", default=0.7)
    
            # Max tokens
            self.console.print("\n[cyan]📏 Max Tokens:[/cyan]")
            self.console.print("  Maximum length of the response")
            self.console.print("  • [green]Short (100-500)[/green]   - Brief responses, simple annotations")
            self.console.print("  • [yellow]Medium (500-2000)[/yellow]  - Standard responses, detailed annotations")
            self.console.print("  • [red]Long (2000+)[/red]     - Extensive responses, complex reasoning")
            self.console.print("  [dim]Note: More tokens = higher API costs[/dim]")
            max_tokens = self._int_prompt_with_validation("Max tokens", 1000, 50, 8000)
    
            # Top_p (nucleus sampling)
            self.console.print("\n[cyan]🎯 Top P (0.0 - 1.0):[/cyan]")
            self.console.print("  Nucleus sampling - alternative to temperature")
            self.console.print("  • [green]Low (0.1-0.5)[/green]  - Focused on most likely tokens")
            self.console.print("           [dim]More deterministic, safer outputs[/dim]")
            self.console.print("  • [yellow]High (0.9-1.0)[/yellow] - Consider broader token range")
            self.console.print("           [dim]More creative, diverse outputs[/dim]")
            self.console.print("  [dim]Tip: Use either temperature OR top_p, not both aggressively[/dim]")
            top_p = FloatPrompt.ask("Top P", default=1.0)
    
            # Top_k (only for some models)
            if provider in ['ollama', 'google']:
                self.console.print("\n[cyan]🔢 Top K:[/cyan]")
                self.console.print("  Limits vocabulary to K most likely next tokens")
                self.console.print("  • [green]Small (1-10)[/green]   - Very focused, repetitive")
                self.console.print("  • [yellow]Medium (20-50)[/yellow]  - Balanced diversity")
                self.console.print("  • [red]Large (50+)[/red]    - Maximum diversity")
                top_k = self._int_prompt_with_validation("Top K", 40, 1, 100)
    
        # Step 7: Execute
        self.console.print("\n[bold]Step 6/7: Review & Execute[/bold]")
    
        # Display comprehensive summary
        summary_table = Table(title="Configuration Summary", border_style="cyan", show_header=True)
        summary_table.add_column("Category", style="bold cyan", width=20)
        summary_table.add_column("Setting", style="yellow", width=25)
        summary_table.add_column("Value", style="white")
    
        # Data section
        summary_table.add_row("📁 Data", "Dataset", str(data_path.name))
        summary_table.add_row("", "Format", data_format.upper())
        summary_table.add_row("", "Text Column", text_column)
        if total_rows:
            summary_table.add_row("", "Total Rows", f"{total_rows:,}")
        if annotation_limit:
            summary_table.add_row("", "Rows to Annotate", f"{annotation_limit:,} ({sample_strategy})")
        else:
            summary_table.add_row("", "Rows to Annotate", "ALL")
    
        # Model section
        summary_table.add_row("🤖 Model", "Provider/Model", f"{provider}/{model_name}")
        summary_table.add_row("", "Temperature", f"{temperature}")
        summary_table.add_row("", "Max Tokens", f"{max_tokens}")
        if configure_params:
            summary_table.add_row("", "Top P", f"{top_p}")
            if provider in ['ollama', 'google']:
                summary_table.add_row("", "Top K", f"{top_k}")
    
        # Prompts section
        summary_table.add_row("📝 Prompts", "Count", f"{len(prompt_configs)}")
        for i, pc in enumerate(prompt_configs, 1):
            prefix_info = f" (prefix: {pc['prefix']}_)" if pc['prefix'] else " (no prefix)"
            summary_table.add_row("", f"  Prompt {i}", f"{pc['prompt']['name']}{prefix_info}")
    
        # Processing section
        summary_table.add_row("⚙️  Processing", "Parallel Workers", str(num_processes))
        summary_table.add_row("", "Batch Size", str(batch_size))
        summary_table.add_row("", "Incremental Save", "Yes" if save_incrementally else "No")
    
        self.console.print("\n")
        self.console.print(summary_table)
    
        if not Confirm.ask("\n[bold yellow]Start annotation?[/bold yellow]", default=True):
            return
    
        # ============================================================
        # REPRODUCIBILITY METADATA
        # ============================================================
        self.console.print("\n[bold cyan]📋 Reproducibility & Metadata[/bold cyan]")
        self.console.print("[yellow]⚠️  IMPORTANT: Save parameters for two critical purposes:[/yellow]\n")
    
        self.console.print("  [green]1. Resume Capability[/green]")
        self.console.print("     • Continue this annotation if it stops or crashes")
        self.console.print("     • Annotate additional rows later with same settings")
        self.console.print("     • Access via 'Resume/Relaunch Annotation' workflow\n")
    
        self.console.print("  [green]2. Scientific Reproducibility[/green]")
        self.console.print("     • Document exact parameters for research papers")
        self.console.print("     • Reproduce identical annotations in the future")
        self.console.print("     • Track model version, prompts, and all settings\n")
    
        self.console.print("  [red]⚠️  If you choose NO:[/red]")
        self.console.print("     • You CANNOT resume this annotation later")
        self.console.print("     • You CANNOT relaunch with same parameters")
        self.console.print("     • Parameters will be lost forever\n")
    
        save_metadata = Confirm.ask(
            "[bold yellow]Save annotation parameters to JSON file?[/bold yellow]",
            default=True
        )
    
        # ============================================================
        # VALIDATION TOOL EXPORT OPTION
        # ============================================================
        self.console.print("\n[bold cyan]📤 Validation Tool Export[/bold cyan]")
        self.console.print("[dim]Export annotations to JSONL format for human validation[/dim]\n")
    
        self.console.print("[yellow]Available validation tools:[/yellow]")
        self.console.print("  • [cyan]Doccano[/cyan] - Simple, lightweight NLP annotation tool")
        self.console.print("  • [cyan]Label Studio[/cyan] - Advanced, feature-rich annotation platform")
        self.console.print("  • Both are open-source and free\n")
    
        self.console.print("[green]Why validate with external tools?[/green]")
        self.console.print("  • Review and correct LLM annotations")
        self.console.print("  • Calculate inter-annotator agreement")
        self.console.print("  • Export validated data for metrics calculation\n")
    
        # Initialize export flags
        export_to_doccano = False
        export_to_labelstudio = False
        export_sample_size = None
    
        # Step 1: Ask if user wants to export
        export_confirm = Confirm.ask(
            "[bold yellow]Export to validation tool?[/bold yellow]",
            default=False
        )
    
        if export_confirm:
            # Step 2: Ask which tool to export to
            tool_choice = Prompt.ask(
                "[bold yellow]Which validation tool?[/bold yellow]",
                choices=["doccano", "labelstudio"],
                default="doccano"
            )
    
            # Set the appropriate export flag
            if tool_choice == "doccano":
                export_to_doccano = True
            else:  # labelstudio
                export_to_labelstudio = True
    
            # Step 2b: If Label Studio, ask export method
            labelstudio_direct_export = False
            labelstudio_api_url = None
            labelstudio_api_key = None
    
            if export_to_labelstudio:
                self.console.print("\n[yellow]Label Studio export method:[/yellow]")
                self.console.print("  • [cyan]jsonl[/cyan] - Export to JSONL file (manual import)")
                if HAS_REQUESTS:
                    self.console.print("  • [cyan]direct[/cyan] - Direct export to Label Studio via API\n")
                    export_choices = ["jsonl", "direct"]
                else:
                    self.console.print("  • [dim]direct[/dim] - Direct export via API [dim](requires 'requests' library)[/dim]\n")
                    export_choices = ["jsonl"]
    
                export_method = Prompt.ask(
                    "[bold yellow]Export method[/bold yellow]",
                    choices=export_choices,
                    default="jsonl"
                )
    
                if export_method == "direct":
                    labelstudio_direct_export = True
    
                    self.console.print("\n[cyan]Label Studio API Configuration:[/cyan]")
                    labelstudio_api_url = Prompt.ask(
                        "Label Studio URL",
                        default="http://localhost:8080"
                    )
    
                    labelstudio_api_key = Prompt.ask(
                        "API Key (from Label Studio Account & Settings)"
                    )
    
            # Step 3: Ask about LLM predictions inclusion
            self.console.print("\n[yellow]Include LLM predictions in export?[/yellow]")
            self.console.print("  • [cyan]with[/cyan] - Include LLM annotations as predictions (for review/correction)")
            self.console.print("  • [cyan]without[/cyan] - Export only data without predictions (for manual annotation)")
            self.console.print("  • [cyan]both[/cyan] - Create two files: one with and one without predictions\n")
    
            prediction_mode = Prompt.ask(
                "[bold yellow]Prediction mode[/bold yellow]",
                choices=["with", "without", "both"],
                default="with"
            )
    
            # Step 4: Ask how many sentences to export
            self.console.print("\n[yellow]How many annotated sentences to export?[/yellow]")
            self.console.print("  • [cyan]all[/cyan] - Export all annotated sentences")
            self.console.print("  • [cyan]representative[/cyan] - Representative sample (stratified by labels)")
            self.console.print("  • [cyan]number[/cyan] - Specify exact number\n")
    
            sample_choice = Prompt.ask(
                "[bold yellow]Export sample[/bold yellow]",
                choices=["all", "representative", "number"],
                default="all"
            )
    
            if sample_choice == "all":
                export_sample_size = "all"
            elif sample_choice == "representative":
                export_sample_size = "representative"
            else:  # number
                export_sample_size = self._int_prompt_with_validation(
                    "Number of sentences to export",
                    100,
                    1,
                    999999
                )
    
        # ============================================================
        # EXECUTE ANNOTATION
        # ============================================================
    
        # CRITICAL: Use new organized structure with dataset-specific subfolder
        # Structure: logs/annotator/{session_id}/annotated_data/{dataset_name}/
        safe_model_name = model_name.replace(':', '_').replace('/', '_')
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # Create dataset-specific subdirectory (like {category} in Training Arena)
        dataset_name = data_path.stem
        dataset_subdir = session_dirs['annotated_data'] / dataset_name
        dataset_subdir.mkdir(parents=True, exist_ok=True)

        output_filename = f"{data_path.stem}_{safe_model_name}_annotations_{timestamp}.{data_format}"
        default_output_path = dataset_subdir / output_filename
    
        self.console.print(f"\n[bold cyan]📁 Output Location:[/bold cyan]")
        self.console.print(f"   {default_output_path}")
        self.console.print()
    
        # Prepare prompts payload for pipeline
        prompts_payload = []
        for pc in prompt_configs:
            prompts_payload.append({
                'prompt': pc['prompt']['content'],
                'expected_keys': pc['prompt']['keys'],
                'prefix': pc['prefix']
            })
    
        # Determine annotation mode
        annotation_mode = 'api' if provider in {'openai', 'anthropic', 'google'} else 'local'
    
        # Build pipeline config
        pipeline_config = {
            'mode': 'file',
            'data_source': data_format,
            'data_format': data_format,
            'file_path': str(data_path),
            'text_column': text_column,
            'text_columns': [text_column],
            'annotation_column': 'annotation',
            'identifier_column': identifier_column,  # From Step 2b: User-selected ID strategy
            'run_annotation': True,
            'annotation_mode': annotation_mode,
            'annotation_provider': provider,
            'annotation_model': model_name,
            'api_key': api_key if api_key else None,
            'prompts': prompts_payload,
            'annotation_sample_size': annotation_limit,
            'annotation_sampling_strategy': sample_strategy if annotation_limit else 'head',
            'annotation_sample_seed': 42,
            'max_tokens': max_tokens,
            'temperature': temperature,
            'top_p': top_p,
            'top_k': top_k if provider in ['ollama', 'google'] else None,
            'max_workers': num_processes,
            'num_processes': num_processes,
            'use_parallel': num_processes > 1,
            'warmup': False,
            'disable_tqdm': True,  # Use Rich progress instead
            'output_format': data_format,
            'output_path': str(default_output_path),
            'save_incrementally': save_incrementally,
            'batch_size': batch_size,
            'run_validation': False,
            'run_training': False,
            'lang_column': lang_column,  # From Step 4b: Language column for training metadata
        }
    
        # Add model-specific options
        if provider == 'ollama':
            options = {
                'temperature': temperature,
                'num_predict': max_tokens,
                'top_p': top_p,
                'top_k': top_k
            }
            pipeline_config['options'] = options
    
        # ============================================================
        # SAVE REPRODUCIBILITY METADATA
        # ============================================================
        if save_metadata:
            import json
    
            # Build comprehensive metadata
            metadata = {
                'annotation_session': {
                    'timestamp': timestamp,
                    'tool_version': 'LLMTool v1.0',
                    'workflow': 'The Annotator - Smart Annotate'
                },
                'data_source': {
                    'file_path': str(data_path),
                    'file_name': data_path.name,
                    'data_format': data_format,
                    'text_column': text_column,
                    'total_rows': annotation_limit if annotation_limit else 'all',
                    'sampling_strategy': sample_strategy if annotation_limit else 'none (all rows)',
                    'sample_seed': 42 if sample_strategy == 'random' else None
                },
                'model_configuration': {
                    'provider': provider,
                    'model_name': model_name,
                    'annotation_mode': annotation_mode,
                    'temperature': temperature,
                    'max_tokens': max_tokens,
                    'top_p': top_p,
                    'top_k': top_k if provider in ['ollama', 'google'] else None
                },
                'prompts': [
                    {
                        'name': pc['prompt']['name'],
                        'file_path': str(pc['prompt']['path']) if 'path' in pc['prompt'] else None,
                        'expected_keys': pc['prompt']['keys'],
                        'prefix': pc['prefix'],
                        'prompt_content': pc['prompt']['content']
                    }
                    for pc in prompt_configs
                ],
                'processing_configuration': {
                    'parallel_workers': num_processes,
                    'batch_size': batch_size,
                    'incremental_save': save_incrementally,
                    'identifier_column': 'annotation_id'
                },
                'output': {
                    'output_path': str(default_output_path),
                    'output_format': data_format
                },
                'export_preferences': {
                    'export_to_doccano': export_to_doccano,
                    'export_to_labelstudio': export_to_labelstudio,
                    'export_sample_size': export_sample_size,
                    'prediction_mode': prediction_mode if (export_to_doccano or export_to_labelstudio) else 'with',
                    'labelstudio_direct_export': labelstudio_direct_export if export_to_labelstudio else False,
                    'labelstudio_api_url': labelstudio_api_url if export_to_labelstudio else None,
                    'labelstudio_api_key': labelstudio_api_key if export_to_labelstudio else None
                },
                'training_workflow': {
                    'enabled': False,  # Will be updated after training workflow
                    'training_params_file': None,  # Will be added after training
                    'note': 'Training parameters will be saved separately after annotation completes'
                }
            }

            # Save metadata JSON (PRE-ANNOTATION SAVE POINT 1)
            # Use dataset-specific subdirectory for metadata too
            metadata_subdir = session_dirs['metadata'] / dataset_name
            metadata_subdir.mkdir(parents=True, exist_ok=True)

            metadata_filename = f"{data_path.stem}_{safe_model_name}_metadata_{timestamp}.json"
            metadata_path = metadata_subdir / metadata_filename

            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)

            self.console.print(f"\n[bold green]✅ Metadata saved for reproducibility[/bold green]")
            self.console.print(f"[bold cyan]📋 Metadata File:[/bold cyan]")
            self.console.print(f"   {metadata_path}\n")
    
        # Execute pipeline with Rich progress
        try:
            self.console.print("\n[bold green]🚀 Starting annotation...[/bold green]\n")
    
            # Create pipeline controller
            from ..pipelines.pipeline_controller import PipelineController
            pipeline_with_progress = PipelineController(settings=self.settings)
    
            # Use RichProgressManager for elegant display
            from ..utils.rich_progress_manager import RichProgressManager
            from ..pipelines.enhanced_pipeline_wrapper import EnhancedPipelineWrapper
    
            with RichProgressManager(
                show_json_every=1,  # Show JSON sample for every annotation
                compact_mode=False   # Full preview panels
            ) as progress_manager:
                # Wrap pipeline for enhanced JSON tracking
                enhanced_pipeline = EnhancedPipelineWrapper(
                    pipeline_with_progress,
                    progress_manager
                )
    
                # Run pipeline
                state = enhanced_pipeline.run_pipeline(pipeline_config)
    
                # Check for errors
                if state.errors:
                    error_msg = state.errors[0]['error'] if state.errors else "Annotation failed"
                    self.console.print(f"\n[bold red]❌ Error:[/bold red] {error_msg}")
                    self.console.print("[dim]Press Enter to return to menu...[/dim]")
                    input()
                    return
    
            # Get results
            annotation_results = state.annotation_results or {}
            output_file = annotation_results.get('output_file', str(default_output_path))

            # Display success message
            self.console.print("\n[bold green]✅ Annotation completed successfully![/bold green]")
            self.console.print(f"\n[bold cyan]📄 Output File:[/bold cyan]")
            self.console.print(f"   {output_file}")

            # Display statistics if available
            total_annotated = annotation_results.get('total_annotated', 0)
            if total_annotated:
                self.console.print(f"\n[bold cyan]📊 Statistics:[/bold cyan]")
                self.console.print(f"   Rows annotated: {total_annotated:,}")

                success_count = annotation_results.get('success_count', 0)
                if success_count:
                    success_rate = (success_count / total_annotated * 100)
                    self.console.print(f"   Success rate: {success_rate:.1f}%")

            # ============================================================
            # AUTOMATIC LANGUAGE DETECTION (if no language column provided)
            # ============================================================
            if not lang_column:
                self.console.print("\n[bold cyan]🌍 Language Detection for Training[/bold cyan]")
                self.console.print("[yellow]No language column was provided. Detecting languages for training...[/yellow]\n")

                try:
                    import pandas as pd
                    from llm_tool.utils.language_detector import LanguageDetector

                    # Load annotated file
                    df_annotated = pd.read_csv(output_file)

                    # CRITICAL: Only detect languages for ANNOTATED rows
                    # The output file may contain ALL original rows, but we only want to detect
                    # languages for rows that were actually annotated
                    original_row_count = len(df_annotated)

                    # Try to identify annotated rows by checking for annotation columns
                    # Common annotation column names: 'label', 'category', 'annotation', 'labels'
                    annotation_cols = [col for col in df_annotated.columns if col in ['label', 'labels', 'category', 'annotation', 'predicted_label']]

                    if annotation_cols:
                        # Filter to only rows that have annotations (non-null in annotation column)
                        annotation_col = annotation_cols[0]
                        df_annotated = df_annotated[df_annotated[annotation_col].notna()].copy()
                        self.console.print(f"[dim]Filtering to {len(df_annotated):,} annotated rows (out of {original_row_count:,} total rows in file)[/dim]")
                    else:
                        self.console.print(f"[yellow]⚠️  Could not identify annotation column. Processing all {original_row_count:,} rows.[/yellow]")

                    if len(df_annotated) == 0:
                        self.console.print("[yellow]⚠️  No annotated rows found. Skipping language detection.[/yellow]")
                    elif text_column in df_annotated.columns:
                        # Get ALL texts (including NaN) to maintain index alignment
                        all_texts = df_annotated[text_column].tolist()

                        # Count non-empty texts for display
                        non_empty_texts = sum(1 for text in all_texts if pd.notna(text) and len(str(text).strip()) > 10)

                        if non_empty_texts > 0:
                            detector = LanguageDetector()
                            detected_languages = []

                            # Progress indicator
                            from tqdm import tqdm
                            self.console.print(f"[dim]Analyzing {non_empty_texts} texts...[/dim]")

                            for text in tqdm(all_texts, desc="Detecting languages", disable=not HAS_RICH):
                                # Handle NaN and empty texts
                                if pd.isna(text) or not text or len(str(text).strip()) <= 10:
                                    detected_languages.append('unknown')
                                else:
                                    try:
                                        detected = detector.detect(str(text))
                                        if detected and detected.get('language'):
                                            detected_languages.append(detected['language'])
                                        else:
                                            detected_languages.append('unknown')
                                    except Exception as e:
                                        self.logger.debug(f"Language detection failed for text: {e}")
                                        detected_languages.append('unknown')

                            # Add language column to the filtered dataframe
                            df_annotated['lang'] = detected_languages

                            # Reload the FULL original file and update only the annotated rows
                            df_full = pd.read_csv(output_file)

                            # Initialize lang column if it doesn't exist
                            if 'lang' not in df_full.columns:
                                df_full['lang'] = 'unknown'

                            # Update language for annotated rows only
                            # Match by index of df_annotated within df_full
                            df_full.loc[df_annotated.index, 'lang'] = df_annotated['lang'].values

                            # Save updated full file with language column
                            df_full.to_csv(output_file, index=False)

                            # Show distribution
                            lang_counts = {}
                            for lang in detected_languages:
                                if lang != 'unknown':
                                    lang_counts[lang] = lang_counts.get(lang, 0) + 1

                            if lang_counts:
                                total = sum(lang_counts.values())
                                self.console.print(f"\n[bold]🌍 Languages Detected ({total:,} texts):[/bold]")

                                lang_table = Table(border_style="cyan", show_header=True, header_style="bold")
                                lang_table.add_column("Language", style="cyan", width=12)
                                lang_table.add_column("Count", style="yellow", justify="right", width=12)
                                lang_table.add_column("Percentage", style="green", justify="right", width=12)

                                for lang, count in sorted(lang_counts.items(), key=lambda x: x[1], reverse=True):
                                    percentage = (count / total * 100) if total > 0 else 0
                                    lang_table.add_row(
                                        lang.upper(),
                                        f"{count:,}",
                                        f"{percentage:.1f}%"
                                    )

                                self.console.print(lang_table)
                                self.console.print(f"\n[green]✓ Language column 'lang' added to {output_file}[/green]")
                            else:
                                self.console.print("[yellow]⚠️  No languages detected successfully[/yellow]")

                except Exception as e:
                    self.console.print(f"[yellow]⚠️  Language detection failed: {e}[/yellow]")
                    self.logger.exception("Language detection failed")

            # ============================================================
            # INTELLIGENT TRAINING WORKFLOW (Post-Annotation)
            # ============================================================
            self._post_annotation_training_workflow(
                output_file=output_file,
                text_column=text_column,
                prompt_configs=prompt_configs
            )

            # Export to Doccano JSONL if requested
            if export_to_doccano:
                self._export_to_doccano_jsonl(
                    output_file=output_file,
                    text_column=text_column,
                    prompt_configs=prompt_configs,
                    data_path=data_path,
                    timestamp=timestamp,
                    sample_size=export_sample_size,
                    session_dirs=session_dirs
                )
    
            # Export to Label Studio if requested
            if export_to_labelstudio:
                if labelstudio_direct_export:
                    # Direct export to Label Studio via API
                    self._export_to_labelstudio_direct(
                        output_file=output_file,
                        text_column=text_column,
                        prompt_configs=prompt_configs,
                        data_path=data_path,
                        timestamp=timestamp,
                        sample_size=export_sample_size,
                        prediction_mode=prediction_mode,
                        api_url=labelstudio_api_url,
                        api_key=labelstudio_api_key
                    )
                else:
                    # Export to JSONL file
                    self._export_to_labelstudio_jsonl(
                        output_file=output_file,
                        text_column=text_column,
                        prompt_configs=prompt_configs,
                        data_path=data_path,
                        timestamp=timestamp,
                        sample_size=export_sample_size,
                        prediction_mode=prediction_mode,
                        session_dirs=session_dirs
                    )
    
            self.console.print("\n[dim]Press Enter to return to menu...[/dim]")
            input()
    
        except Exception as exc:
            self.console.print(f"\n[bold red]❌ Annotation failed:[/bold red] {exc}")
            self.logger.exception("Annotation execution failed")
            self.console.print("\n[dim]Press Enter to return to menu...[/dim]")
            input()

    def _resume_mode2(self):
        """Resume or relaunch annotation → training workflow using saved parameters"""
        self.console.print("\n[bold cyan]🔄 Resume/Relaunch Workflow[/bold cyan]\n")
        self.console.print("[dim]Load saved parameters from previous annotation → training sessions[/dim]\n")

        # ============================================================
        # DETECT METADATA FILES
        # ============================================================
        annotations_dir = self.settings.paths.data_dir / 'annotations'

        if not annotations_dir.exists():
            self.console.print("[yellow]No annotations directory found.[/yellow]")
            self.console.print("[dim]Run Complete Workflow first to create annotation sessions.[/dim]")
            self.console.print("\n[dim]Press Enter to continue...[/dim]")
            input()
            return

        # Find all metadata JSON files
        metadata_files = list(annotations_dir.glob("*_metadata_*.json"))

        if not metadata_files:
            self.console.print("[yellow]No saved workflow parameters found.[/yellow]")
            self.console.print("[dim]Run Complete Workflow and save parameters to use this feature.[/dim]")
            self.console.print("\n[dim]Press Enter to continue...[/dim]")
            input()
            return

        # Sort by modification time (most recent first)
        metadata_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)

        # Display available sessions
        self.console.print(f"[green]Found {len(metadata_files)} saved workflow session(s)[/green]\n")

        sessions_table = Table(border_style="cyan", show_header=True)
        sessions_table.add_column("#", style="cyan", width=3)
        sessions_table.add_column("Session", style="white")
        sessions_table.add_column("Date", style="yellow")
        sessions_table.add_column("Workflow", style="green")
        sessions_table.add_column("Model", style="magenta")

        import json
        from datetime import datetime

        # Load and display sessions
        valid_sessions = []
        for i, mf in enumerate(metadata_files[:20], 1):  # Show max 20 most recent
            try:
                with open(mf, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)

                session_info = metadata.get('annotation_session', {})
                model_config = metadata.get('model_configuration', {})

                timestamp_str = session_info.get('timestamp', '')
                try:
                    dt = datetime.strptime(timestamp_str, '%Y%m%d_%H%M%S')
                    date_str = dt.strftime('%Y-%m-%d %H:%M')
                except:
                    date_str = timestamp_str

                workflow = session_info.get('workflow', 'Unknown')
                model_name = model_config.get('model_name', 'Unknown')

                sessions_table.add_row(
                    str(i),
                    mf.stem[:40],
                    date_str,
                    workflow.split(' - ')[0] if ' - ' in workflow else workflow,
                    model_name
                )

                valid_sessions.append((mf, metadata))
            except Exception as e:
                self.logger.warning(f"Could not load metadata file {mf}: {e}")
                continue

        if not valid_sessions:
            self.console.print("[yellow]No valid metadata files found.[/yellow]")
            self.console.print("\n[dim]Press Enter to continue...[/dim]")
            input()
            return

        self.console.print(sessions_table)

        # Select session
        session_choice = self._int_prompt_with_validation(
            "\n[bold yellow]Select session to resume/relaunch[/bold yellow]",
            1, 1, len(valid_sessions)
        )

        selected_file, metadata = valid_sessions[session_choice - 1]

        self.console.print(f"\n[green]✓ Selected: {selected_file.name}[/green]")

        # ============================================================
        # DISPLAY ALL PARAMETERS
        # ============================================================
        self._display_metadata_parameters(metadata)

        # ============================================================
        # ASK: RESUME OR RELAUNCH?
        # ============================================================
        self.console.print("\n[bold cyan]📋 Action Mode[/bold cyan]\n")
        self.console.print("[yellow]What would you like to do?[/yellow]")
        self.console.print("  • [cyan]resume[/cyan]   - Continue an incomplete workflow (skip already completed steps)")
        self.console.print("           [dim]Requires output files from annotation and/or training[/dim]")
        self.console.print("  • [cyan]relaunch[/cyan] - Start a new workflow with same parameters")
        self.console.print("           [dim]Runs a fresh annotation → training session[/dim]")

        action_mode = Prompt.ask(
            "\n[bold yellow]Select action[/bold yellow]",
            choices=["resume", "relaunch"],
            default="relaunch"
        )

        # ============================================================
        # ASK: MODIFY PARAMETERS?
        # ============================================================
        self.console.print("\n[bold cyan]⚙️  Parameter Modification[/bold cyan]\n")

        modify_params = Confirm.ask(
            "Do you want to modify any parameters?",
            default=False
        )

        # Extract and potentially modify parameters
        modified_metadata = self._modify_parameters_if_requested(metadata, modify_params)

        # ============================================================
        # EXECUTE WORKFLOW
        # ============================================================
        self._execute_mode2_from_metadata(modified_metadata, action_mode, selected_file)

        self.console.print("\n[dim]Press Enter to return to menu...[/dim]")
        input()

    def _execute_mode2_from_metadata(self, metadata: dict, action_mode: str, metadata_file: Path):
        """Execute annotation → training workflow based on loaded metadata

        This supports both annotation and training phases:
        - Resume: Skip completed annotation, optionally skip completed training
        - Relaunch: Re-run both annotation and training with same parameters
        """
        import json
        from datetime import datetime

        # Extract all parameters from metadata
        data_source = metadata.get('data_source', {})
        model_config = metadata.get('model_configuration', {})
        prompts = metadata.get('prompts', [])
        proc_config = metadata.get('processing_configuration', {})
        output_config = metadata.get('output', {})
        export_prefs = metadata.get('export_preferences', {})
        training_workflow = metadata.get('training_workflow', {})

        # Get export preferences
        export_to_doccano = export_prefs.get('export_to_doccano', False)
        export_to_labelstudio = export_prefs.get('export_to_labelstudio', False)
        export_sample_size = export_prefs.get('export_sample_size', 'all')

        # Prepare paths
        data_path = Path(data_source.get('file_path', ''))
        data_format = data_source.get('data_format', 'csv')
        text_column = data_source.get('text_column', 'text')

        # Check what phases are completed
        annotation_complete = False
        training_complete = False

        if action_mode == 'resume':
            # Try to find annotation output file
            original_output = Path(output_config.get('output_path', ''))

            if original_output.exists():
                self.console.print(f"\n[green]✓ Found annotation output: {original_output.name}[/green]")
                annotation_complete = True

                # Count already annotated rows
                import pandas as pd
                try:
                    if data_format == 'csv':
                        df_output = pd.read_csv(original_output)
                    elif data_format in ['excel', 'xlsx']:
                        df_output = pd.read_excel(original_output)
                    elif data_format == 'parquet':
                        df_output = pd.read_parquet(original_output)

                    # Count rows with valid annotations
                    if 'annotation' in df_output.columns:
                        annotated_mask = (
                            df_output['annotation'].notna() &
                            (df_output['annotation'].astype(str).str.strip() != '') &
                            (df_output['annotation'].astype(str) != 'nan')
                        )
                        annotated_count = annotated_mask.sum()
                        self.console.print(f"[cyan]  Rows already annotated: {annotated_count:,}[/cyan]")
                except Exception as e:
                    self.logger.warning(f"Could not read output file: {e}")
                    annotation_complete = False

            # Check if training was completed
            training_params_file = training_workflow.get('training_params_file')
            if training_params_file and Path(training_params_file).exists():
                self.console.print(f"[green]✓ Found training parameters: {Path(training_params_file).name}[/green]")

                # Check if model was saved
                model_dir = Path(original_output).parent / f"{Path(original_output).stem}_model"
                if model_dir.exists():
                    self.console.print(f"[green]✓ Found trained model: {model_dir.name}[/green]")
                    training_complete = True

        # Determine what to run
        run_annotation = not annotation_complete or action_mode == 'relaunch'
        run_training = not training_complete or action_mode == 'relaunch'

        if action_mode == 'resume' and annotation_complete and training_complete:
            self.console.print("\n[yellow]✓ Both annotation and training are already complete![/yellow]")
            retry_training = Confirm.ask("Re-run training with same annotations?", default=False)
            if retry_training:
                run_annotation = False
                run_training = True
            else:
                self.console.print("[yellow]Nothing to do. Workflow is complete.[/yellow]")
                return

        # ============================================================
        # PHASE 1: ANNOTATION (if needed)
        # ============================================================
        if run_annotation:
            self.console.print("\n[bold cyan]📝 Phase 1: LLM Annotation[/bold cyan]\n")

            # CRITICAL: Use organized structure logs/annotator_factory/{session_id}/annotated_data/{dataset_name}/
            safe_model_name = model_config.get('model_name', 'unknown').replace(':', '_').replace('/', '_')
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

            # Create dataset-specific subdirectory
            dataset_name = data_path.stem
            dataset_subdir = session_dirs['annotated_data'] / dataset_name
            dataset_subdir.mkdir(parents=True, exist_ok=True)

            output_filename = f"{data_path.stem}_{safe_model_name}_annotations_{timestamp}.{data_format}"
            output_path = dataset_subdir / output_filename

            # Build pipeline config (same as Mode 1)
            provider = model_config.get('provider', 'ollama')
            model_name = model_config.get('model_name', 'llama2')

            # Get API key if needed
            api_key = None
            if provider in ['openai', 'anthropic', 'google']:
                api_key = self._get_api_key(provider)
                if not api_key:
                    self.console.print(f"[red]API key required for {provider}[/red]")
                    return

            # Prepare prompts payload
            prompts_payload = []
            for p in prompts:
                prompts_payload.append({
                    'prompt': p.get('prompt_content', p.get('prompt', '')),
                    'expected_keys': p.get('expected_keys', []),
                    'prefix': p.get('prefix', '')
                })

            pipeline_config = {
                'mode': 'file',
                'data_source': data_format,
                'data_format': data_format,
                'file_path': str(data_path),
                'text_column': text_column,
                'text_columns': [text_column],
                'annotation_column': 'annotation',
                'identifier_column': 'annotation_id',
                'run_annotation': True,
                'annotation_mode': model_config.get('annotation_mode', 'local'),
                'annotation_provider': provider,
                'annotation_model': model_name,
                'api_key': api_key,
                'prompts': prompts_payload,
                'annotation_sample_size': data_source.get('total_rows'),
                'annotation_sampling_strategy': data_source.get('sampling_strategy', 'head'),
                'max_tokens': model_config.get('max_tokens', 1000),
                'temperature': model_config.get('temperature', 0.7),
                'top_p': model_config.get('top_p', 1.0),
                'max_workers': proc_config.get('parallel_workers', 1),
                'output_format': data_format,
                'output_path': str(output_path),
                'save_incrementally': True,
                'batch_size': proc_config.get('batch_size', 1),
                'run_validation': False,
                'run_training': False,
            }

            # Execute annotation
            try:
                self.console.print("\n[bold green]🚀 Starting annotation...[/bold green]\n")

                from ..pipelines.pipeline_controller import PipelineController
                from ..utils.rich_progress_manager import RichProgressManager
                from ..pipelines.enhanced_pipeline_wrapper import EnhancedPipelineWrapper

                pipeline_with_progress = PipelineController(settings=self.settings)

                with RichProgressManager(show_json_every=1, compact_mode=False) as progress_manager:
                    enhanced_pipeline = EnhancedPipelineWrapper(pipeline_with_progress, progress_manager)
                    state = enhanced_pipeline.run_pipeline(pipeline_config)

                    if state.errors:
                        error_msg = state.errors[0]['error'] if state.errors else "Annotation failed"
                        self.console.print(f"\n[bold red]❌ Error:[/bold red] {error_msg}")
                        return

                annotation_results = state.annotation_results or {}
                output_file = annotation_results.get('output_file', str(output_path))

                self.console.print("\n[bold green]✅ Annotation completed successfully![/bold green]")
                self.console.print(f"[bold cyan]📄 Output File:[/bold cyan] {output_file}\n")

            except Exception as exc:
                self.console.print(f"\n[bold red]❌ Annotation failed:[/bold red] {exc}")
                self.logger.exception("Mode 2 annotation failed")
                return

        else:
            # Use existing annotation file
            output_file = str(original_output)
            self.console.print(f"\n[yellow]⏭️  Skipping annotation (already complete)[/yellow]")
            self.console.print(f"[cyan]Using existing file: {output_file}[/cyan]\n")

        # ============================================================
        # LANGUAGE DETECTION (if not already done)
        # ============================================================
        self.console.print("\n[bold cyan]🌍 Language Detection for Training[/bold cyan]")
        self.console.print("[yellow]Checking for language column...[/yellow]\n")

        try:
            import pandas as pd
            df_for_lang = pd.read_csv(output_file)

            # Check if language detection already done
            if 'lang' not in df_for_lang.columns:
                self.console.print("[yellow]No language column found. Detecting languages...[/yellow]\n")

                # Get annotated rows only
                annotation_cols = [col for col in df_for_lang.columns if col in ['label', 'labels', 'category', 'annotation', 'predicted_label']]
                if annotation_cols:
                    annotation_col = annotation_cols[0]
                    df_annotated = df_for_lang[df_for_lang[annotation_col].notna()].copy()

                    self.console.print(f"[dim]Filtering to {len(df_annotated):,} annotated rows[/dim]")

                    # Detect languages
                    from llm_tool.utils.language_detector import LanguageDetector
                    from tqdm import tqdm

                    detector = LanguageDetector()
                    all_texts = df_annotated[text_column].tolist()
                    detected_languages = []

                    non_empty_texts = sum(1 for t in all_texts if pd.notna(t) and str(t).strip() and len(str(t).strip()) > 10)
                    self.console.print(f"[dim]Analyzing {non_empty_texts} texts...[/dim]")

                    for text in tqdm(all_texts, desc="Detecting languages", disable=not HAS_RICH):
                        if pd.isna(text) or not text or len(str(text).strip()) <= 10:
                            detected_languages.append('unknown')
                        else:
                            try:
                                detected = detector.detect(str(text))
                                if detected and detected.get('language'):
                                    detected_languages.append(detected['language'])
                                else:
                                    detected_languages.append('unknown')
                            except Exception as e:
                                self.logger.debug(f"Language detection failed for text: {e}")
                                detected_languages.append('unknown')

                    # Add language column
                    df_annotated['lang'] = detected_languages

                    # Update full file
                    df_full = pd.read_csv(output_file)
                    if 'lang' not in df_full.columns:
                        df_full['lang'] = 'unknown'
                    df_full.loc[df_annotated.index, 'lang'] = df_annotated['lang'].values
                    df_full.to_csv(output_file, index=False)

                    # Display language distribution
                    lang_counts = df_annotated['lang'].value_counts()
                    lang_counts_filtered = {k: v for k, v in lang_counts.items() if k != 'unknown'}

                    if lang_counts_filtered:
                        total = sum(lang_counts_filtered.values())
                        self.console.print(f"\n[bold]🌍 Languages Detected ({total:,} texts):[/bold]")

                        lang_table = Table(border_style="cyan", show_header=True, header_style="bold")
                        lang_table.add_column("Language", style="cyan", width=12)
                        lang_table.add_column("Count", style="yellow", justify="right", width=12)
                        lang_table.add_column("Percentage", style="green", justify="right", width=12)

                        for lang, count in sorted(lang_counts_filtered.items(), key=lambda x: x[1], reverse=True):
                            pct = (count / total * 100)
                            lang_table.add_row(lang.upper(), f"{count:,}", f"{pct:.1f}%")

                        self.console.print(lang_table)
                        self.console.print(f"\n[green]✓ Language column 'lang' added to output file[/green]\n")
            else:
                self.console.print("[green]✓ Language column already exists[/green]\n")

        except Exception as e:
            self.console.print(f"[yellow]⚠️  Language detection failed: {e}[/yellow]")
            self.logger.exception("Language detection failed")

        # ============================================================
        # PHASE 2: TRAINING (if needed)
        # ============================================================
        if run_training:
            self.console.print("\n[bold cyan]🎓 Phase 2: Model Training[/bold cyan]\n")

            # Build prompt_configs for training workflow
            prompt_configs_for_training = []
            for p in prompts:
                prompt_configs_for_training.append({
                    'prompt': {
                        'keys': p.get('expected_keys', []),
                        'content': p.get('prompt_content', p.get('prompt', '')),
                        'name': p.get('name', 'prompt')
                    },
                    'prefix': p.get('prefix', '')
                })

            # Execute training workflow
            self._post_annotation_training_workflow(
                output_file=output_file,
                text_column=text_column,
                prompt_configs=prompt_configs_for_training
            )

        else:
            self.console.print(f"\n[yellow]⏭️  Skipping training (already complete)[/yellow]\n")

        self.console.print("\n[bold green]✅ Workflow complete![/bold green]")

    def _post_annotation_training_workflow(
        self,
        output_file: str,
        text_column: str,
        prompt_configs: list
    ):
        """
        Comprehensive post-annotation training workflow.
        Inspired by Training Arena (Mode 3) but adapted for annotated data.

        Features:
        - Language detection & analysis (with low-percentage reclassification)
        - Text length analysis for long-document model recommendation
        - Model recommendation based on languages & text length
        - Training strategy selection (multilingual/specialized/hybrid)
        - Benchmark mode option for specific categories
        - Full training pipeline integration
        """
        try:
            self.console.print("\n[bold cyan]🎓 Post-Annotation Training[/bold cyan]")
            self.console.print("[dim]Intelligent model training from your LLM annotations[/dim]\n")

            # Ask if user wants to train a model
            train_model = Confirm.ask(
                "[bold]Would you like to train a classifier model from these annotations?[/bold]",
                default=True
            )

            if not train_model:
                self.console.print("[yellow]Skipping training. Annotations are ready for manual use or export.[/yellow]")
                return

            # Load annotated data
            import pandas as pd
            df = pd.read_csv(output_file)

            # Filter to annotated rows only
            annotation_cols = [col for col in df.columns if col in ['label', 'labels', 'category', 'annotation', 'predicted_label']]
            if not annotation_cols:
                self.console.print("[yellow]⚠️  No annotation columns found. Cannot proceed with training.[/yellow]")
                return

            annotation_col = annotation_cols[0]
            df_annotated = df[df[annotation_col].notna()].copy()

            if len(df_annotated) == 0:
                self.console.print("[yellow]⚠️  No annotated rows found. Cannot proceed with training.[/yellow]")
                return

            self.console.print(f"[green]✓ Found {len(df_annotated):,} annotated rows for training[/green]\n")

            # Step 1: Category/Label Analysis
            self.console.print("[bold cyan]Step 1: Label/Category Analysis[/bold cyan]\n")

            # Detect label format (single vs multi-label)
            is_multi_label = False
            try:
                first_label = df_annotated[annotation_col].iloc[0]
                if isinstance(first_label, str) and (first_label.startswith('[') or first_label.startswith('{')):
                    import json
                    parsed = json.loads(first_label)
                    is_multi_label = isinstance(parsed, list)
            except:
                pass

            # Analyze categories
            if is_multi_label:
                self.console.print("[yellow]Multi-label classification detected[/yellow]")
                all_labels = []
                for val in df_annotated[annotation_col]:
                    try:
                        import json
                        parsed = json.loads(str(val)) if isinstance(val, str) else val
                        if isinstance(parsed, list):
                            all_labels.extend(parsed)
                    except:
                        pass
                label_counts = pd.Series(all_labels).value_counts()
            else:
                self.console.print("[yellow]Single-label classification detected[/yellow]")
                label_counts = df_annotated[annotation_col].value_counts()

            # Display category distribution
            from rich.table import Table
            cat_table = Table(title="Category Distribution", border_style="cyan", show_header=True, header_style="bold")
            cat_table.add_column("Category", style="cyan", width=30)
            cat_table.add_column("Count", style="yellow", justify="right", width=12)
            cat_table.add_column("Percentage", style="green", justify="right", width=12)

            total_labels = label_counts.sum()
            for label, count in label_counts.head(20).items():
                percentage = (count / total_labels * 100)
                cat_table.add_row(
                    str(label)[:30],
                    f"{count:,}",
                    f"{percentage:.1f}%"
                )

            if len(label_counts) > 20:
                cat_table.add_row("...", f"... and {len(label_counts) - 20} more", "...")

            self.console.print(cat_table)
            self.console.print(f"\n[green]✓ {len(label_counts)} unique categories detected[/green]\n")

            # Step 2: Benchmark Mode Option
            self.console.print("[bold cyan]Step 2: Training Mode Selection[/bold cyan]\n")
            self.console.print("[yellow]Training Mode Options:[/yellow]")
            self.console.print("  • [cyan]full[/cyan]     - Train on ALL categories")
            self.console.print("             Best for: Production deployment")
            self.console.print("  • [cyan]benchmark[/cyan] - Test MULTIPLE models on ONE specific category")
            self.console.print("             Best for: Finding the best model for your data")
            self.console.print("             Output: Comparison metrics to choose optimal model\n")

            training_mode = Prompt.ask(
                "Select training mode",
                choices=["full", "benchmark"],
                default="full"
            )

            benchmark_category = None
            if training_mode == "benchmark":
                self.console.print("\n[bold]Benchmark Mode: Select Target Category[/bold]")
                self.console.print("[dim]You'll train multiple models on this category to find the best one[/dim]\n")

                # Show top categories
                top_cats = list(label_counts.head(10).index)
                for i, cat in enumerate(top_cats, 1):
                    count = label_counts[cat]
                    pct = (count / total_labels * 100)
                    self.console.print(f"  {i}. {cat} ({count:,} samples, {pct:.1f}%)")

                cat_choice = Prompt.ask(
                    "\nSelect category number or enter name",
                    default="1"
                )

                if cat_choice.isdigit() and 0 < int(cat_choice) <= len(top_cats):
                    benchmark_category = top_cats[int(cat_choice) - 1]
                else:
                    benchmark_category = cat_choice

                self.console.print(f"[green]✓ Benchmark category: {benchmark_category}[/green]\n")

            # Step 3: Text Length Analysis (from Training Arena)
            self.console.print("[bold cyan]Step 3: Text Length Analysis[/bold cyan]\n")

            text_length_stats = {}
            requires_long_document_model = False

            if text_column in df_annotated.columns:
                all_texts = df_annotated[text_column].dropna().astype(str).tolist()

                if len(all_texts) > 0:
                    try:
                        from transformers import AutoTokenizer
                        import numpy as np
                        from tqdm import tqdm

                        tokenizer = AutoTokenizer.from_pretrained("bert-base-multilingual-cased")
                        token_lengths = []

                        for text in tqdm(all_texts, desc="Analyzing text lengths", disable=not HAS_RICH):
                            tokens = tokenizer.encode(text, truncation=False, add_special_tokens=True)
                            token_lengths.append(len(tokens))

                        token_lengths = np.array(token_lengths)

                        text_length_stats = {
                            'token_mean': float(np.mean(token_lengths)),
                            'token_median': float(np.median(token_lengths)),
                            'token_max': int(np.max(token_lengths)),
                            'token_p95': float(np.percentile(token_lengths, 95))
                        }

                        # Classify documents
                        long_docs = np.sum(token_lengths >= 512)
                        total = len(token_lengths)
                        long_pct = (long_docs / total * 100) if total > 0 else 0

                        self.console.print(f"[dim]Average: {text_length_stats['token_mean']:.0f} tokens | Max: {text_length_stats['token_max']} tokens[/dim]")

                        if long_pct > 20:
                            requires_long_document_model = True
                            self.console.print(f"[yellow]⚠ {long_pct:.1f}% of texts exceed 512 tokens[/yellow]")
                            use_long = Confirm.ask("Use long-document models (Longformer/BigBird)?", default=True)
                            text_length_stats['user_prefers_long_models'] = use_long
                        else:
                            self.console.print(f"[green]✓ {100-long_pct:.1f}% fit within standard BERT limits[/green]")
                            text_length_stats['user_prefers_long_models'] = False

                    except Exception as e:
                        self.logger.debug(f"Text length analysis failed: {e}")
                        self.console.print("[yellow]Could not perform detailed length analysis[/yellow]")

            self.console.print()

            # Step 4: Language Strategy Analysis (from Training Arena)
            self.console.print("[bold cyan]Step 4: Language Strategy[/bold cyan]\n")

            # Check if language column exists
            has_lang_col = 'lang' in df_annotated.columns
            confirmed_languages = set()
            language_distribution = {}

            if has_lang_col:
                lang_counts = df_annotated['lang'].value_counts()
                total_lang = len(df_annotated)

                # Display language distribution
                lang_table = Table(border_style="cyan", show_header=True, header_style="bold")
                lang_table.add_column("Language", style="cyan", width=12)
                lang_table.add_column("Count", style="yellow", justify="right", width=12)
                lang_table.add_column("Percentage", style="green", justify="right", width=12)

                for lang, count in lang_counts.items():
                    if lang != 'unknown':
                        pct = (count / total_lang * 100)
                        lang_table.add_row(lang.upper(), f"{count:,}", f"{pct:.1f}%")
                        language_distribution[lang] = int(count)
                        confirmed_languages.add(lang)

                self.console.print(lang_table)
                self.console.print(f"\n[green]✓ {len(confirmed_languages)} language(s) detected[/green]\n")
            else:
                self.console.print("[yellow]No language column found. Assuming single language.[/yellow]\n")

            # Language training strategy
            model_strategy = "multilingual"
            language_model_mapping = {}

            if len(confirmed_languages) > 1:
                self.console.print("[yellow]Multiple languages detected. Select training strategy:[/yellow]")
                self.console.print("  • [cyan]multilingual[/cyan] - ONE model for all languages")
                self.console.print("  • [cyan]specialized[/cyan] - SEPARATE model per language")
                self.console.print("  • [cyan]hybrid[/cyan] - Multilingual base + specialized fine-tuning\n")

                model_strategy = Prompt.ask(
                    "Training strategy",
                    choices=["multilingual", "specialized", "hybrid"],
                    default="multilingual"
                )

                self.console.print(f"[green]✓ Strategy: {model_strategy}[/green]\n")

            # Step 5: Model Selection
            self.console.print("[bold cyan]Step 5: Model Selection[/bold cyan]\n")

            # Handle specialized training: select model per language
            if model_strategy == "specialized":
                self.console.print("[yellow]Specialized Training: Select a model for EACH language[/yellow]\n")

                for lang in sorted(confirmed_languages):
                    lang_upper = lang.upper()
                    self.console.print(f"[bold]Models for {lang_upper}:[/bold]")

                    # Get language-specific recommendations
                    if text_length_stats.get('user_prefers_long_models'):
                        lang_recommendations = self._get_long_document_models_for_language(lang)
                    else:
                        from llm_tool.utils.language_normalizer import LanguageNormalizer
                        lang_recs = LanguageNormalizer.recommend_models({lang}, self.available_trainer_models)
                        lang_recommendations = lang_recs if lang_recs else []

                    if not lang_recommendations:
                        # Fallback to multilingual models
                        lang_recommendations = [
                            {'model': 'xlm-roberta-base', 'reason': 'Multilingual baseline (100+ languages)'},
                            {'model': 'bert-base-multilingual-cased', 'reason': 'Multilingual BERT (104 languages)'},
                        ]

                    # Display recommendations
                    for i, rec in enumerate(lang_recommendations[:5], 1):
                        self.console.print(f"  {i}. [cyan]{rec['model']}[/cyan] - {rec['reason']}")

                    # Loop until valid model is selected
                    selected = None
                    while selected is None:
                        model_choice = Prompt.ask(
                            f"\nSelect model for {lang_upper} (number or name)",
                            default="1"
                        )

                        if model_choice.isdigit():
                            choice_num = int(model_choice)
                            if 0 < choice_num <= len(lang_recommendations):
                                selected = lang_recommendations[choice_num - 1]['model']
                            else:
                                self.console.print(f"[red]Invalid number. Please enter 1-{len(lang_recommendations)}[/red]")
                        else:
                            # Check if it's a valid model name
                            if model_choice in self.available_trainer_models:
                                selected = model_choice
                            else:
                                self.console.print(f"[red]Model '{model_choice}' not found. Use number or valid model name.[/red]")

                    language_model_mapping[lang] = selected
                    self.console.print(f"[green]✓ {lang_upper}: {selected}[/green]\n")

                # For training params, use first language's model as default
                training_model = list(language_model_mapping.values())[0]

            else:
                # Multilingual or Hybrid: single model for all languages
                recommended_models = []

                if text_length_stats.get('user_prefers_long_models'):
                    self.console.print("[yellow]Long-document models (4096+ tokens):[/yellow]")
                    recommended_models = [
                        "markussagen/xlm-roberta-longformer-base-4096",  # Multilingual FIRST
                        "google/long-t5-local-base",  # Multilingual
                        "allenai/longformer-base-4096",  # English only
                        "google/bigbird-roberta-base"  # English only
                    ]
                else:
                    if confirmed_languages:
                        from llm_tool.utils.language_normalizer import LanguageNormalizer
                        recs = LanguageNormalizer.recommend_models(confirmed_languages, self.available_trainer_models)
                        recommended_models = [r['model'] for r in recs[:5]] if recs else []

                    if not recommended_models:
                        recommended_models = ["bert-base-multilingual-cased", "xlm-roberta-base", "distilbert-base-multilingual-cased"]

                self.console.print("[green]Recommended models:[/green]")
                for i, model in enumerate(recommended_models[:5], 1):
                    self.console.print(f"  {i}. {model}")

                # Loop until valid model is selected
                training_model = None
                while training_model is None:
                    selected_model = Prompt.ask(
                        "\nSelect model (number or name)",
                        default="1"
                    )

                    if selected_model.isdigit():
                        choice_num = int(selected_model)
                        if 0 < choice_num <= len(recommended_models):
                            training_model = recommended_models[choice_num - 1]
                        else:
                            self.console.print(f"[red]Invalid number. Please enter 1-{len(recommended_models)}[/red]")
                    else:
                        # Check if it's a valid model name
                        if selected_model in self.available_trainer_models:
                            training_model = selected_model
                        else:
                            self.console.print(f"[red]Model '{selected_model}' not found. Use number or valid model name.[/red]")

                self.console.print(f"[green]✓ Selected model: {training_model}[/green]\n")

            # Step 6: Training Configuration
            self.console.print("[bold cyan]Step 6: Training Configuration[/bold cyan]\n")

            # Training mode selection
            training_modes = {
                "quick": "Quick training (10 epochs, fast)",
                "benchmark": "Benchmark mode (compare models)",
                "custom": "Custom configuration"
            }

            self.console.print("[yellow]Training modes:[/yellow]")
            for mode, desc in training_modes.items():
                self.console.print(f"  • [cyan]{mode}[/cyan]: {desc}")

            train_mode = Prompt.ask(
                "\nSelect training mode",
                choices=list(training_modes.keys()),
                default="quick"
            )

            # Epochs
            if train_mode == "quick":
                epochs = 10
            elif train_mode == "benchmark":
                epochs = 3
            else:
                epochs = self._int_prompt_with_validation("Number of epochs", default=3, min_value=1, max_value=20)

            # Batch size
            batch_size = self._int_prompt_with_validation("Batch size", default=16, min_value=1, max_value=128)

            # Learning rate
            learning_rate = 2e-5
            if train_mode == "custom":
                lr_input = Prompt.ask("Learning rate", default="2e-5")
                try:
                    learning_rate = float(lr_input)
                except:
                    learning_rate = 2e-5

            self.console.print(f"\n[green]✓ Configuration:[/green]")
            self.console.print(f"  Epochs: {epochs}")
            self.console.print(f"  Batch size: {batch_size}")
            self.console.print(f"  Learning rate: {learning_rate}\n")

            # ===========================================================
            # SAVE TRAINING PARAMETERS (Second save point)
            # ===========================================================
            self.console.print("[bold cyan]💾 Saving Training Parameters[/bold cyan]\n")

            training_params = {
                "training_metadata": {
                    "timestamp": pd.Timestamp.now().isoformat(),
                    "annotation_file": output_file,
                    "total_annotated_rows": len(df_annotated),
                    "training_mode": training_mode,
                    "benchmark_category": benchmark_category
                },
                "data_config": {
                    "text_column": text_column,
                    "label_column": annotation_col,
                    "is_multi_label": is_multi_label,
                    "num_categories": len(label_counts),
                    "category_distribution": label_counts.to_dict()
                },
                "language_config": {
                    "confirmed_languages": list(confirmed_languages),
                    "language_distribution": language_distribution,
                    "model_strategy": model_strategy,
                    "language_model_mapping": language_model_mapping
                },
                "text_analysis": {
                    "text_length_stats": text_length_stats,
                    "requires_long_document_model": requires_long_document_model
                },
                "model_config": {
                    "selected_model": training_model,
                    "epochs": epochs,
                    "batch_size": batch_size,
                    "learning_rate": learning_rate
                },
                "prompt_configs": prompt_configs
            }

            # Save training parameters
            import json
            from pathlib import Path

            params_file = Path(output_file).parent / f"{Path(output_file).stem}_training_params.json"
            with open(params_file, 'w', encoding='utf-8') as f:
                json.dump(training_params, f, indent=2, ensure_ascii=False)

            self.console.print(f"[green]✓ Training parameters saved to:[/green]")
            self.console.print(f"  {params_file}\n")

            # Step 7: Execute Training
            self.console.print("[bold cyan]Step 7: Model Training[/bold cyan]\n")

            start_training = Confirm.ask(
                "[bold]Start training now?[/bold]",
                default=True
            )

            if start_training:
                self.console.print("[green]🚀 Starting training...[/green]\n")

                # Call Training Arena's training method
                try:
                    from llm_tool.trainers.training_data_builder import TrainingDatasetBuilder

                    # Prepare training dataset
                    builder = TrainingDatasetBuilder()

                    # Convert annotated data to training format
                    training_data_path = Path(output_file).parent / f"{Path(output_file).stem}_training.csv"

                    # Prepare data in correct format
                    train_df = df_annotated[[text_column, annotation_col]].copy()
                    train_df.columns = ['text', 'label']
                    train_df.to_csv(training_data_path, index=False)

                    self.console.print(f"[green]✓ Training data prepared: {training_data_path}[/green]")

                    # Initialize training pipeline
                    from llm_tool.trainers.model_trainer import ModelTrainer

                    trainer = ModelTrainer(
                        model_name=training_model,
                        num_labels=len(label_counts),
                        device="cuda" if self._check_cuda_available() else "cpu"
                    )

                    self.console.print(f"[yellow]Training {training_model} for {epochs} epochs...[/yellow]")

                    # Load and split data
                    from sklearn.model_selection import train_test_split
                    from collections import Counter

                    # Check if we have at least 2 instances per class for stratification
                    from ..utils.data_filter_logger import get_filter_logger

                    stratify_col = None
                    if not is_multi_label:
                        label_counts = Counter(train_df['label'])
                        min_count = min(label_counts.values())

                        if min_count < 2:
                            # Find which classes have insufficient instances
                            insufficient_classes = [cls for cls, count in label_counts.items() if count < 2]

                            self.console.print(f"\n[yellow]⚠️  Found {len(insufficient_classes)} label(s) with insufficient samples:[/yellow]")
                            for cls in insufficient_classes:
                                count = label_counts[cls]
                                self.console.print(f"  • [red]'{cls}'[/red]: {count} sample(s) - need at least 2")

                            self.console.print(f"\n[bold]What would you like to do?[/bold]")
                            self.console.print(f"  [cyan]1.[/cyan] [green]Remove[/green] these {len(insufficient_classes)} value(s) and continue")
                            self.console.print(f"  [cyan]2.[/cyan] [red]Cancel[/red] training\n")

                            remove_labels = Confirm.ask(
                                f"[bold yellow]Remove insufficient labels and continue?[/bold yellow]",
                                default=True
                            )

                            if remove_labels:
                                # Filter out samples with insufficient classes
                                # Pass session_id if available for contextualized logging
                                filter_logger = get_filter_logger(session_id=getattr(self, 'current_session_id', None))
                                df_before = train_df.copy()
                                train_df = train_df[~train_df['label'].isin(insufficient_classes)]

                                # Log filtered data
                                filter_logger.log_dataframe_filtering(
                                    df_before=df_before,
                                    df_after=train_df,
                                    reason="insufficient_samples_per_class",
                                    location="advanced_cli.train_single_model",
                                    text_column='text' if 'text' in train_df.columns else None,
                                    log_filtered_samples=5
                                )

                                self.console.print(f"\n[green]✓ Removed {len(df_before) - len(train_df)} sample(s)[/green]")

                                # Recompute label counts
                                label_counts = Counter(train_df['label'])
                                min_count = min(label_counts.values()) if label_counts else 0

                                if min_count < 2:
                                    self.console.print("[red]Still insufficient samples after removal. Cannot proceed with training.[/red]")
                                    return

                                stratify_col = train_df['label']
                            else:
                                self.console.print(f"[red]Training cancelled by user[/red]")
                                return
                        else:
                            stratify_col = train_df['label']

                    train_data, val_data = train_test_split(
                        train_df,
                        test_size=0.2,
                        random_state=42,
                        stratify=stratify_col
                    )

                    # Train model
                    results = trainer.train(
                        train_texts=train_data['text'].tolist(),
                        train_labels=train_data['label'].tolist(),
                        val_texts=val_data['text'].tolist() if len(val_data) > 0 else None,
                        val_labels=val_data['label'].tolist() if len(val_data) > 0 else None,
                        epochs=epochs,
                        batch_size=batch_size,
                        learning_rate=learning_rate
                    )

                    # Save model
                    model_save_path = Path(output_file).parent / f"{Path(output_file).stem}_model"
                    trainer.save_model(str(model_save_path))

                    self.console.print(f"\n[bold green]✅ Training completed![/bold green]")
                    self.console.print(f"[green]Model saved to: {model_save_path}[/green]")

                    if results:
                        self.console.print(f"\n[bold cyan]Training Results:[/bold cyan]")
                        for key, value in results.items():
                            self.console.print(f"  {key}: {value}")

                except Exception as train_error:
                    self.console.print(f"[red]❌ Training error: {train_error}[/red]")
                    self.logger.exception("Training execution failed")
            else:
                self.console.print("[yellow]Training skipped. You can resume later using the saved parameters.[/yellow]")

        except Exception as e:
            self.console.print(f"[red]Training workflow error: {e}[/red]")
            self.logger.exception("Post-annotation training workflow failed")

    def _get_model_description(self, model_name: str) -> str:
        """Get factual description for a model based on its name"""
        # Normalize model name for matching
        name_lower = model_name.lower()

        # Try exact match first
        if name_lower in MODEL_DESCRIPTIONS:
            return MODEL_DESCRIPTIONS[name_lower]

        # Try prefix match for variants (e.g., "llama3.2:3b" matches "llama3.2")
        for key in MODEL_DESCRIPTIONS:
            if name_lower.startswith(key):
                return MODEL_DESCRIPTIONS[key]

        # Try substring match for common patterns
        for key in MODEL_DESCRIPTIONS:
            if key in name_lower or name_lower.split(':')[0] == key:
                return MODEL_DESCRIPTIONS[key]

        # Default for unknown models
        return "Custom or community model"

    def _select_llm_interactive(self) -> ModelInfo:
        """Let user interactively select an LLM from available options"""
        if HAS_RICH and self.console:
            self.console.print("\n[bold]Available LLMs:[/bold]")
            self.console.print("[dim]ℹ️  Additional API models (Anthropic, Google, etc.) will be added as they are tested in the pipeline[/dim]\n")

            # Collect all available LLMs
            all_llms = []
            local_llms = self.detected_llms.get('local', [])
            openai_llms = self.detected_llms.get('openai', [])
            anthropic_llms = self.detected_llms.get('anthropic', [])

            # Display Local Models with Rich Table
            if local_llms:
                self.console.print("\n[bold cyan]🖥️  Local Models (Ollama):[/bold cyan]\n")

                local_table = Table(border_style="cyan", show_header=True)
                local_table.add_column("#", style="bold yellow", width=4)
                local_table.add_column("Model Name", style="white", width=25)
                local_table.add_column("Size", style="green", width=8)
                local_table.add_column("Description", style="dim", width=70)

                for llm in local_llms:
                    idx = len(all_llms) + 1
                    description = self._get_model_description(llm.name)
                    local_table.add_row(
                        str(idx),
                        llm.name,
                        llm.size or "N/A",
                        description
                    )
                    all_llms.append(llm)

                self.console.print(local_table)

            # Display OpenAI Models with Rich Table
            if openai_llms:
                self.console.print("\n[bold cyan]☁️  OpenAI Models:[/bold cyan]\n")

                openai_table = Table(border_style="blue", show_header=True)
                openai_table.add_column("#", style="bold yellow", width=4)
                openai_table.add_column("Model Name", style="white", width=30)
                openai_table.add_column("Cost", style="magenta", width=12)
                openai_table.add_column("Description", style="dim", width=65)

                for llm in openai_llms:
                    idx = len(all_llms) + 1
                    cost = f"${llm.cost_per_1k_tokens}/1K" if llm.cost_per_1k_tokens else "N/A"
                    description = self._get_model_description(llm.name)
                    openai_table.add_row(
                        str(idx),
                        llm.name,
                        cost,
                        description
                    )
                    all_llms.append(llm)

                # Add custom option row
                idx = len(all_llms) + 1
                openai_table.add_row(
                    str(idx),
                    "[bold]Custom model[/bold]",
                    "-",
                    "[dim]Enter OpenAI model name manually[/dim]"
                )
                custom_openai_option_idx = idx

                self.console.print(openai_table)

            # Display Anthropic Models with Rich Table
            if anthropic_llms:
                self.console.print("\n[bold cyan]🤖 Anthropic Models:[/bold cyan]\n")

                anthropic_table = Table(border_style="magenta", show_header=True)
                anthropic_table.add_column("#", style="bold yellow", width=4)
                anthropic_table.add_column("Model Name", style="white", width=30)
                anthropic_table.add_column("Cost", style="cyan", width=12)
                anthropic_table.add_column("Description", style="dim", width=65)

                for llm in anthropic_llms[:3]:  # Show top 3
                    idx = len(all_llms) + 1
                    cost = f"${llm.cost_per_1k_tokens}/1K" if llm.cost_per_1k_tokens else "N/A"
                    description = self._get_model_description(llm.name)
                    anthropic_table.add_row(
                        str(idx),
                        llm.name,
                        cost,
                        description
                    )
                    all_llms.append(llm)

                self.console.print(anthropic_table)

            if not all_llms:
                self.console.print("[red]No LLMs detected![/red]")
                return ModelInfo("llama3.2", "ollama", is_available=False)

            # Ask user to select with validation
            max_choice = len(all_llms) + (1 if openai_llms else 0)  # +1 for custom option if OpenAI available
            choice = self._int_prompt_with_validation("\nSelect LLM", default=1, min_value=1, max_value=max_choice)

            # Check if user selected custom OpenAI option
            if openai_llms and choice == custom_openai_option_idx:
                self.console.print("\n[dim]Examples: gpt-3.5-turbo, gpt-4, gpt-4o, gpt-4o-2025-01-01, o1, o1-mini, o3-mini, gpt-5[/dim]")
                custom_model = Prompt.ask("Enter OpenAI model name")

                # Create a ModelInfo for the custom model with estimated parameters
                return ModelInfo(
                    name=custom_model,
                    provider="openai",
                    context_length=128000,  # Default estimate
                    requires_api_key=True,
                    supports_json=True,
                    supports_streaming=not any(x in custom_model.lower() for x in ['o1-', 'o3-', 'o4-']),
                    is_available=True
                )

            return all_llms[choice - 1]
        else:
            # Fallback
            return self._auto_select_llm()

    def _auto_select_llm(self) -> ModelInfo:
        """Intelligently select the best available LLM for annotation"""
        # Priority: Local LLMs > API LLMs with key > Others

        local_llms = self.detected_llms.get('local', [])
        if local_llms:
            # Prefer larger, more capable local LLMs
            # Sort by size (if available) or by preferred order
            preferred_order = ['gemma3:27b', 'llama3.2:latest', 'mixtral', 'llama3', 'llama2', 'mistral', 'gemma2', 'gemma']

            for preferred in preferred_order:
                for llm in local_llms:
                    if preferred in llm.name.lower() or llm.name.lower().startswith(preferred.split(':')[0]):
                        return llm

            # Return the first available if no preferred found
            return local_llms[0]

        # Check for API keys in environment
        if os.getenv('OPENAI_API_KEY'):
            openai_llms = self.detected_llms.get('openai', [])
            if openai_llms:
                # Prefer GPT-4 Turbo for best quality/cost ratio
                for llm in openai_llms:
                    if 'gpt-4-turbo' in llm.name:
                        return llm
                return openai_llms[0]

        if os.getenv('ANTHROPIC_API_KEY'):
            anthropic_llms = self.detected_llms.get('anthropic', [])
            if anthropic_llms:
                # Prefer Sonnet for balance
                for llm in anthropic_llms:
                    if 'sonnet' in llm.name:
                        return llm
                return anthropic_llms[0]

        # Default fallback
        return ModelInfo(
            name="llama3.2",
            provider="ollama",
            is_available=False,
            requires_api_key=False
        )

    def _auto_select_dataset(self) -> Optional[DatasetInfo]:
        """Intelligently select the most likely dataset"""
        if not self.detected_datasets:
            return None

        # Prefer CSV files with obvious text columns
        for dataset in self.detected_datasets:
            if dataset.format == 'csv':
                text_col = self.data_detector.suggest_text_column(dataset)
                if text_col:
                    return dataset

        # Return largest dataset as fallback
        return max(self.detected_datasets, key=lambda d: d.size_mb or 0)

    def _suggest_max_tokens(self, model: ModelInfo) -> int:
        """Suggest a reasonable max token budget for the selected model"""
        if model.max_tokens:
            return model.max_tokens

        if model.context_length:
            # Reserve a safety margin so prompts+output stay within context
            safe_default = max(256, int(model.context_length * 0.25))
            return min(safe_default, getattr(self.settings.api, 'max_tokens', safe_default))

        if model.provider in {'openai', 'anthropic', 'google'}:
            return getattr(self.settings.api, 'max_tokens', 4096)

        # Fallback for local/unknown providers
        return 1000

    def _prompt_max_tokens(self, model: ModelInfo) -> int:
        """Ask the user for a max token budget with intelligent defaults"""
        suggested = self._suggest_max_tokens(model)
        max_context = model.context_length or None

        question = "Max tokens per response"
        if max_context:
            question += f" (≤ {max_context})"

        while True:
            max_tokens = IntPrompt.ask(question, default=suggested, show_default=True)

            if max_tokens <= 0:
                self.console.print("[yellow]Please provide a positive number of tokens.[/yellow]")
                continue

            if max_context and max_tokens >= max_context:
                self.console.print(
                    f"[yellow]The value must stay below the model context window ({max_context}). "
                    "Try a smaller number.[/yellow]"
                )
                continue

            return max_tokens

    def _prompt_file_path(self, prompt_text: str, filter_format: str = None) -> str:
        """Prompt for file path with validation and Rich table display of detected datasets"""
        while True:
            if HAS_RICH and self.console:
                # Use detected datasets if available
                if self.detected_datasets:
                    # Filter by format if specified
                    datasets = self.detected_datasets
                    if filter_format:
                        datasets = [d for d in datasets if d.format.lower() == filter_format.lower()]

                    if datasets:
                        self.console.print(f"\n[bold cyan]📊 Detected Datasets[/bold cyan]\n")

                        datasets_table = Table(show_header=True, header_style="bold magenta", border_style="cyan", box=box.ROUNDED)
                        datasets_table.add_column("#", style="cyan", width=3)
                        datasets_table.add_column("Name", style="white", width=50, no_wrap=True)
                        datasets_table.add_column("Format", style="yellow bold", width=8, justify="center")
                        datasets_table.add_column("Size", style="green", width=10, justify="right")
                        datasets_table.add_column("Path", style="dim", width=45, no_wrap=True)

                        for i, dataset in enumerate(datasets, 1):
                            # Format size
                            size_str = f"{dataset.size_mb:.1f} MB" if dataset.size_mb else "—"

                            # Shorten path
                            path_str = str(dataset.path.parent)
                            if len(path_str) > 40:
                                path_str = "..." + path_str[-37:]

                            datasets_table.add_row(
                                str(i),
                                dataset.path.name,
                                dataset.format.upper(),
                                size_str,
                                path_str
                            )

                        self.console.print(datasets_table)
                        self.console.print()

                        # Better instructions for user
                        self.console.print("[dim]💡 You can either:[/dim]")
                        self.console.print("[dim]   • Enter the [cyan]#[/cyan] number from the table above (e.g., '1', '13')[/dim]")
                        self.console.print("[dim]   • Enter an [cyan]absolute path[/cyan] to any file (e.g., '/Users/name/data/file.csv')[/dim]\n")

                        path = Prompt.ask(f"{prompt_text}")

                        # Allow number selection
                        if path.isdigit() and 1 <= int(path) <= len(datasets):
                            return str(datasets[int(path)-1].path)

                        # Allow path input
                        if Path(path).exists():
                            return path
                        else:
                            self.console.print(f"[red]File not found: {path}[/red]")
                            continue

                # Fallback: show files from current directory
                files = list(Path.cwd().glob("*.csv")) + list(Path.cwd().glob("*.json")) + list(Path.cwd().glob("*.xlsx"))
                if files:
                    self.console.print("\n[dim]Available files in current directory:[/dim]")
                    for i, f in enumerate(files[:10], 1):
                        self.console.print(f"  {i}. {f.name}")

                path = Prompt.ask(f"\n{prompt_text}")

                # Allow number selection
                if path.isdigit() and int(path) <= len(files):
                    path = str(files[int(path)-1])

                if Path(path).exists():
                    return path
                else:
                    self.console.print(f"[red]File not found: {path}[/red]")
            else:
                path = input(f"{prompt_text}: ").strip()
                if Path(path).exists():
                    return path
                else:
                    print(f"File not found: {path}")

    def _check_return_to_menu(self, step_name: str = "this step") -> bool:
        """Ask user if they want to return to main menu. Returns True if user wants to go back."""
        if HAS_RICH and self.console:
            go_back = Confirm.ask(
                f"[dim]Return to main menu? (or press Enter to continue {step_name})[/dim]",
                default=False
            )
        else:
            response = input(f"Return to main menu? [y/N] (or Enter to continue {step_name}): ").strip().lower()
            go_back = response in ['y', 'yes']

        if go_back:
            if HAS_RICH and self.console:
                self.console.print("[yellow]↩ Returning to main menu...[/yellow]\n")
            else:
                print("↩ Returning to main menu...\n")
        return go_back

    def _generate_auto_prompt(self, dataset_path: str, text_column: str) -> str:
        """Generate intelligent prompt based on dataset analysis"""
        # Simple heuristic-based prompt generation
        prompt_template = """Analyze the following text and extract key information.

Text: {text}

Please provide a structured analysis including:
1. Main topic or subject
2. Sentiment (positive/negative/neutral)
3. Key entities mentioned
4. Summary in one sentence

Format your response as JSON with keys: topic, sentiment, entities, summary"""

        # Could be enhanced with actual dataset sampling and analysis
        return prompt_template

    def _detect_prompts_in_directory(self) -> List[Path]:
        """Detect available prompt files in the prompts directory"""
        prompts_dir = self.settings.paths.prompts_dir
        if prompts_dir.exists():
            return sorted(list(prompts_dir.glob("*.txt")))
        return []

    def _get_custom_prompt(self, skip_detection: bool = False) -> str:
        """Get custom prompt from user - detect from directory, file path, paste, or wizard

        Args:
            skip_detection: If True, skip prompts directory detection and go straight to wizard
        """
        if HAS_RICH and self.console:
            # If skip_detection is True, go directly to wizard
            if skip_detection:
                return self._run_social_science_wizard()

            # Try to detect prompts in the prompts directory
            detected_prompts = self._detect_prompts_in_directory()

            if detected_prompts:
                self.console.print("\n[bold green]✓ Detected prompts in prompts directory:[/bold green]")
                for i, prompt_file in enumerate(detected_prompts, 1):
                    self.console.print(f"  {i}. {prompt_file.name}")

                use_detected = Confirm.ask("\nUse one of these prompts?", default=True)

                if use_detected:
                    if len(detected_prompts) == 1:
                        selected_prompt = detected_prompts[0]
                    else:
                        choice = IntPrompt.ask(
                            "Select prompt",
                            default=1,
                            min_value=1,
                            max_value=len(detected_prompts)
                        )
                        selected_prompt = detected_prompts[choice - 1]

                    # Load and return the prompt
                    try:
                        full_prompt, expected_keys = self.prompt_manager.load_prompt(str(selected_prompt))
                        self.console.print(f"\n[green]✓ Loaded prompt: {selected_prompt.name}[/green]")
                        self.console.print(f"[dim]Detected JSON keys: {', '.join(expected_keys[:5])}{'...' if len(expected_keys) > 5 else ''}[/dim]")
                        return full_prompt
                    except Exception as e:
                        self.console.print(f"[red]Error loading prompt: {e}[/red]")

            # If no detected prompts or user declined, ask for input method
            self.console.print("\n[bold]Prompt Input Method:[/bold]")
            self.console.print("[dim]• wizard: 🧙‍♂️ Interactive Social Science Prompt Wizard (Recommended!)[/dim]")
            self.console.print("[dim]• path: Load from existing file[/dim]")
            self.console.print("[dim]• paste: Paste prompt text directly[/dim]")

            method = Prompt.ask(
                "How do you want to provide the prompt?",
                choices=["wizard", "path", "paste"],
                default="wizard"
            )

            if method == "wizard":
                # Launch the Social Science Prompt Wizard
                return self._run_social_science_wizard()

            elif method == "path":
                # Ask for file path
                prompt_path = Prompt.ask("\nPath to prompt file (.txt)")
                while not Path(prompt_path).exists():
                    self.console.print(f"[red]File not found: {prompt_path}[/red]")
                    prompt_path = Prompt.ask("Path to prompt file (.txt)")

                try:
                    full_prompt, expected_keys = self.prompt_manager.load_prompt(prompt_path)
                    self.console.print(f"[green]✓ Loaded prompt from: {prompt_path}[/green]")
                    self.console.print(f"[dim]Detected JSON keys: {', '.join(expected_keys[:5])}{'...' if len(expected_keys) > 5 else ''}[/dim]")
                    return full_prompt
                except Exception as e:
                    self.console.print(f"[red]Error loading prompt: {e}[/red]")
                    return ""

            else:  # paste
                self.console.print("\n[bold cyan]Paste your prompt below[/bold cyan]")
                self.console.print("[dim]Press Ctrl+D (Unix/Mac) or Ctrl+Z (Windows) when done[/dim]\n")
                lines = []
                try:
                    while True:
                        line = input()
                        lines.append(line)
                except EOFError:
                    pass

                prompt_text = "\n".join(lines)
                self.console.print(f"\n[green]✓ Received prompt ({len(prompt_text)} characters)[/green]")
                return prompt_text

        else:
            # Fallback for non-Rich environments
            return input("Enter prompt: ").strip()

    def _run_social_science_wizard(self) -> str:
        """Launch the Social Science Prompt Wizard for guided prompt creation"""
        try:
            # Check if user wants LLM assistance for definition generation
            use_llm_assist = Confirm.ask(
                "\n[cyan]🤖 Do you want AI assistance for creating your prompt (wizard mode)?[/cyan]",
                default=True
            )

            llm_client = None
            if use_llm_assist:
                # Detect available models for assistance
                all_llms = LLMDetector.detect_all_llms()
                available_models = []

                # Collect all available models
                for provider, models in all_llms.items():
                    for model in models:
                        if model.is_available:
                            available_models.append(model)

                if not available_models:
                    self.console.print("[yellow]⚠️  No LLM models detected. Continuing without AI assistance.[/yellow]")
                else:
                    # Display available models
                    self.console.print("\n[bold cyan]🤖 Available Models for AI Assistance:[/bold cyan]")

                    table = Table(
                        box=box.ROUNDED,
                        title="[bold]LLM Model Selection[/bold]",
                        title_style="cyan"
                    )
                    table.add_column("#", justify="right", style="cyan bold", width=4)
                    table.add_column("Model", style="white bold", width=35)
                    table.add_column("Provider", style="yellow", width=15)
                    table.add_column("Type / Size", style="green", width=20)

                    for i, model in enumerate(available_models, 1):
                        # Determine model type and quality indicator
                        model_name = model.name
                        provider = model.provider

                        # Extract size/quality info from model name
                        type_info = ""
                        if "120b" in model_name.lower():
                            type_info = "🚀 Very Large (120B)"
                        elif "72b" in model_name.lower() or "70b" in model_name.lower():
                            type_info = "⚡ Large (70B+)"
                        elif "27b" in model_name.lower() or "22b" in model_name.lower():
                            type_info = "💪 Medium (20B+)"
                        elif "8x" in model_name.lower():
                            type_info = "🔀 MoE (Mixture)"
                        elif "3.2" in model_name.lower() or "3.3" in model_name.lower():
                            type_info = "⚡ Fast (Llama 3)"
                        elif "gpt-5-nano" in model_name.lower():
                            type_info = "⚡ Ultra Fast"
                        elif "gpt-5-mini" in model_name.lower():
                            type_info = "🎯 Balanced"
                        elif "deepseek-r1" in model_name.lower():
                            type_info = "🧠 Reasoning"
                        elif "nemotron" in model_name.lower():
                            type_info = "📝 Instruction"
                        else:
                            type_info = model.size or "Standard"

                        # Style the provider
                        if provider == "ollama":
                            provider_styled = "🏠 Ollama"
                        elif provider == "openai":
                            provider_styled = "☁️  OpenAI"
                        elif provider == "anthropic":
                            provider_styled = "☁️  Anthropic"
                        else:
                            provider_styled = provider

                        table.add_row(str(i), model_name, provider_styled, type_info)

                    self.console.print(table)
                    self.console.print("\n[dim italic]💡 Tip: Larger models give better results but are slower[/dim italic]\n")

                    # Let user select model
                    while True:
                        choice = IntPrompt.ask(
                            "\n[cyan]Select model for AI assistance[/cyan]",
                            default=1
                        )
                        if 1 <= choice <= len(available_models):
                            break
                        self.console.print(f"[red]Please select a number between 1 and {len(available_models)}[/red]")

                    selected_model = available_models[choice - 1]
                    self.console.print(f"[green]✓ Selected: {selected_model.name}[/green]\n")

                    # Get API key if needed
                    api_key = None
                    if selected_model.requires_api_key:
                        api_key = self._get_or_prompt_api_key(
                            selected_model.provider,
                            selected_model.name
                        )

                    # Create LLM client for wizard
                    try:
                        llm_client = create_llm_client_for_wizard(
                            provider=selected_model.provider,
                            model=selected_model.name,
                            api_key=api_key
                        )
                        self.console.print("[green]✓ AI assistant ready![/green]\n")
                    except Exception as e:
                        self.console.print(f"[yellow]⚠️  Failed to initialize AI assistant: {e}[/yellow]")
                        self.console.print("[yellow]Continuing without AI assistance.[/yellow]\n")
                        llm_client = None

            # Create and run wizard
            wizard = SocialSciencePromptWizard(llm_client=llm_client)
            prompt_text, expected_keys = wizard.run()

            # Store expected keys in prompt manager for later use
            if expected_keys:
                self.console.print(f"\n[green]✓ Generated prompt with {len(expected_keys)} JSON keys:[/green]")
                self.console.print(f"[dim]{', '.join(expected_keys)}[/dim]\n")

            return prompt_text

        except Exception as e:
            self.console.print(f"\n[red]✗ Error running wizard: {e}[/red]")
            import traceback
            self.console.print(f"[dim]{traceback.format_exc()}[/dim]")

            # Fallback to manual prompt entry
            self.console.print("\n[yellow]Falling back to manual prompt entry...[/yellow]")
            return self._get_custom_prompt()

    def _get_multi_prompts(self) -> List[Tuple[str, List[str], str]]:
        """Get multiple prompts for multi-prompt mode"""
        if HAS_RICH and self.console:
            self.console.print("\n[bold cyan]Multi-Prompt Configuration[/bold cyan]")

            # Option to load from folder or individually
            load_from_folder = Confirm.ask(
                "Load all prompts from the prompts directory?",
                default=True
            )

            if load_from_folder:
                # Use the PromptManager's folder loading feature
                prompts_dir = str(self.settings.paths.prompts_dir)
                txt_files = sorted([f for f in os.listdir(prompts_dir) if f.endswith('.txt')])

                if not txt_files:
                    self.console.print(f"[yellow]No .txt files found in {prompts_dir}[/yellow]")
                    return []

                self.console.print(f"\n[green]Found {len(txt_files)} prompt files:[/green]")
                for i, filename in enumerate(txt_files, 1):
                    self.console.print(f"  {i}. {filename}")

                # Load each file
                prompts_list = []
                for i, filename in enumerate(txt_files, 1):
                    filepath = os.path.join(prompts_dir, filename)
                    self.console.print(f"\n[cyan][{i}/{len(txt_files)}] Loading {filename}...[/cyan]")

                    try:
                        full_prompt, expected_keys = self.prompt_manager.load_prompt(filepath)

                        if expected_keys:
                            self.console.print(f"[green]✓ Detected {len(expected_keys)} JSON keys[/green]")

                            # Ask for prefix
                            use_prefix = Confirm.ask(f"Add prefix to keys from '{filename}'?", default=False)
                            prefix_word = ""
                            if use_prefix:
                                default_prefix = Path(filename).stem.lower().replace(' ', '_')
                                prefix_word = Prompt.ask(f"Prefix (default: {default_prefix})", default=default_prefix)
                                self.console.print(f"[dim]✓ Keys will be prefixed with '{prefix_word}_'[/dim]")

                            prompts_list.append((full_prompt, expected_keys, prefix_word))
                        else:
                            self.console.print(f"[yellow]⚠ No JSON keys detected, skipping...[/yellow]")

                    except Exception as e:
                        self.console.print(f"[red]❌ Error loading {filename}: {e}[/red]")
                        continue

                if prompts_list:
                    self.console.print(f"\n[bold green]✓ Successfully loaded {len(prompts_list)} prompts[/bold green]")
                else:
                    self.console.print("\n[red]❌ No valid prompts could be loaded[/red]")

                return prompts_list

            else:
                # Load prompts individually
                num_prompts = self._int_prompt_with_validation("How many prompts do you want to use?", default=2, min_value=2, max_value=10)

                prompts_list = []
                for i in range(1, num_prompts + 1):
                    self.console.print(f"\n[bold]=== Prompt {i}/{num_prompts} ===[/bold]")

                    # Use the single prompt loader
                    prompt_text = self._get_custom_prompt()

                    # Ask for prefix
                    use_prefix = Confirm.ask(f"Add prefix to keys from prompt {i}?", default=False)
                    prefix_word = ""
                    if use_prefix:
                        prefix_word = Prompt.ask(f"Prefix for prompt {i}", default=f"p{i}")
                        self.console.print(f"[dim]✓ Keys will be prefixed with '{prefix_word}_'[/dim]")

                    # Extract expected keys from prompt
                    from ..annotators.json_cleaner import extract_expected_keys
                    expected_keys = extract_expected_keys(prompt_text)

                    prompts_list.append((prompt_text, expected_keys, prefix_word))

                self.console.print(f"\n[bold green]✓ Configured {len(prompts_list)} prompts[/bold green]")
                return prompts_list

        else:
            # Fallback for non-Rich environments
            return []

    def _get_prompt_template(self, template_type: str) -> str:
        """Get predefined prompt template"""
        templates = {
            "1": "Classify the following text into one of these categories: {categories}\n\nText: {text}\n\nCategory:",
            "2": "Extract all named entities from the text.\n\nText: {text}\n\nEntities:",
            "3": "Analyze the sentiment of this text (positive/negative/neutral).\n\nText: {text}\n\nSentiment:",
            "4": "Summarize the following text in one sentence.\n\nText: {text}\n\nSummary:"
        }
        return templates.get(template_type, "Analyze: {text}")

    def _extract_annotation_schema(self, prompt_text: str) -> Dict[str, List[str]]:
        """Extract annotation keys and their possible values from the prompt"""
        # First, use the improved extract_expected_keys to get all keys
        keys = extract_expected_keys(prompt_text)
        schema = {key: [] for key in keys}

        # Try to extract possible values for each key from the prompt description
        try:
            # Look for value descriptions in the prompt (e.g., "key": "value1" if ...; "value2" if ...)
            for key in keys:
                # Pattern to find value enumerations for this key
                key_pattern = rf'"{key}":\s*"([^"]+)"'
                matches = re.findall(key_pattern, prompt_text)

                if matches:
                    # Extract values from the description
                    values = []
                    for match in matches:
                        # Look for quoted values in the description
                        value_matches = re.findall(r'"([a-zA-Z_][a-zA-Z0-9_]*)"', match)
                        values.extend(value_matches)

                    if values:
                        schema[key] = list(set(values))  # Remove duplicates
        except:
            pass

        return schema

    def _display_training_strategy_explanation(self, prompt_text: str):
        """Display training strategy explanation with real examples from the prompt"""
        # Extract schema with keys and values from the prompt
        schema = self._extract_annotation_schema(prompt_text)

        if HAS_RICH and self.console:
            self.console.print("\n[bold]Training Strategy:[/bold]")
            self.console.print("How do you want to create training labels from annotations?")
            self.console.print()

            if schema:
                # Generate examples based on actual schema
                keys_list = list(schema.keys())
                multi_label_example = ", ".join(keys_list[:3]) if len(keys_list) >= 3 else ", ".join(keys_list)

                # For single-label, show real VALUE examples from the schema
                single_label_examples = []
                for key, values in list(schema.items())[:2]:  # Take first 2 keys
                    if values:
                        # Use real values from the prompt
                        for val in values[:2]:  # Take first 2 values per key
                            single_label_examples.append(f"{key}_{val}")
                    else:
                        # Fallback if no values detected
                        single_label_examples.append(f"{key}_valueX")

                single_label_example = ", ".join(single_label_examples[:3]) if single_label_examples else "key_value1, key_value2"

                self.console.print("• [cyan]single-label[/cyan]: Create ONE BINARY model per label VALUE")
                self.console.print(f"  [dim]Example: {single_label_example} (each = yes/no)[/dim]")
                self.console.print("  [dim]→ Many simple models, each predicting presence/absence of ONE specific label[/dim]")
                self.console.print()
                self.console.print("• [cyan]multi-label[/cyan]: Create ONE MULTI-CLASS model per annotation KEY")
                self.console.print(f"  [dim]Example: One model for {multi_label_example}[/dim]")
                self.console.print("  [dim]→ Few complex models, each predicting MULTIPLE possible values for its key[/dim]")
            else:
                # Fallback if we can't extract keys
                self.console.print("• [cyan]single-label[/cyan]: Create ONE BINARY model per label VALUE")
                self.console.print("  [dim]Example: theme_defense, theme_economy, sentiment_positive (each = yes/no)[/dim]")
                self.console.print("  [dim]→ Many simple models, each predicting presence/absence of ONE specific label[/dim]")
                self.console.print()
                self.console.print("• [cyan]multi-label[/cyan]: Create ONE MULTI-CLASS model per annotation KEY")
                self.console.print("  [dim]Example: One model for themes, one for sentiment, one for parties[/dim]")
                self.console.print("  [dim]→ Few complex models, each predicting MULTIPLE possible values for its key[/dim]")

            self.console.print()
        else:
            print("\nTraining Strategy:")
            print("• single-label: One binary model per label value")
            print("• multi-label: One multi-class model per annotation key")

    def _display_training_modes_explanation(self):
        """Display detailed explanation of training modes for social science researchers"""
        if HAS_RICH and self.console:
            self.console.print("\n[bold cyan]📚 Training Modes Guide[/bold cyan]\n")

            # Explain parameters first
            self.console.print("[bold yellow]Key Parameters:[/bold yellow]")
            self.console.print("  • [cyan]Epochs[/cyan]: Number of times the model sees the entire dataset")
            self.console.print("    [dim]Example: With 1000 tweets and 10 epochs, the model learns from 10,000 examples[/dim]")
            self.console.print("  • [cyan]Batch size[/cyan]: Number of examples processed simultaneously")
            self.console.print("    [dim]Example: Batch=16 → the model analyzes 16 news articles in parallel[/dim]")
            self.console.print("  • [cyan]Learning rate[/cyan]: Speed of learning (2e-5 = 0.00002)")
            self.console.print("    [dim]Too high → unstable learning; too low → slow learning[/dim]")
            self.console.print("  • [cyan]Warmup[/cyan]: Proportion of initial training with gradual learning rate increase")
            self.console.print("    [dim]0.1 = the first 10% of examples serve to 'warm up' the model[/dim]\n")

            # Create comparison table
            table = Table(title="Mode Comparison", border_style="blue", show_header=True)
            table.add_column("Mode", style="bold cyan", width=12)
            table.add_column("Epochs", justify="center", style="yellow", width=8)
            table.add_column("Batch", justify="center", style="yellow", width=8)
            table.add_column("L.Rate", justify="center", style="yellow", width=10)
            table.add_column("Models", justify="center", style="yellow", width=9)
            table.add_column("Use Case", style="green", width=45)

            table.add_row(
                "Quick", "10", "32", "5e-5", "2",
                "Quick test of a Facebook posts dataset (500 ex.)"
            )
            table.add_row(
                "Balanced", "10", "16", "2e-5", "5",
                "Parliamentary speeches classification (2000 ex.)"
            )
            table.add_row(
                "Thorough", "20", "8", "1e-5", "10",
                "Qualitative interview analysis (500-1000 ex.)"
            )
            table.add_row(
                "Custom", "?", "?", "?", "?",
                "Manual configuration for specific cases"
            )

            self.console.print(table)

            self.console.print("\n[bold green]💡 Recommendations:[/bold green]")
            self.console.print("  • [cyan]Quick[/cyan]: Fast prototyping, verify everything works")
            self.console.print("  • [cyan]Balanced[/cyan]: Best compromise for most projects (recommended)")
            self.console.print("  • [cyan]Thorough[/cyan]: Small or imbalanced dataset, academic publication")
            self.console.print("  • [cyan]Custom[/cyan]: You know exactly what you want\n")

    def _get_training_preset(self, mode: str) -> Dict[str, Any]:
        """Get training configuration preset"""
        presets = {
            "quick": {
                "epochs": 10,
                "batch_size": 32,
                "learning_rate": 5e-5,
                "warmup_ratio": 0.1,
                "models_to_test": 2
            },
            "balanced": {
                "epochs": 10,
                "batch_size": 16,
                "learning_rate": 2e-5,
                "warmup_ratio": 0.1,
                "models_to_test": 5
            },
            "thorough": {
                "epochs": 20,
                "batch_size": 8,
                "learning_rate": 1e-5,
                "warmup_ratio": 0.2,
                "models_to_test": 10
            }
        }

        if mode == "custom":
            if HAS_RICH and self.console:
                return {
                    "epochs": IntPrompt.ask("Number of epochs", default=10),
                    "batch_size": IntPrompt.ask("Batch size", default=16),
                    "learning_rate": FloatPrompt.ask("Learning rate", default=2e-5),
                    "warmup_ratio": FloatPrompt.ask("Warmup ratio", default=0.1),
                    "models_to_test": IntPrompt.ask("Models to benchmark", default=5)
                }
            else:
                return presets["balanced"]

        return presets.get(mode, presets["balanced"])

    def _display_configuration_summary(self, config: Dict[str, Any]):
        """Display configuration summary in professional format"""
        if HAS_RICH and self.console:
            table = Table(title="📋 Configuration Summary", border_style="green", show_lines=True)
            table.add_column("Setting", style="cyan", width=20)
            table.add_column("Value", style="white")

            for key, value in config.items():
                if value is not None:
                    table.add_row(key.replace('_', ' ').title(), str(value))

            self.console.print(table)
        else:
            print("\n=== Configuration Summary ===")
            for key, value in config.items():
                if value is not None:
                    print(f"{key}: {value}")

    def _save_profile(self, name: str, config: Dict[str, Any]):
        """Save configuration as profile"""
        profile = ExecutionProfile(
            name=name,
            created_at=datetime.now(),
            last_used=datetime.now(),
            configuration=config
        )
        self.profile_manager.save_profile(profile)

        if HAS_RICH and self.console:
            self.console.print(f"[green]✓ Profile '{name}' saved successfully[/green]")

    def _recommend_models_for_training(self, dataset_path: str, text_column: str, sample_size: int = 200) -> Dict[str, Any]:
        """Recommend models based on detected language distribution in the dataset."""
        fallback = {
            'language_code': 'multilingual',
            'language_name': 'multilingual',
            'candidate_models': ['bert-base-multilingual-cased', 'xlm-roberta-base', 'mdeberta-v3-base'],
            'default_model': 'bert-base-multilingual-cased',
        }

        if not HAS_PANDAS or not dataset_path or not Path(dataset_path).exists():
            return fallback

        try:
            suffix = Path(dataset_path).suffix.lower()
            if suffix == '.csv':
                df = pd.read_csv(dataset_path)
            elif suffix in {'.xls', '.xlsx'}:
                df = pd.read_excel(dataset_path)
            elif suffix in {'.parquet'}:
                df = pd.read_parquet(dataset_path)
            else:
                return fallback
        except Exception as exc:
            logging.warning("Quick start: unable to read dataset for language detection (%s)", exc)
            return fallback

        if text_column not in df.columns:
            logging.warning("Quick start: text column '%s' not found in dataset", text_column)
            return fallback

        texts = df[text_column].dropna().astype(str)
        if texts.empty:
            return fallback

        sampled = texts.sample(min(len(texts), sample_size), random_state=42) if len(texts) > sample_size else texts
        detections = self.language_detector.detect_batch(sampled.tolist(), parallel=False)

        language_counts = Counter()
        for detection in detections:
            if not detection:
                continue
            lang = detection.get('language')
            confidence = detection.get('confidence', 0)
            if lang and confidence >= 0.5:
                language_counts[lang] += 1

        if not language_counts:
            return fallback

        top_language, _ = language_counts.most_common(1)[0]
        candidate_models = self.language_detector.get_recommended_models(top_language)
        default_model = candidate_models[0] if candidate_models else fallback['default_model']
        language_name = self.language_detector.get_language_name(top_language)

        return {
            'language_code': top_language,
            'language_name': language_name,
            'candidate_models': candidate_models or fallback['candidate_models'],
            'default_model': default_model,
        }

    def _execute_quick_start(self, config: Dict[str, Any]):
        """Execute the quick start pipeline"""
        dataset_path = config['dataset']
        text_column = config['text_column']
        identifier_column = config.get('identifier_column')
        model_entry = config['model']
        if isinstance(model_entry, ModelInfo):
            model_info = model_entry
        elif isinstance(model_entry, dict):
            model_info = ModelInfo(**model_entry)
        else:
            model_info = ModelInfo(name=str(model_entry), provider='ollama', is_available=True)
        api_key = config.get('api_key')
        prompt_config = config.get('prompt')
        training_preset = config.get('training_config', {})
        annotation_settings = config.get('annotation_settings', {})
        # Persist user training choices passed from the wizard
        run_training = bool(config.get('run_training', False))
        training_strategy = config.get('training_strategy')
        label_strategy = config.get('label_strategy')
        training_annotation_keys = config.get('training_annotation_keys')

        data_format = Path(dataset_path).suffix.lower().lstrip('.') if dataset_path else 'csv'
        if data_format == '':
            data_format = 'csv'
        if data_format not in {'csv', 'json', 'jsonl', 'excel', 'xlsx', 'xls', 'parquet'}:
            data_format = 'csv'

        # Prepare prompts payload
        prompts_payload: List[Dict[str, Any]] = []
        if isinstance(prompt_config, list):
            # Multi-prompt returns list of tuples (prompt, keys, prefix)
            for item in prompt_config:
                if isinstance(item, tuple):
                    prompt_text, expected_keys, prefix = item
                elif isinstance(item, dict):
                    prompt_text = item.get('prompt')
                    expected_keys = item.get('expected_keys', [])
                    prefix = item.get('prefix', '')
                else:
                    continue
                prompts_payload.append({
                    'prompt': prompt_text,
                    'expected_keys': expected_keys or [],
                    'prefix': prefix or ''
                })
        else:
            prompt_text = prompt_config or ""
            prompts_payload.append({
                'prompt': prompt_text,
                'expected_keys': extract_expected_keys(prompt_text) if prompt_text else [],
                'prefix': ''
            })

        # Use recommended training model if available from earlier detection
        if config.get('recommended_training_model'):
            default_model = config['recommended_training_model']
            detected_language = ', '.join([l.upper() for l in config.get('detected_languages', [])])
            candidate_models = [default_model]
            if HAS_RICH and self.console:
                self.console.print(f"[green]✓ Using recommended model from language detection: {default_model}[/green]")
        else:
            # Fallback to old recommendation method
            training_reco = self._recommend_models_for_training(dataset_path, text_column)
            candidate_models = training_reco['candidate_models']
            default_model = training_reco['default_model']
            detected_language = training_reco['language_name']

        # Quick start focuses on a single recommended model for speed/stability
        models_to_test = [default_model]
        benchmark_mode = False

        annotations_dir = self.settings.paths.data_dir / 'annotations'
        annotations_dir.mkdir(parents=True, exist_ok=True)
        safe_model_name = model_info.name.replace(':', '_')
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        default_output_path = annotations_dir / f"{Path(dataset_path).stem}_{safe_model_name}_annotations_{timestamp}.csv"

        annotation_mode = 'api' if model_info.provider in {'openai', 'anthropic', 'google', 'custom'} else 'local'

        pipeline_config = {
            'mode': 'file',
            'data_source': data_format,
            'data_format': data_format,
            'file_path': dataset_path,
            'text_column': text_column,
            'text_columns': [text_column],
            'annotation_column': 'annotation',
            'identifier_column': identifier_column,
            'lang_column': config.get('lang_column'),
            'run_annotation': True,
            'annotation_mode': annotation_mode,
            'annotation_provider': model_info.provider,
            'annotation_model': model_info.name,
            'api_key': api_key,
            'prompts': prompts_payload,
            'annotation_sample_size': annotation_settings.get('annotation_sample_size'),
            'annotation_sampling_strategy': annotation_settings.get('annotation_sampling_strategy', 'head'),
            'annotation_sample_seed': annotation_settings.get('annotation_sample_seed', 42),
            'max_tokens': annotation_settings.get('max_tokens'),
            'max_workers': 1,
            'num_processes': 1,
            'use_parallel': False,
            'warmup': False,
            'disable_tqdm': True,  # Disable tqdm to avoid duplicate progress bars
            'output_format': 'csv',
            'output_path': str(default_output_path),
            'run_validation': False,
            'run_training': run_training,  # Based on user choice
            'training_strategy': training_strategy,
            'label_strategy': label_strategy,
            'training_annotation_keys': training_annotation_keys,
            'benchmark_mode': benchmark_mode,
            'models_to_test': models_to_test,
            'auto_select_best': True,
            'max_epochs': training_preset.get('epochs', 10),
            'batch_size': training_preset.get('batch_size', 16),
            'learning_rate': training_preset.get('learning_rate', 2e-5),
            'run_deployment': False,
            'training_model_type': models_to_test[0] if not benchmark_mode else default_model,
        }

        # Ensure local model options honour the requested token budget
        user_max_tokens = annotation_settings.get('max_tokens')
        if user_max_tokens:
            options = pipeline_config.get('options', {})
            options.setdefault('num_predict', user_max_tokens)
            pipeline_config['options'] = options

        # ============================================================
        # REPRODUCIBILITY METADATA
        # ============================================================
        if HAS_RICH and self.console:
            self.console.print("\n[bold cyan]📋 Reproducibility & Metadata[/bold cyan]")
            self.console.print("[yellow]⚠️  IMPORTANT: Save parameters for two critical purposes:[/yellow]\n")

            self.console.print("  [green]1. Resume Capability[/green]")
            self.console.print("     • Continue this annotation if it stops or crashes")
            self.console.print("     • Annotate additional rows later with same settings")
            self.console.print("     • Access via 'Resume/Relaunch Annotation' workflow\n")

            self.console.print("  [green]2. Scientific Reproducibility[/green]")
            self.console.print("     • Document exact parameters for research papers")
            self.console.print("     • Reproduce identical annotations in the future")
            self.console.print("     • Track model version, prompts, and all settings\n")

            self.console.print("  [red]⚠️  If you choose NO:[/red]")
            self.console.print("     • You CANNOT resume this annotation later")
            self.console.print("     • You CANNOT relaunch with same parameters")
            self.console.print("     • Parameters will be lost forever\n")

            save_metadata = Confirm.ask(
                "[bold yellow]Save annotation parameters to JSON file?[/bold yellow]",
                default=True
            )

            # Validation tool export option
            self.console.print("\n[bold cyan]📤 Validation Tool Export[/bold cyan]")
            self.console.print("[dim]Export annotations to JSONL format for human validation[/dim]\n")

            self.console.print("[yellow]Available validation tools:[/yellow]")
            self.console.print("  • [cyan]Doccano[/cyan] - Simple, lightweight NLP annotation tool")
            self.console.print("  • [cyan]Label Studio[/cyan] - Advanced, feature-rich annotation platform")
            self.console.print("  • Both are open-source and free\n")

            export_enabled = Confirm.ask(
                "[bold yellow]Export to validation tool?[/bold yellow]",
                default=False
            )

            export_to_doccano = False
            export_to_labelstudio = False
            export_sample_size = None

            if export_enabled:
                # Ask which tool to use
                self.console.print("\n[yellow]Select validation tool:[/yellow]")
                export_tool = Prompt.ask(
                    "[bold yellow]Which tool?[/bold yellow]",
                    choices=["doccano", "labelstudio"],
                    default="doccano"
                )

                export_to_doccano = export_tool == "doccano"
                export_to_labelstudio = export_tool == "labelstudio"

                # Step 2b: If Label Studio, ask export method
                labelstudio_direct_export = False
                labelstudio_api_url = None
                labelstudio_api_key = None

                if export_to_labelstudio:
                    self.console.print("\n[yellow]Label Studio export method:[/yellow]")
                    self.console.print("  • [cyan]jsonl[/cyan] - Export to JSONL file (manual import)")
                    if HAS_REQUESTS:
                        self.console.print("  • [cyan]direct[/cyan] - Direct export to Label Studio via API\n")
                        export_choices = ["jsonl", "direct"]
                    else:
                        self.console.print("  • [dim]direct[/dim] - Direct export via API [dim](requires 'requests' library)[/dim]\n")
                        export_choices = ["jsonl"]

                    export_method = Prompt.ask(
                        "[bold yellow]Export method[/bold yellow]",
                        choices=export_choices,
                        default="jsonl"
                    )

                    if export_method == "direct":
                        labelstudio_direct_export = True

                        self.console.print("\n[cyan]Label Studio API Configuration:[/cyan]")
                        labelstudio_api_url = Prompt.ask(
                            "Label Studio URL",
                            default="http://localhost:8080"
                        )

                        labelstudio_api_key = Prompt.ask(
                            "API Key (from Label Studio Account & Settings)"
                        )

                # Ask about LLM predictions inclusion
                self.console.print("\n[yellow]Include LLM predictions in export?[/yellow]")
                self.console.print("  • [cyan]with[/cyan] - Include LLM annotations as predictions (for review/correction)")
                self.console.print("  • [cyan]without[/cyan] - Export only data without predictions (for manual annotation)")
                self.console.print("  • [cyan]both[/cyan] - Create two files/projects: one with and one without predictions\n")

                prediction_mode = Prompt.ask(
                    "[bold yellow]Prediction mode[/bold yellow]",
                    choices=["with", "without", "both"],
                    default="with"
                )

                # Ask how many sentences to export
                self.console.print("\n[yellow]How many annotated sentences to export?[/yellow]")
                self.console.print("  • [cyan]all[/cyan] - Export all annotated sentences")
                self.console.print("  • [cyan]representative[/cyan] - Representative sample (stratified by labels)")
                self.console.print("  • [cyan]number[/cyan] - Specify exact number\n")

                sample_choice = Prompt.ask(
                    "[bold yellow]Export sample[/bold yellow]",
                    choices=["all", "representative", "number"],
                    default="all"
                )

                if sample_choice == "all":
                    export_sample_size = "all"
                elif sample_choice == "representative":
                    export_sample_size = "representative"
                else:  # number
                    export_sample_size = self._int_prompt_with_validation(
                        "Number of sentences to export",
                        100,
                        1,
                        999999
                    )
        else:
            save_metadata = False
            export_to_doccano = False
            export_to_labelstudio = False
            export_sample_size = None
            labelstudio_direct_export = False
            labelstudio_api_url = None
            labelstudio_api_key = None

        # Execute pipeline
        try:
            # Save metadata before execution
            if save_metadata:
                import json

                # Build comprehensive metadata
                metadata = {
                    'annotation_session': {
                        'timestamp': timestamp,
                        'tool_version': 'LLMTool v1.0',
                        'workflow': 'Quick Start - Intelligent Pipeline Setup'
                    },
                    'data_source': {
                        'file_path': dataset_path,
                        'file_name': Path(dataset_path).name,
                        'data_format': data_format,
                        'text_column': text_column,
                        'identifier_column': identifier_column,
                        'detected_language': detected_language,
                        'total_rows': annotation_settings.get('annotation_sample_size') if annotation_settings.get('annotation_sample_size') else 'all',
                        'sampling_strategy': annotation_settings.get('annotation_sampling_strategy', 'head'),
                        'sample_seed': annotation_settings.get('annotation_sample_seed', 42)
                    },
                    'model_configuration': {
                        'provider': model_info.provider,
                        'model_name': model_info.name,
                        'annotation_mode': annotation_mode,
                        'temperature': annotation_settings.get('temperature'),
                        'max_tokens': annotation_settings.get('max_tokens'),
                        'top_p': annotation_settings.get('top_p'),
                        'top_k': annotation_settings.get('top_k')
                    },
                    'prompts': [
                        {
                            'prompt_content': p['prompt'],
                            'expected_keys': p['expected_keys'],
                            'prefix': p['prefix']
                        }
                        for p in prompts_payload
                    ],
                    'processing_configuration': {
                        'parallel_workers': pipeline_config.get('num_processes', 1),
                        'batch_size': pipeline_config.get('batch_size', 16)
                    },
                    'training_configuration': {
                        'run_training': run_training,
                        'training_strategy': training_strategy,
                        'label_strategy': label_strategy,
                        'training_annotation_keys': training_annotation_keys,
                        'benchmark_mode': benchmark_mode,
                        'training_model': default_model,
                        'max_epochs': training_preset.get('epochs', 10),
                        'batch_size': training_preset.get('batch_size', 16),
                        'learning_rate': training_preset.get('learning_rate', 2e-5)
                    },
                    'output': {
                        'output_path': str(default_output_path),
                        'output_format': 'csv'
                    },
                    'export_preferences': {
                        'export_to_doccano': export_to_doccano,
                        'export_to_labelstudio': export_to_labelstudio,
                        'export_sample_size': export_sample_size,
                        'prediction_mode': prediction_mode if (export_to_doccano or export_to_labelstudio) else 'with',
                        'labelstudio_direct_export': labelstudio_direct_export if export_to_labelstudio else False,
                        'labelstudio_api_url': labelstudio_api_url if export_to_labelstudio else None,
                        'labelstudio_api_key': labelstudio_api_key if export_to_labelstudio else None
                    }
                }

                # Save metadata JSON
                metadata_filename = f"{Path(dataset_path).stem}_{safe_model_name}_metadata_{timestamp}.json"
                metadata_path = annotations_dir / metadata_filename

                with open(metadata_path, 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, indent=2, ensure_ascii=False)

                if HAS_RICH and self.console:
                    self.console.print(f"\n[bold green]✅ Metadata saved for reproducibility[/bold green]")
                    self.console.print(f"[bold cyan]📋 Metadata File:[/bold cyan]")
                    self.console.print(f"   {metadata_path}\n")
                else:
                    print(f"\n✅ Metadata saved: {metadata_path}\n")

            # Execute with real-time progress tracking
            print("\n🚀 Starting pipeline...\n")

            # Create pipeline controller
            from ..pipelines.pipeline_controller import PipelineController
            pipeline_with_progress = PipelineController(
                settings=self.settings
            )

            # Use the enhanced Rich progress manager with JSON display
            if HAS_RICH and self.console:
                # Use the unified RichProgressManager with compact mode
                from ..utils.rich_progress_manager import RichProgressManager
                from ..pipelines.enhanced_pipeline_wrapper import EnhancedPipelineWrapper

                # Use RichProgressManager in compact mode for elegant display
                with RichProgressManager(
                    show_json_every=1,  # Show JSON sample for every annotation
                    compact_mode=False   # Display full preview panels
                ) as progress_manager:
                    # Wrap the pipeline for enhanced JSON tracking
                    enhanced_pipeline = EnhancedPipelineWrapper(
                        pipeline_with_progress,
                        progress_manager
                    )

                    # Run pipeline
                    state = enhanced_pipeline.run_pipeline(pipeline_config)

                    # Check for errors
                    if state.errors:
                        error_msg = state.errors[0]['error'] if state.errors else "Pipeline failed"
                        self.console.print(f"\n[bold red]❌ Error:[/bold red] {error_msg}")
                        raise Exception(error_msg)
            else:
                # Fallback without Rich
                state = pipeline_with_progress.run_pipeline(pipeline_config)
        except Exception as exc:
            message = f"❌ Quick start pipeline failed: {exc}"
            if HAS_RICH and self.console:
                self.console.print(f"[red]{message}[/red]")
            else:
                print(message)
            logging.exception("Quick start pipeline failed")
            return

        annotation_results = state.annotation_results or {}
        training_results = state.training_results or {}
        output_file = annotation_results.get('output_file', str(default_output_path))

        if HAS_RICH and self.console:
            self.console.print("\n[bold green]✅ Pipeline completed successfully![/bold green]")
            self.console.print(f"📄 Annotated file: [cyan]{output_file}[/cyan]")
            self.console.print(f"🗣️ Detected language: [cyan]{detected_language}[/cyan]")
            if training_results:
                best_model = training_results.get('best_model') or training_results.get('model_name')
                best_f1 = training_results.get('best_f1_macro')
                if best_model:
                    self.console.print(f"🏆 Best model: [cyan]{best_model}[/cyan]")
                if best_f1 is not None:
                    self.console.print(f"📊 Macro F1: [cyan]{best_f1:.3f}[/cyan]")
        else:
            print("✅ Pipeline completed successfully!")
            print(f"Annotated file: {output_file}")
            print(f"Detected language: {detected_language}")
            if training_results:
                best_model = training_results.get('best_model') or training_results.get('model_name')
                best_f1 = training_results.get('best_f1_macro')
                if best_model:
                    print(f"Best model: {best_model}")
                if best_f1 is not None:
                    print(f"Macro F1: {best_f1:.3f}")

        # Export to Doccano JSONL if requested
        if export_to_doccano and HAS_RICH and self.console:
            # Build prompt_configs from prompts_payload for Quick Start
            prompt_configs_for_export = []
            for p in prompts_payload:
                prompt_configs_for_export.append({
                    'prompt': {
                        'keys': p.get('expected_keys', []),
                        'content': p.get('prompt', '')
                    },
                    'prefix': p.get('prefix', '')
                })

            self._export_to_doccano_jsonl(
                output_file=output_file,
                text_column=text_column,
                prompt_configs=prompt_configs_for_export,
                data_path=Path(dataset_path),
                timestamp=timestamp,
                sample_size=export_sample_size
            )

        # Export to Label Studio if requested
        if export_to_labelstudio and HAS_RICH and self.console:
            # Build prompt_configs from prompts_payload for Quick Start
            prompt_configs_for_export = []
            for p in prompts_payload:
                prompt_configs_for_export.append({
                    'prompt': {
                        'keys': p.get('expected_keys', []),
                        'content': p.get('prompt', '')
                    },
                    'prefix': p.get('prefix', '')
                })

            if labelstudio_direct_export:
                # Direct export to Label Studio via API
                self._export_to_labelstudio_direct(
                    output_file=output_file,
                    text_column=text_column,
                    prompt_configs=prompt_configs_for_export,
                    data_path=Path(dataset_path),
                    timestamp=timestamp,
                    sample_size=export_sample_size,
                    prediction_mode=prediction_mode,
                    api_url=labelstudio_api_url,
                    api_key=labelstudio_api_key
                )
            else:
                # Export to JSONL file for manual import
                self._export_to_labelstudio_jsonl(
                    output_file=output_file,
                    text_column=text_column,
                    prompt_configs=prompt_configs_for_export,
                    data_path=Path(dataset_path),
                    timestamp=timestamp,
                    sample_size=export_sample_size,
                    prediction_mode=prediction_mode
                )

        # Persist last run details for other wizards
        self.last_annotation_config = {
            'data_path': dataset_path,
            'data_format': data_format,
            'text_column': text_column,
            'annotation_column': 'annotation',
            'mode': annotation_mode,
            'provider': model_info.provider,
            'model': model_info.name,
            'output_path': output_file,
            'annotation_sample_size': annotation_settings.get('annotation_sample_size'),
            'annotation_sampling_strategy': annotation_settings.get('annotation_sampling_strategy', 'head'),
        }

    def run(self):
        """Main run loop for the advanced CLI"""
        self.display_banner()

        while True:
            try:
                choice = self.get_main_menu_choice()

                if choice == "1":
                    self.llm_annotation_studio()
                elif choice == "2":
                    self.quick_start_wizard()
                elif choice == "3":
                    self.training_studio()
                elif choice == "4":
                    self.bert_annotation_studio()
                elif choice == "5":
                    self.validation_lab()
                elif choice == "6":
                    self.profile_manager_ui()
                elif choice == "7":
                    self.show_documentation()
                elif choice == "0":
                    if HAS_RICH and self.console:
                        self.console.print("\n[bold cyan]Thank you for using LLMTool! 👋[/bold cyan]\n")
                    else:
                        print("\nThank you for using LLMTool!\n")
                    sys.exit(0)

                # Update session
                self.current_session['operations_count'] += 1
                self.current_session['last_operation'] = choice

            except KeyboardInterrupt:
                if HAS_RICH and self.console:
                    if Confirm.ask("\n[yellow]Exit LLMTool?[/yellow]", default=False):
                        self.console.print("\n[bold cyan]Goodbye! 👋[/bold cyan]\n")
                        sys.exit(0)
                else:
                    if input("\nExit? (y/n): ").lower() == 'y':
                        print("Goodbye!\n")
                        sys.exit(0)
            except Exception as e:
                self.logger.error(f"Error: {str(e)}", exc_info=True)
                if HAS_RICH and self.console:
                    self.console.print(f"[bold red]Error: {str(e)}[/bold red]")
                else:
                    print(f"Error: {str(e)}")

    # Placeholder methods for other menu options
    def llm_annotation_studio(self):
        """
        The Annotator - Complete annotation workflow without training.

        Features:
        - Auto-detection of prompts from prompts/ folder
        - Automatic JSON key extraction
        - Multi-prompt with prefix system
        - Incremental save for ALL data formats
        - Automatic ID creation and tracking
        - Organized output structure
        """
        # Display ASCII logo only
        self._display_ascii_logo()

        # Display mode-specific banner
        self._display_mode_banner('annotator')

        # Display personalized mode info
        self._display_section_header(
            "🎨 The Annotator - Zero-Shot LLM Annotation → Label Studio/Doccano Export",
            "Professional zero-shot annotation with Ollama/OpenAI/Claude, automatic JSON repair, and export to review platforms",
            mode_info={
                'workflow': 'Data → Prompt Wizard → LLM Annotate (Parallel) → JSON Repair → Export (Doccano/Label Studio)',
                'capabilities': ['Ollama/OpenAI/Claude Support', 'Prompt Wizard', '200K Context', 'Multi-Label Categories', 'NER', 'Pydantic Validation'],
                'input': 'Raw text data (CSV/Excel/JSON/SQL)',
                'output': 'Annotated JSON + Doccano JSONL + Label Studio JSON (API or file)',
                'best_for': 'Zero-shot annotation with LLMs, human review workflows, data labeling for training',
                'duration': '~2-20 min (depends on dataset size, LLM speed, and context length)'
            }
        )

        if HAS_RICH and self.console:
            # Get smart suggestions
            suggestions = self._get_smart_suggestions()

            # Create workflow menu table
            from rich.table import Table
            workflow_table = Table(show_header=False, box=None, padding=(0, 2))
            workflow_table.add_column("Option", style="cyan", width=8)
            workflow_table.add_column("Description")

            workflows = [
                ("1", "🔄 Resume/Relaunch Annotation (Use saved parameters or resume incomplete)"),
                ("2", "🎯 Smart Annotate (Guided wizard with all options)"),
                ("3", "🗄️  Database Annotator (PostgreSQL direct)"),
                ("4", "🗑️  Clean Old Metadata (Delete saved parameters)"),
                ("0", "⬅️  Back to main menu")
            ]

            for option, desc in workflows:
                workflow_table.add_row(f"[bold cyan]{option}[/bold cyan]", desc)

            # Display panel with suggestions
            panel = Panel(
                workflow_table,
                title="[bold]🎨 The Annotator[/bold]",
                subtitle=f"[dim]{suggestions}[/dim]" if suggestions else None,
                border_style="cyan"
            )

            self.console.print("\n")
            self.console.print(panel)

            workflow = Prompt.ask(
                "\n[bold yellow]Select workflow[/bold yellow]",
                choices=["0", "1", "2", "3", "4"],
                default="2"
            )

            if workflow == "0":
                return
            elif workflow == "1":
                self._quick_annotate()
            elif workflow == "2":
                # CRITICAL: Ask user for session name first (like Training Arena)
                from datetime import datetime

                self.console.print("\n[bold cyan]═══════════════════════════════════════════════════════════════[/bold cyan]")
                self.console.print("[bold cyan]           📝 Session Name Configuration                       [/bold cyan]")
                self.console.print("[bold cyan]═══════════════════════════════════════════════════════════════[/bold cyan]\n")

                self.console.print("[bold]Why session names matter:[/bold]")
                self.console.print("  • [green]Organization:[/green] Easily identify annotation projects (e.g., 'sentiment_tweets', 'legal_documents')")
                self.console.print("  • [green]Traceability:[/green] Track your annotations across data, logs, and exports")
                self.console.print("  • [green]Collaboration:[/green] Team members understand what each session represents")
                self.console.print("  • [green]Audit trail:[/green] Timestamp ensures uniqueness\n")

                self.console.print("[dim]Format: {session_name}_{yyyymmdd_hhmmss}[/dim]")
                self.console.print("[dim]Example: sentiment_analysis_20251008_143022[/dim]\n")

                # Ask for user-defined session name
                user_session_name = Prompt.ask(
                    "[bold yellow]Enter a descriptive name for this annotation session[/bold yellow]",
                    default="annotation_session"
                ).strip()

                # Sanitize the user input (remove special chars, replace spaces with underscores)
                user_session_name = user_session_name.replace(' ', '_').replace('/', '_').replace('\\', '_')
                user_session_name = ''.join(c for c in user_session_name if c.isalnum() or c in ['_', '-'])

                # Create full session ID with timestamp
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                session_id = f"{user_session_name}_{timestamp}"

                self.console.print(f"\n[bold green]✓ Session ID:[/bold green] [cyan]{session_id}[/cyan]")
                self.console.print(f"[dim]This ID will be used consistently across all data, logs, and exports[/dim]\n")

                # Pass session_id to _smart_annotate
                self._smart_annotate(session_id=session_id)
            elif workflow == "3":
                self._database_annotator()
            elif workflow == "4":
                self._clean_metadata()
        else:
            print("\n=== The Annotator ===")
            print("LLM Tool annotates, you decide\n")
            print("1. Resume/Relaunch Annotation")
            print("2. Smart Annotate (Recommended)")
            print("3. Database Annotator")
            print("4. Clean Old Metadata")
            print("0. Back")
            choice = input("\nSelect workflow: ").strip()

            if choice == "1":
                self._quick_annotate()
            elif choice == "2":
                self._smart_annotate()
            elif choice == "3":
                self._database_annotator()
            elif choice == "4":
                self._clean_metadata()

    def training_studio(self):
        """Training studio bringing dataset builders and trainers together."""
        # Display ASCII logo only
        self._display_ascii_logo()

        # Display mode-specific banner
        self._display_mode_banner('arena')

        # Display personalized mode info
        self._display_section_header(
            "🎮 Training Arena - Train 50+ Models (BERT/RoBERTa/DeBERTa) with Multi-Label & Benchmarking",
            "Professional model training with intelligent optimization, reinforcement learning, and comprehensive benchmarking",
            mode_info={
                'workflow': 'Load Data → Language Detection → Model Selection → Multi-Label Training → Reinforcement Learning → Benchmark',
                'capabilities': ['50+ Models (BERT/RoBERTa/DeBERTa/Longformer)', 'Multi-Label Classification', 'Parallel GPU/CPU', 'Class Imbalance Handling', 'Hard Negative Mining'],
                'input': 'Annotated CSV/JSON/JSONL/SQL with labels (single or multi-label)',
                'output': 'Trained models + Confusion matrices + F1 scores + Training summaries + Best model selection',
                'best_for': 'Production-ready model training with automatic optimization and comprehensive evaluation',
                'duration': '~5-30 min per model (benchmark mode: 30min-3hrs depending on data size)'
            }
        )

        if not (HAS_RICH and self.console):
            print("\nTraining Arena requires the Rich interface. Launch `llm-tool --simple` for basic commands.")
            return

        self._ensure_training_models_loaded()

        # NEW: Add resume/new menu BEFORE starting wizard
        self.console.print("\n[bold cyan]🎯 Training Session Options[/bold cyan]\n")

        session_options_table = Table(show_header=True, header_style="bold magenta", border_style="cyan", box=box.ROUNDED)
        session_options_table.add_column("Option", style="cyan bold", width=10)
        session_options_table.add_column("Description", style="white", width=70)

        session_options_table.add_row(
            "1",
            "🔄 Resume/Relaunch Training\n   Load saved parameters from previous training sessions"
        )
        session_options_table.add_row(
            "2",
            "🆕 New Training Session\n   Start fresh with dataset selection and configuration"
        )
        session_options_table.add_row(
            "3",
            "← Back to Main Menu"
        )

        self.console.print(session_options_table)
        self.console.print()

        session_choice = Prompt.ask(
            "[bold yellow]Select an option[/bold yellow]",
            choices=["1", "2", "3"],
            default="2"
        )

        if session_choice == "1":
            # Resume/Relaunch existing session
            self._resume_training_studio()
            return
        elif session_choice == "3":
            # Back to main menu
            return

        # Continue with NEW training session
        # CRITICAL: Ask user for session name first
        from datetime import datetime
        from llm_tool.utils.training_data_utils import TrainingDataSessionManager

        self.console.print("\n[bold cyan]═══════════════════════════════════════════════════════════════[/bold cyan]")
        self.console.print("[bold cyan]           📝 Session Name Configuration                       [/bold cyan]")
        self.console.print("[bold cyan]═══════════════════════════════════════════════════════════════[/bold cyan]\n")

        self.console.print("[bold]Why session names matter:[/bold]")
        self.console.print("  • [green]Organization:[/green] Easily identify experiments (e.g., 'baseline', 'improved_features')")
        self.console.print("  • [green]Traceability:[/green] Track your training runs across data, logs, and models")
        self.console.print("  • [green]Collaboration:[/green] Team members understand what each session represents")
        self.console.print("  • [green]Audit trail:[/green] Timestamp ensures uniqueness\n")

        self.console.print("[dim]Format: {session_name}_{yyyymmdd_hhmmss}[/dim]")
        self.console.print("[dim]Example: sentiment_analysis_20251008_143022[/dim]\n")

        # Ask for user-defined session name
        user_session_name = Prompt.ask(
            "[bold yellow]Enter a descriptive name for this training session[/bold yellow]",
            default="training_session"
        ).strip()

        # Sanitize the user input (remove special chars, replace spaces with underscores)
        user_session_name = user_session_name.replace(' ', '_').replace('/', '_').replace('\\', '_')
        user_session_name = ''.join(c for c in user_session_name if c.isalnum() or c in ['_', '-'])

        # Create full session ID with timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        session_id = f"{user_session_name}_{timestamp}"

        self.console.print(f"\n[bold green]✓ Session ID:[/bold green] [cyan]{session_id}[/cyan]")
        self.console.print(f"[dim]This ID will be used consistently across all data, logs, and models[/dim]\n")

        # Initialize session manager for comprehensive data distribution logging
        session_manager = TrainingDataSessionManager(session_id=session_id)

        # Initialize builder with session-based organization
        builder = TrainingDatasetBuilder(
            self.settings.paths.data_dir / "training_data",
            session_id=session_id
        )

        # Store for later use throughout the training session
        self.current_session_id = session_id
        self.current_session_manager = session_manager

        self._training_studio_show_model_catalog()

        # First, configure the dataset
        try:
            bundle = self._training_studio_dataset_wizard(builder)
        except Exception as exc:  # pylint: disable=broad-except
            self.console.print(f"[red]Dataset preparation failed:[/red] {exc}")
            self.logger.exception("Training Arena dataset preparation failed", exc_info=exc)
            return

        if bundle is None:
            self.console.print("[yellow]Training cancelled.[/yellow]")
            return

        # Show dataset summary
        self._training_studio_render_bundle_summary(bundle)

        # Note: Comprehensive logging will be done AFTER training/benchmark
        # to include complete information about what was used for what

        # Configure learning parameters and start training
        self.console.print("\n[bold cyan]Configuring learning parameters...[/bold cyan]\n")

        # Proceed directly to parameter configuration and training
        self._training_studio_confirm_and_execute(bundle, "quick")

    # ------------------------------------------------------------------
    # Training Arena helpers
    # ------------------------------------------------------------------
    def _training_studio_confirm_and_execute(
        self,
        bundle: TrainingDataBundle,
        mode: str,
        preloaded_config: Optional[Dict[str, Any]] = None,
        is_resume: bool = False
    ) -> None:
        """
        Display training parameters and ask for confirmation before execution.
        This ensures the user reviews all settings before starting training.

        Parameters
        ----------
        bundle : TrainingDataBundle
            The training data bundle
        mode : str
            Training mode (quick)
        preloaded_config : dict, optional
            Pre-loaded configuration from saved session (for resume/relaunch)
        is_resume : bool
            Whether this is a resume (True) or fresh start (False)
        """
        from datetime import datetime
        from rich.prompt import Confirm

        # STEP 1: Collect mode-specific parameters BEFORE showing config summary
        quick_params = None
        if mode == "quick" and not is_resume:
            quick_params = self._collect_quick_mode_parameters(bundle, preloaded_config)
            if quick_params is None:
                # User cancelled
                self.console.print("[yellow]Training cancelled by user.[/yellow]")
                return

        # STEP 2: Show configuration summary with modification loop
        while True:
            self.console.print("\n[bold cyan]═══════════════════════════════════════════════════════════════[/bold cyan]")
            self.console.print("[bold cyan]           ✅ Training Configuration Summary                     [/bold cyan]")
            self.console.print("[bold cyan]═══════════════════════════════════════════════════════════════[/bold cyan]\n")

            # Create configuration table
            config_table = Table(show_header=True, header_style="bold magenta", border_style="green", box=box.ROUNDED)
            config_table.add_column("Parameter", style="cyan bold", width=25)
            config_table.add_column("Value", style="white", width=60)

            # Dataset information
            config_table.add_row("📊 Dataset", str(bundle.primary_file.name) if bundle.primary_file else "—")
            config_table.add_row("📝 Format", bundle.strategy)
            config_table.add_row("📖 Text Column", bundle.text_column)
            config_table.add_row("🏷️  Label Column", bundle.label_column)

            if bundle.metadata.get('confirmed_languages'):
                langs = ', '.join([l.upper() for l in bundle.metadata['confirmed_languages']])
                config_table.add_row("🌍 Languages", langs)

            # Training mode
            config_table.add_row("🎯 Training Mode", "⚡ Quick Start - Fast training with defaults")

            # Mode-specific parameters
            if mode == "quick" and quick_params:
                # Check if per-language models were selected
                if quick_params.get('models_by_language'):
                    # Show each language's model
                    models_display = []
                    for lang, model in sorted(quick_params['models_by_language'].items()):
                        models_display.append(f"{lang}: {model}")
                    config_table.add_row("🤖 Selected Models", "\n".join(models_display))
                else:
                    # Single model for all languages
                    config_table.add_row("🤖 Selected Model", quick_params['model_name'])

                # Reinforced learning display
                if quick_params['reinforced_learning']:
                    rl_details = (
                        f"Yes\n"
                        f"  • F1 Threshold: {quick_params.get('rl_f1_threshold', 0.70):.2f}\n"
                        f"  • Oversample: {quick_params.get('rl_oversample_factor', 2.0):.1f}×\n"
                        f"  • Loss Weight: {quick_params.get('rl_class_weight_factor', 2.0):.1f}×"
                    )
                    config_table.add_row("🎓 Reinforced Learning", rl_details)
                else:
                    config_table.add_row("🎓 Reinforced Learning", "No")

                # Epochs display with reinforced learning info
                if quick_params['reinforced_learning']:
                    manual_rl_epochs = quick_params.get('manual_rl_epochs')
                    if manual_rl_epochs:
                        max_epochs = quick_params['epochs'] + manual_rl_epochs
                        config_table.add_row("⏱️  Epochs", f"{quick_params['epochs']} (up to {max_epochs} with reinforced learning)")
                    else:
                        config_table.add_row("⏱️  Epochs", f"{quick_params['epochs']} (up to {quick_params['epochs']}+auto with reinforced learning)")
                else:
                    config_table.add_row("⏱️  Epochs", str(quick_params['epochs']))
                config_table.add_row("📦 Batch Size", "16 (default)")
            elif mode == "quick":
                config_table.add_row("⏱️  Epochs", "Will be asked (default: 10)")
                config_table.add_row("📦 Batch Size", "16 (default)")

            # Statistics
            if bundle.metadata.get('text_length_stats'):
                stats = bundle.metadata['text_length_stats']
                avg_len = stats.get('avg_chars', stats.get('avg_length', 0))
                config_table.add_row("📏 Avg Text Length", f"{avg_len:.0f} characters")

            self.console.print(config_table)
            self.console.print()

            # Ask for confirmation
            confirm = Confirm.ask(
                "\n[bold yellow]Confirm these parameters?[/bold yellow]",
                default=True
            )

            if confirm:
                break
            else:
                # User wants to modify - ask what to modify for quick mode
                if mode == "quick":
                    self.console.print("\n[yellow]What would you like to modify?[/yellow]")

                    # Ask if user wants to modify base parameters
                    modify_base = Confirm.ask(
                        "[bold yellow]Modify base parameters (model, epochs)?[/bold yellow]",
                        default=False
                    )

                    modify_rl = False
                    if quick_params.get('reinforced_learning'):
                        modify_rl = Confirm.ask(
                            "[bold yellow]Modify reinforced learning parameters?[/bold yellow]",
                            default=False
                        )

                    if not modify_base and not modify_rl:
                        # User doesn't want to modify anything, ask again
                        self.console.print("[yellow]No modifications requested. Please confirm parameters again or modify them.[/yellow]\n")
                        continue

                    # Only re-collect if user wants to modify something
                    if modify_base or modify_rl:
                        self.console.print("\n[cyan]Modifying parameters...[/cyan]\n")
                        quick_params = self._collect_quick_mode_parameters(bundle, quick_params)
                        if quick_params is None:
                            self.console.print("[yellow]Training cancelled by user.[/yellow]")
                            return
                else:
                    self.console.print("[yellow]Modification not available for this mode. Training cancelled.[/yellow]")
                    return

        # STEP 3: Metadata is ALWAYS saved (mandatory for session persistence)
        # This ensures ALL training sessions are recallable for resume/relaunch
        save_metadata = True
        metadata_path = None

        if not is_resume:
            self.console.print("\n[bold cyan]📋 Reproducibility & Metadata[/bold cyan]")
            self.console.print("  [green]✓ Session metadata will be automatically saved for:[/green]")
            self.console.print("     • Resume capability if training is interrupted")
            self.console.print("     • Complete parameter tracking for reproducibility")
            self.console.print("     • Access via 'Resume/Relaunch Training' option\n")

        # STEP 4: Start training
        confirm_start = Confirm.ask(
            "\n[bold yellow]🚀 Start training now?[/bold yellow]",
            default=True
        )

        if not confirm_start:
            self.console.print("[yellow]Training cancelled by user.[/yellow]")
            return

        # Prepare COMPLETE model configuration for metadata (ALL MODES)
        # This ensures FULL reproducibility for quick, benchmark, and custom modes
        model_config = {
            # Core training mode
            'training_mode': mode,

            # Common hyperparameters
            'selected_model': preloaded_config.get('selected_model') if preloaded_config else (quick_params['model_name'] if quick_params else None),
            'epochs': preloaded_config.get('epochs') if preloaded_config else (quick_params['epochs'] if quick_params else None),
            'batch_size': preloaded_config.get('batch_size') if preloaded_config else 16,
            'learning_rate': preloaded_config.get('learning_rate') if preloaded_config else 2e-5,
            'early_stopping': True,
            'recommended_model': bundle.recommended_model if hasattr(bundle, 'recommended_model') else None,

            # Advanced training options (will be filled by each mode)
            'use_reinforcement': preloaded_config.get('use_reinforcement') if preloaded_config else (quick_params['reinforced_learning'] if quick_params else True),
            'reinforced_epochs': preloaded_config.get('reinforced_epochs') if preloaded_config else 10,
            'validation_split': preloaded_config.get('validation_split') if preloaded_config else 0.2,
            'test_split': preloaded_config.get('test_split') if preloaded_config else 0.1,
            'stratified_split': preloaded_config.get('stratified_split') if preloaded_config else True,

            # Benchmark-specific parameters (filled if mode=='benchmark')
            'selected_models': None,  # Will be filled by benchmark mode
            'selected_labels': None,  # Will be filled by benchmark mode
            'benchmark_category': None,  # Will be filled if multi-class → binary

            # Quick-specific parameters (filled if mode=='quick')
            'quick_model_name': quick_params['model_name'] if quick_params else None,
            'quick_epochs': quick_params['epochs'] if quick_params else None,

            # Custom-specific parameters (filled if mode=='custom')
            'custom_config': None,  # Will be filled by custom mode

            # Runtime parameters (to be filled during execution)
            'actual_models_trained': [],  # Will be updated post-training
            'training_start_time': None,
            'training_end_time': None
        }

        # Get session ID BEFORE saving metadata
        # Reuse the session ID created at the beginning (self.current_session_id)
        # Add defensive check in case this attribute wasn't initialized
        if not hasattr(self, 'current_session_id') or not self.current_session_id:
            # Fallback: generate a session_id if not set (should not happen in normal flow)
            self.logger.warning("current_session_id not set, generating fallback session_id")
            from datetime import datetime
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            session_id = f"training_session_{timestamp}"
            self.current_session_id = session_id
        else:
            session_id = self.current_session_id

        # Save PRE-TRAINING metadata
        metadata_path = None  # Initialize before conditional block
        if save_metadata:
            try:
                metadata_path = self._save_training_metadata(
                    bundle=bundle,
                    mode=mode,
                    model_config=model_config,
                    quick_params=quick_params,  # Pass quick_params for comprehensive capture
                    execution_status={
                        'status': 'pending',
                        'started_at': datetime.now().isoformat(),
                        'completed_at': None,
                        'models_trained': [],
                        'best_model': None,
                        'best_f1': None
                    },
                    session_id=session_id,
                    training_context={
                        'user_choices': {
                            'save_metadata': save_metadata,
                            'modification_requested': not confirm if mode == "quick" else False
                        }
                    }
                )
                self.console.print(f"\n[green]✅ Metadata saved for reproducibility[/green]")
                self.console.print(f"[cyan]📋 Metadata File:[/cyan]")
                self.console.print(f"   {metadata_path}\n")
            except Exception as e:
                self.logger.error(f"Failed to save metadata: {e}")
                self.console.print(f"[yellow]⚠️  Failed to save metadata: {e}[/yellow]\n")

        # Execute the selected training mode
        self.console.print("\n[green]✓ Starting training...[/green]\n")
        self.console.print(f"[dim]Session ID: {session_id}[/dim]\n")

        training_result = None
        runtime_params = {}  # Will store actual parameters used during training
        try:
            # Only quick mode is supported
            training_result = self._training_studio_run_quick(bundle, model_config, quick_params, session_id)
            runtime_params = training_result.get('runtime_params', {}) if training_result else {}

            # Update POST-TRAINING metadata with COMPLETE information
            if save_metadata and metadata_path:
                try:
                    # Merge runtime params into model_config for complete save
                    final_model_config = {**model_config, **runtime_params}

                    execution_status = {
                        'status': 'completed',
                        'completed_at': datetime.now().isoformat(),
                        'models_trained': training_result.get('models_trained', []) if training_result else [],
                        'best_model': training_result.get('best_model') if training_result else None,
                        'best_f1': training_result.get('best_f1') if training_result else None
                    }

                    # Update both execution_status AND model_config with runtime params
                    self._update_training_metadata(
                        metadata_path,
                        execution_status=execution_status,
                        model_config=final_model_config
                    )
                    self.console.print(f"\n[green]✅ Training metadata updated with complete parameters[/green]\n")
                except Exception as e:
                    self.logger.error(f"Failed to update metadata: {e}")

            # Generate comprehensive training data logs AFTER training completion
            if hasattr(self, 'current_session_manager') and self.current_session_manager:
                try:
                    training_context = {
                        'mode': mode,
                        'training_result': training_result,
                        'runtime_params': runtime_params,
                        'models_trained': training_result.get('models_trained', []) if training_result else [],
                    }
                    self._log_training_data_distributions(bundle, training_context=training_context)
                except Exception as e:
                    self.logger.warning(f"Could not generate comprehensive training logs: {e}")

            # Generate comprehensive summary files (CSV and JSONL) at the end of training
            try:
                from llm_tool.utils.training_summary_generator import generate_training_summaries

                self.console.print("\n[bold cyan]📊 Generating Comprehensive Training Summaries...[/bold cyan]")
                csv_path, jsonl_path = generate_training_summaries(session_id)

                self.console.print("[green]✓ Training summaries generated successfully:[/green]")
                self.console.print(f"  • CSV Summary: [cyan]{csv_path.name}[/cyan]")
                self.console.print(f"  • JSONL Summary: [cyan]{jsonl_path.name}[/cyan]")
                self.console.print(f"\n[dim]Full paths:[/dim]")
                self.console.print(f"  • {csv_path}")
                self.console.print(f"  • {jsonl_path}")

            except Exception as e:
                self.logger.error(f"Failed to generate training summaries: {e}")
                self.console.print(f"[yellow]⚠️  Could not generate comprehensive summaries: {e}[/yellow]")

        except Exception as e:
            # Update metadata with failure status
            if save_metadata and metadata_path:
                try:
                    execution_status = {
                        'status': 'failed',
                        'completed_at': datetime.now().isoformat(),
                        'error_message': str(e)
                    }
                    self._update_training_metadata(metadata_path, execution_status=execution_status)
                except:
                    pass
            raise  # Re-raise the exception

    def _ensure_training_models_loaded(self) -> None:
        if self.available_trainer_models:
            return

        if HAS_RICH and self.console:
            with self.console.status("[cyan]Detecting available training backbones...[/cyan]"):
                self.available_trainer_models = self.trainer_model_detector.get_available_models()
        else:
            self.available_trainer_models = self.trainer_model_detector.get_available_models()

    def _training_studio_show_model_catalog(self) -> None:
        if not self.available_trainer_models:
            return

        table = Table(title="Available Model Categories (70+ models)", border_style="blue")
        table.add_column("Category", style="cyan", width=30)
        table.add_column("Models (sample)", style="white", width=50)

        # Define display order for categories
        category_order = [
            "Multilingual Models",
            "Long Document Models",
            "Long Document Models - French",
            "Long Document Models - Spanish",
            "Long Document Models - German",
            "Long Document Models - Italian",
            "Long Document Models - Portuguese",
            "Long Document Models - Dutch",
            "Long Document Models - Polish",
            "Long Document Models - Chinese",
            "Long Document Models - Japanese",
            "Long Document Models - Arabic",
            "Long Document Models - Russian",
            "Efficient Models",
            "English Models",
            "French Models",
            "Other Language Models"
        ]

        # Display categories in order
        for category in category_order:
            if category in self.available_trainer_models:
                models = self.available_trainer_models[category]
                sample = ", ".join(model["name"] for model in models[:2])
                if len(models) > 2:
                    sample += f" (+{len(models) - 2} more)"
                table.add_row(category, sample)

        # Add any remaining categories not in the order
        for category, models in self.available_trainer_models.items():
            if category not in category_order:
                sample = ", ".join(model["name"] for model in models[:2])
                if len(models) > 2:
                    sample += f" (+{len(models) - 2} more)"
                table.add_row(category, sample)

        self.console.print(table)

    def _training_studio_intelligent_dataset_selector(
        self,
        format_type: str
    ) -> Optional[Dict[str, Any]]:
        """
        Universal sophisticated interface for dataset and column selection.
        Adapted specifically for Training Arena with:
        - Automatic dataset detection
        - Intelligent column analysis with confidence scores
        - Category/label detection and display
        - Sophisticated ID strategy (single/combine/none)
        - Model recommendations based on languages and data

        Args:
            format_type: One of 'llm-json', 'category-csv', 'binary-long', 'jsonl-single', 'jsonl-multi'

        Returns:
            Dictionary with selected dataset path and all column information, or None if cancelled
        """

        # Step 1: Dataset Detection and Selection
        self.console.print("\n[bold cyan]━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[/bold cyan]")
        self.console.print("[bold cyan]  STEP 1:[/bold cyan] [bold white]Dataset Selection[/bold white]")
        self.console.print("[bold cyan]━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[/bold cyan]")
        self.console.print("[dim]Select your annotated dataset file to prepare for training.[/dim]\n")

        # Show detected datasets if available
        if self.detected_datasets:
            datasets_table = Table(title="📊 Detected Datasets", border_style="cyan")
            datasets_table.add_column("#", style="cyan", width=3)
            datasets_table.add_column("Name", style="white", width=40)
            datasets_table.add_column("Format", style="yellow", width=8)
            datasets_table.add_column("Size", style="green", width=10)
            datasets_table.add_column("Folder", style="magenta", width=20)

            for i, ds in enumerate(self.detected_datasets, 1):  # Show ALL datasets
                # Calculate file size
                try:
                    if hasattr(ds, 'path') and ds.path.exists():
                        size_bytes = ds.path.stat().st_size
                        if size_bytes < 1024:
                            size_str = f"{size_bytes} B"
                        elif size_bytes < 1024 * 1024:
                            size_str = f"{size_bytes / 1024:.1f} KB"
                        else:
                            size_str = f"{size_bytes / (1024 * 1024):.1f} MB"
                    else:
                        size_str = "—"
                except Exception as e:
                    self.logger.debug(f"Could not get size for {ds.path}: {e}")
                    size_str = "—"

                # Get folder name (parent directory name)
                folder_name = ds.path.parent.name if hasattr(ds, 'path') and ds.path.parent.name else "data"

                datasets_table.add_row(
                    str(i),
                    ds.path.name if hasattr(ds, 'path') else "—",
                    ds.format if hasattr(ds, 'format') else "—",
                    size_str,
                    folder_name
                )

            self.console.print(datasets_table)
            self.console.print()

            use_detected = Confirm.ask("[bold yellow]Use detected dataset?[/bold yellow]", default=True)
            if use_detected:
                choice = self._int_prompt_with_validation("Select dataset", 1, 1, len(self.detected_datasets))
                data_path = self.detected_datasets[choice - 1].path
            else:
                data_path = Path(self._prompt_file_path("Dataset path"))
        else:
            self.console.print("[dim]No datasets auto-detected in data/ folder[/dim]")
            data_path = Path(self._prompt_file_path("Dataset path"))

        self.console.print(f"[green]✓ Selected: {data_path.name} ({data_path.suffix[1:]})[/green]\n")

        # Step 2: Intelligent File Analysis
        self.console.print("\n[bold cyan]━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[/bold cyan]")
        self.console.print("[bold cyan]  STEP 2:[/bold cyan] [bold white]Analyzing Dataset Structure[/bold white]")
        self.console.print("[bold cyan]━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[/bold cyan]")
        self.console.print("[dim]🔍 Analyzing columns, detecting types, and extracting samples...[/dim]")

        analysis = DataDetector.analyze_file_intelligently(data_path)

        if analysis['issues']:
            self.console.print("\n[yellow]⚠️  Analysis warnings:[/yellow]")
            for issue in analysis['issues']:
                self.console.print(f"  • {issue}")

        # Step 3: Intelligent Language Detection (MOVED HERE - before column selection)
        self.console.print("\n[bold cyan]━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[/bold cyan]")
        self.console.print("[bold cyan]  STEP 3:[/bold cyan] [bold white]Language Detection[/bold white]")
        self.console.print("[bold cyan]━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[/bold cyan]")
        self.console.print("[dim]Detecting languages to recommend the best training model.[/dim]\n")

        languages_found_in_column = set(analysis.get('languages_detected', {}).keys())
        confirmed_languages = set()
        lang_column = None
        text_length_stats = {}  # Initialize - will be populated after text column selection
        languages_from_content = {}

        # Check if we have a language column with detected languages
        has_lang_column = bool(analysis.get('language_column_candidates'))

        if has_lang_column and languages_found_in_column:
            # Option 1: Language column exists - offer to use it or detect automatically
            self.console.print("[bold]🌍 Languages Found in Column:[/bold]")
            for lang, count in analysis['languages_detected'].items():
                self.console.print(f"  • {lang.upper()}: {count:,} rows")

            lang_column_candidate = analysis['language_column_candidates'][0]
            self.console.print(f"\n[green]✓ Language column detected: '{lang_column_candidate}'[/green]")

            use_lang_column = Confirm.ask(
                f"\n[bold]Use language column '{lang_column_candidate}'?[/bold]",
                default=True
            )

            if use_lang_column:
                confirmed_languages = languages_found_in_column
                lang_column = lang_column_candidate
                self.console.print(f"[green]✓ Using language column: {lang_column}[/green]")
            else:
                # User said no to language column - offer automatic detection
                self.console.print("\n[yellow]Language column not used. Applying automatic detection...[/yellow]")
                apply_auto_detection = True
        else:
            # Option 2: No language column - go straight to automatic detection
            self.console.print("[yellow]ℹ️  No language column detected[/yellow]")
            apply_auto_detection = Confirm.ask("Apply automatic language detection on text content?", default=True)

        # We need to detect text column first for content-based language detection
        # Quick text column detection for language analysis
        temp_column_info = self._detect_text_columns(data_path)
        temp_text_column = None
        if temp_column_info.get('text_candidates'):
            temp_text_column = temp_column_info['text_candidates'][0]['name']
        else:
            temp_text_column = "text"  # fallback

        # Automatic language detection from text content
        language_distribution = {}  # Store exact language counts

        if not lang_column and ('apply_auto_detection' not in locals() or apply_auto_detection):
            self.console.print("\n[dim]🔍 Analyzing ALL texts to detect languages (this may take a moment)...[/dim]")

            try:
                import pandas as pd
                from llm_tool.utils.language_detector import LanguageDetector

                df = pd.read_csv(data_path) if data_path.suffix == '.csv' else pd.read_json(data_path, lines=data_path.suffix == '.jsonl')

                if temp_text_column in df.columns:
                    # Analyze ALL texts (not just sample) for precise distribution
                    all_texts = df[temp_text_column].dropna().tolist()

                    if all_texts:
                        detector = LanguageDetector()
                        lang_counts = {}
                        detected_languages_per_text = []  # Store language for each text

                        # Progress indicator
                        from tqdm import tqdm
                        self.console.print(f"[dim]Analyzing {len(all_texts)} texts...[/dim]")

                        for text in tqdm(all_texts, desc="Detecting languages", disable=not HAS_RICH):
                            if text and len(str(text).strip()) > 10:
                                try:
                                    detected = detector.detect(str(text))
                                    if detected and detected.get('language'):
                                        lang = detected['language']
                                        lang_counts[lang] = lang_counts.get(lang, 0) + 1
                                        detected_languages_per_text.append(lang)
                                    else:
                                        detected_languages_per_text.append(None)
                                except Exception as e:
                                    self.logger.debug(f"Language detection failed for text: {e}")
                                    detected_languages_per_text.append(None)
                            else:
                                detected_languages_per_text.append(None)  # Empty or too short text

                        if lang_counts:
                            # Store exact distribution
                            language_distribution = lang_counts
                            total = sum(lang_counts.values())

                            self.console.print(f"\n[bold]🌍 Languages Detected from Content ({total:,} texts analyzed):[/bold]")

                            # Create detailed table
                            lang_table = Table(border_style="cyan", show_header=True, header_style="bold")
                            lang_table.add_column("Language", style="cyan", width=12)
                            lang_table.add_column("Count", style="yellow", justify="right", width=12)
                            lang_table.add_column("Percentage", style="green", justify="right", width=12)

                            for lang, count in sorted(lang_counts.items(), key=lambda x: x[1], reverse=True):
                                percentage = (count / total * 100) if total > 0 else 0
                                lang_table.add_row(
                                    lang.upper(),
                                    f"{count:,}",
                                    f"{percentage:.1f}%"
                                )

                            self.console.print(lang_table)

                            # Detect low-percentage languages (likely detection errors)
                            LOW_PERCENTAGE_THRESHOLD = 1.0  # Languages with < 1% are considered low
                            majority_languages = {}  # Languages above threshold
                            minority_languages = {}  # Languages below threshold (likely errors)

                            for lang, count in lang_counts.items():
                                percentage = (count / total * 100) if total > 0 else 0
                                if percentage >= LOW_PERCENTAGE_THRESHOLD:
                                    majority_languages[lang] = count
                                else:
                                    minority_languages[lang] = count

                            confirmed_languages = set(lang_counts.keys())
                            texts_to_reclassify = []  # Store texts that need manual classification

                            # Handle low-percentage languages if detected
                            if minority_languages:
                                self.console.print(f"\n[yellow]⚠ Warning: {len(minority_languages)} language(s) detected with very low percentage (< {LOW_PERCENTAGE_THRESHOLD}%):[/yellow]")
                                for lang, count in sorted(minority_languages.items(), key=lambda x: x[1], reverse=True):
                                    percentage = (count / total * 100)
                                    self.console.print(f"  • {lang.upper()}: {count} texts ({percentage:.2f}%)")

                                self.console.print("\n[dim]These are likely detection errors. You have options:[/dim]")
                                self.console.print("  [cyan]1. exclude[/cyan] - Exclude ALL low-percentage languages from training")
                                self.console.print("  [cyan]2. keep[/cyan] - Keep ALL detected languages (not recommended)")
                                self.console.print("  [cyan]3. select[/cyan] - Manually select which languages to keep")
                                self.console.print("  [cyan]4. correct[/cyan] - Force ALL minority languages to a single language (quick fix)")
                                self.console.print("  [cyan]5. reclassify[/cyan] - Manually review and reclassify texts phrase-by-phrase")

                                minority_action = Prompt.ask(
                                    "\n[bold yellow]How to handle low-percentage languages?[/bold yellow]",
                                    choices=["exclude", "keep", "select", "correct", "reclassify"],
                                    default="correct"
                                )

                                if minority_action == "correct":
                                    # Quick correction: force all minority languages to one language
                                    self.console.print("\n[bold cyan]🔧 Quick Language Correction[/bold cyan]\n")

                                    # Show available languages (majority + all supported languages)
                                    all_supported_langs = [
                                        'en', 'fr', 'es', 'de', 'it', 'pt', 'nl', 'ru', 'zh', 'ja',
                                        'ar', 'pl', 'tr', 'ko', 'hi', 'sv', 'no', 'da', 'fi', 'cs',
                                        'el', 'he', 'ro', 'uk', 'bg', 'hr', 'vi', 'th', 'id', 'fa'
                                    ]

                                    # Suggest the majority language
                                    majority_lang = max(majority_languages.items(), key=lambda x: x[1])[0] if majority_languages else 'en'

                                    self.console.print(f"[bold]Available languages:[/bold]")
                                    self.console.print(f"  • Majority language detected: [green]{majority_lang.upper()}[/green] ({majority_languages.get(majority_lang, 0)} texts)")
                                    self.console.print(f"  • All supported: {', '.join([l.upper() for l in all_supported_langs])}")

                                    correction_target = Prompt.ask(
                                        f"\n[bold yellow]Force ALL minority languages to which language?[/bold yellow]",
                                        default=majority_lang
                                    ).lower().strip()

                                    if correction_target not in all_supported_langs:
                                        self.console.print(f"[yellow]Warning: '{correction_target}' not in standard list, but will be used anyway[/yellow]")

                                    # CRITICAL FIX: Update detected_languages_per_text with corrections
                                    total_corrected = 0
                                    if 'detected_languages_per_text' in locals() and detected_languages_per_text:
                                        for i in range(len(detected_languages_per_text)):
                                            if detected_languages_per_text[i] in minority_languages:
                                                detected_languages_per_text[i] = correction_target
                                                total_corrected += 1

                                    # Update language_distribution
                                    for minority_lang in minority_languages.keys():
                                        if minority_lang in language_distribution:
                                            del language_distribution[minority_lang]

                                    # Add corrected texts to target language
                                    if correction_target in language_distribution:
                                        language_distribution[correction_target] += total_corrected
                                    else:
                                        language_distribution[correction_target] = total_corrected

                                    # Update confirmed languages
                                    confirmed_languages = set([correction_target] + list(majority_languages.keys()))

                                    self.console.print(f"\n[green]✓ Corrected {total_corrected} texts from {len(minority_languages)} languages to {correction_target.upper()}[/green]")
                                    # Display updated distribution
                                    update_table = Table(title="Updated Language Distribution", border_style="green")
                                    update_table.add_column("Language", style="cyan", justify="center")
                                    update_table.add_column("Count", justify="right")
                                    update_table.add_column("Percentage", justify="right")

                                    new_total = sum(language_distribution.values())
                                    for lang, count in sorted(language_distribution.items(), key=lambda x: x[1], reverse=True):
                                        if count > 0:  # Only show non-zero counts
                                            percentage = (count / new_total) * 100 if new_total > 0 else 0
                                            update_table.add_row(lang.upper(), f"{count:,}", f"{percentage:.1f}%")

                                    self.console.print(update_table)

                                elif minority_action == "reclassify":
                                    # Manual reclassification
                                    self.console.print("\n[bold cyan]Manual Reclassification[/bold cyan]\n")
                                    self.console.print(f"[dim]Available majority languages: {', '.join([l.upper() for l in sorted(majority_languages.keys())])}[/dim]\n")

                                    # Create mapping for reclassification
                                    reclassification_map = {}

                                    # Get the texts for each minority language and show samples
                                    minority_lang_codes = list(minority_languages.keys())

                                    # Load texts with their detected languages
                                    all_texts_with_lang = []
                                    for idx, text in enumerate(all_texts):
                                        if text and len(str(text).strip()) > 10:
                                            try:
                                                detected = detector.detect(str(text))
                                                if detected and detected.get('language'):
                                                    lang = detected['language']
                                                    if lang in minority_languages:
                                                        all_texts_with_lang.append({
                                                            'index': idx,
                                                            'text': str(text),
                                                            'detected_lang': lang
                                                        })
                                            except:
                                                continue

                                    # Show samples for reclassification
                                    if all_texts_with_lang:
                                        self.console.print(f"[bold]Found {len(all_texts_with_lang)} texts to reclassify[/bold]\n")

                                        # Group by detected language
                                        from collections import defaultdict
                                        texts_by_lang = defaultdict(list)
                                        for item in all_texts_with_lang:
                                            texts_by_lang[item['detected_lang']].append(item)

                                        # For each minority language, show samples and ask for reclassification
                                        for minority_lang in sorted(minority_lang_codes):
                                            if minority_lang in texts_by_lang:
                                                lang_texts = texts_by_lang[minority_lang]
                                                self.console.print(f"\n[bold yellow]Reclassifying {minority_lang.upper()} ({len(lang_texts)} texts)[/bold yellow]")

                                                majority_choices = sorted(majority_languages.keys())

                                                # PHRASE-BY-PHRASE RECLASSIFICATION
                                                # Each text can be assigned to a different language
                                                reclassification_choices = {}  # Store per-text choices

                                                for item in lang_texts:
                                                    idx = item['index']
                                                    text = item['text']

                                                    # Show current text
                                                    sample_table = Table(border_style="yellow", show_header=True, header_style="bold", title=f"Text {lang_texts.index(item) + 1}/{len(lang_texts)}")
                                                    sample_table.add_column("Text", width=90)
                                                    display_text = text if len(text) <= 200 else text[:200] + "..."
                                                    sample_table.add_row(display_text)
                                                    self.console.print(sample_table)

                                                    # Ask for this specific text
                                                    majority_choices_str = '/'.join([l.lower() for l in majority_choices])

                                                    reclassify_choice = Prompt.ask(
                                                        f"Classify this text as [{majority_choices_str}/exclude]",
                                                        choices=majority_choices + ["exclude"],
                                                        default=majority_choices[0] if majority_choices else "exclude"
                                                    )

                                                    reclassification_choices[idx] = reclassify_choice

                                                # Count the choices
                                                choice_counts = {}
                                                for choice in reclassification_choices.values():
                                                    choice_counts[choice] = choice_counts.get(choice, 0) + 1

                                                # Update language distribution
                                                for choice, count in choice_counts.items():
                                                    if choice != "exclude":
                                                        language_distribution[choice] = language_distribution.get(choice, 0) + count
                                                        if choice not in reclassification_map:
                                                            reclassification_map[choice] = {}
                                                        # Store the mapping
                                                        if minority_lang not in reclassification_map[choice]:
                                                            reclassification_map[choice][minority_lang] = count
                                                        else:
                                                            reclassification_map[choice][minority_lang] += count

                                                # Remove from original language distribution
                                                language_distribution[minority_lang] = 0

                                                # Display summary
                                                self.console.print(f"\n[bold]Reclassification Summary for {minority_lang.upper()}:[/bold]")
                                                for choice, count in sorted(choice_counts.items(), key=lambda x: x[1], reverse=True):
                                                    if choice == "exclude":
                                                        self.console.print(f"  [yellow]✗ {count} text(s) excluded[/yellow]")
                                                    else:
                                                        self.console.print(f"  [green]✓ {count} text(s) → {choice.upper()}[/green]")

                                    # Update confirmed languages (remove excluded)
                                    # Filter out metadata keys and only keep languages with count > 0
                                    confirmed_languages = set([lang for lang, count in language_distribution.items()
                                                             if not lang.startswith('_') and isinstance(count, (int, float)) and count > 0])

                                    # Store reclassification map for later use
                                    if reclassification_map:
                                        language_distribution['_reclassification_map'] = reclassification_map

                                    self.console.print(f"\n[green]✓ Reclassification complete. Final languages: {', '.join([l.upper() for l in sorted(confirmed_languages)])}[/green]")

                                elif minority_action == "exclude":
                                    # Exclude low-percentage languages
                                    for lang in minority_languages.keys():
                                        language_distribution[lang] = 0  # Mark as excluded

                                    # CRITICAL FIX: Mark excluded language texts as None
                                    if 'detected_languages_per_text' in locals() and detected_languages_per_text:
                                        for i in range(len(detected_languages_per_text)):
                                            if detected_languages_per_text[i] in minority_languages:
                                                detected_languages_per_text[i] = None

                                    confirmed_languages = set(majority_languages.keys())
                                    excluded_count = sum(minority_languages.values())
                                    self.console.print(f"\n[yellow]✗ Excluded {excluded_count} texts from {len(minority_languages)} low-percentage language(s)[/yellow]")
                                    self.console.print(f"[green]✓ Final languages: {', '.join([l.upper() for l in sorted(confirmed_languages)])}[/green]")

                                elif minority_action == "keep":
                                    self.console.print("[yellow]⚠ Keeping all detected languages (including low-percentage ones)[/yellow]")

                                elif minority_action == "select":
                                    # Manual selection of languages to keep
                                    self.console.print("\n[bold cyan]📝 Language Selection:[/bold cyan]")
                                    self.console.print(f"[dim]Select which languages to keep for training (from all {len(lang_counts)} detected)[/dim]\n")

                                    # Show all languages sorted by count
                                    self.console.print("[bold]All Detected Languages:[/bold]")
                                    for i, (lang, count) in enumerate(sorted(lang_counts.items(), key=lambda x: x[1], reverse=True), 1):
                                        percentage = (count / total * 100)
                                        status = "[green]✓ majority[/green]" if lang in majority_languages else "[yellow]⚠ minority[/yellow]"
                                        self.console.print(f"  {i:2d}. {lang.upper():5s} - {count:6,} texts ({percentage:5.2f}%) {status}")

                                    self.console.print("\n[bold yellow]Select languages to KEEP:[/bold yellow]")
                                    self.console.print("[dim]Enter language codes separated by commas (e.g., 'fr,en,de')[/dim]")
                                    self.console.print("[dim]Press Enter without typing to keep ALL languages[/dim]")

                                    selected_langs = Prompt.ask("\n[bold]Languages to keep[/bold]", default="")

                                    if selected_langs.strip():
                                        # User selected specific languages
                                        selected_set = set([l.strip().lower() for l in selected_langs.split(',') if l.strip()])

                                        # Validate that selected languages exist
                                        invalid_langs = selected_set - set(lang_counts.keys())
                                        if invalid_langs:
                                            self.console.print(f"[yellow]⚠ Warning: These languages were not detected: {', '.join(invalid_langs)}[/yellow]")
                                            selected_set = selected_set - invalid_langs

                                        # Exclude non-selected languages
                                        for lang in lang_counts.keys():
                                            if lang not in selected_set:
                                                language_distribution[lang] = 0  # Mark as excluded

                                        # CRITICAL FIX: Mark non-selected language texts as None
                                        if 'detected_languages_per_text' in locals() and detected_languages_per_text:
                                            for i in range(len(detected_languages_per_text)):
                                                if detected_languages_per_text[i] and detected_languages_per_text[i] not in selected_set:
                                                    detected_languages_per_text[i] = None

                                        confirmed_languages = selected_set
                                        kept_count = sum([lang_counts[lang] for lang in selected_set])
                                        excluded_count = total - kept_count

                                        self.console.print(f"\n[green]✓ Kept {len(selected_set)} language(s): {', '.join([l.upper() for l in sorted(selected_set)])}[/green]")
                                        self.console.print(f"[dim]  → {kept_count:,} texts kept, {excluded_count:,} texts excluded[/dim]")
                                    else:
                                        # User pressed Enter - keep all
                                        self.console.print("[green]✓ Keeping all detected languages[/green]")

                            # Final confirmation (allow override even after selection)
                            lang_list = ', '.join([l.upper() for l in sorted(confirmed_languages)])
                            lang_confirmed = Confirm.ask(
                                f"\n[bold]Final languages: {lang_list}. Is this correct?[/bold]",
                                default=True
                            )

                            if not lang_confirmed:
                                self.console.print("\n[yellow]Override with manual selection[/yellow]")
                                manual_langs = Prompt.ask("Enter language codes (comma-separated, e.g., en,fr,de)")
                                confirmed_languages = set([l.strip().lower() for l in manual_langs.split(',') if l.strip()])

                                # Update distribution to exclude non-selected languages
                                for lang in lang_counts.keys():
                                    if lang not in confirmed_languages:
                                        language_distribution[lang] = 0

                                # CRITICAL FIX: Mark non-confirmed language texts as None
                                if 'detected_languages_per_text' in locals() and detected_languages_per_text:
                                    for i in range(len(detected_languages_per_text)):
                                        if detected_languages_per_text[i] and detected_languages_per_text[i] not in confirmed_languages:
                                            detected_languages_per_text[i] = None

                                self.console.print(f"[green]✓ Manual override: {', '.join([l.upper() for l in sorted(confirmed_languages)])}[/green]")
                            else:
                                self.console.print("[green]✓ Languages confirmed from content analysis[/green]")

                            # CRITICAL FIX: Add detected language column to DataFrame and save
                            if 'detected_languages_per_text' in locals() and detected_languages_per_text:
                                # Create a temporary DataFrame for non-null texts
                                temp_df = df[df[temp_text_column].notna()].copy()

                                # Ensure same length
                                if len(detected_languages_per_text) == len(temp_df):
                                    # Map detected languages to the full DataFrame
                                    df['language'] = None
                                    df.loc[df[temp_text_column].notna(), 'language'] = detected_languages_per_text

                                    # Set lang_column to use this new column
                                    lang_column = 'language'

                                    # Save updated DataFrame back to CSV
                                    df.to_csv(data_path, index=False)
                                    self.console.print(f"[dim]✓ Added 'language' column to dataset ({len([l for l in detected_languages_per_text if l])} texts with detected language)[/dim]")
                        else:
                            # Fallback: ask user
                            self.console.print("[yellow]Could not detect languages automatically[/yellow]")
                            manual_langs = Prompt.ask("Expected language codes (e.g., en,fr,de)", default="")
                            if manual_langs.strip():
                                confirmed_languages = set([l.strip().lower() for l in manual_langs.split(',') if l.strip()])
                    else:
                        self.console.print("[yellow]Not enough text samples for language detection[/yellow]")
                        manual_langs = Prompt.ask("Expected language codes (optional, e.g., en,fr,de)", default="")
                        if manual_langs.strip():
                            confirmed_languages = set([l.strip().lower() for l in manual_langs.split(',') if l.strip()])

            except Exception as e:
                self.logger.debug(f"Language detection from content failed: {e}")
                self.console.print("[yellow]Automatic detection failed. Please specify manually[/yellow]")
                manual_langs = Prompt.ask("Expected language codes (optional, e.g., en,fr,de)", default="")
                if manual_langs.strip():
                    confirmed_languages = set([l.strip().lower() for l in manual_langs.split(',') if l.strip()])

                self.console.print("[yellow]Standard models will be used (texts will be truncated to 512 tokens)[/yellow]")
        else:
            text_length_stats['user_prefers_long_models'] = False

        # Step 4: Text Column Selection with Sophisticated Table
        self.console.print("\n[bold cyan]━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[/bold cyan]")
        self.console.print("[bold cyan]  STEP 4:[/bold cyan] [bold white]Text Column Selection[/bold white]")
        self.console.print("[bold cyan]━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[/bold cyan]")
        self.console.print("[bold]💡 What You Need to Select:[/bold]")
        self.console.print("   [cyan]• Text Column[/cyan] - Contains the text data to train on (input for predictions)\n")

        column_info = self._detect_text_columns(data_path)
        all_columns = column_info.get('all_columns', analysis.get('all_columns', []))

        if column_info.get('text_candidates'):
            self.console.print("[dim]Detected text columns (sorted by confidence):[/dim]")

            col_table = Table(border_style="blue")
            col_table.add_column("#", style="cyan", width=5)
            col_table.add_column("Column", style="white", width=15)
            col_table.add_column("Confidence", style="yellow", width=12)
            col_table.add_column("Avg Length", style="green", width=12)
            col_table.add_column("Sample", style="dim", width=55)

            for i, candidate in enumerate(column_info['text_candidates'][:10], 1):
                conf_color = {
                    "high": "[green]High[/green]",
                    "medium": "[yellow]Medium[/yellow]",
                    "low": "[orange1]Low[/orange1]",
                    "very_low": "[red]Very Low[/red]"
                }
                conf_display = conf_color.get(candidate.get('confidence', 'low'), candidate.get('confidence', 'Unknown'))

                sample = candidate.get('sample', '')
                sample_display = (sample[:50] + "...") if len(sample) > 50 else sample

                col_table.add_row(
                    str(i),
                    candidate['name'],
                    conf_display,
                    f"{candidate.get('avg_length', 0):.0f} chars",
                    sample_display
                )

            self.console.print(col_table)
            self.console.print(f"\n[dim]All columns ({len(all_columns)}): {', '.join(all_columns)}[/dim]")

            default_text_col = column_info['text_candidates'][0]['name']
        else:
            self.console.print("[yellow]No text columns auto-detected[/yellow]")
            self.console.print(f"[dim]Available columns: {', '.join(all_columns)}[/dim]")
            default_text_col = "text"

        # Ask for text column with validation
        while True:
            text_column = Prompt.ask("\n[bold yellow]Enter column name[/bold yellow] (or choose from above)", default=default_text_col)
            if text_column in all_columns:
                break
            self.console.print(f"[red]✗ Column '{text_column}' not found in dataset![/red]")
            self.console.print(f"[dim]Available columns: {', '.join(all_columns)}[/dim]")

        # Step 4b: CRITICAL - Text Length Analysis (MUST be done AFTER text column selection)
        self.console.print("\n[bold cyan]━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[/bold cyan]")
        text_length_stats = self.analyze_text_lengths(
            data_path=data_path,
            text_column=text_column,  # Use the ACTUAL selected column
            display_results=True,
            step_label="STEP 4b: Text Length Analysis"
        )
        self.console.print("[bold cyan]━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[/bold cyan]")

        # Store stats for later use in model selection (no user choice yet)
        # User will choose strategy in model selection step

        # Step 5: Label/Category Column Selection with Category Analysis
        self.console.print("\n[bold cyan]━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[/bold cyan]")
        self.console.print("[bold cyan]  STEP 5:[/bold cyan] [bold white]Label/Category Column Selection[/bold white]")
        self.console.print("[bold cyan]━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[/bold cyan]")
        self.console.print("[bold]💡 What You Need to Select:[/bold]")
        self.console.print("   [cyan]• Label Column[/cyan] - Contains the labels/categories (what the model will learn to predict)\n")

        label_column_default = "labels" if "multi" in format_type else "label"

        if analysis.get('annotation_column_candidates'):
            best_label = analysis['annotation_column_candidates'][0]['name']
            label_column_default = best_label

            self.console.print(f"[green]✓ Label column detected: '{best_label}'[/green]")

            stats = analysis.get('annotation_stats', {}).get(best_label, {})
            fill_rate = stats.get('fill_rate', 0)
            if fill_rate > 0:
                self.console.print(f"[dim]  ({fill_rate*100:.1f}% of rows have labels)[/dim]")

            # NOUVEAU: Analyze and display categories/labels
            try:
                import pandas as pd
                df = pd.read_csv(data_path) if data_path.suffix == '.csv' else pd.read_json(data_path, lines=data_path.suffix == '.jsonl')

                if best_label in df.columns:
                    # Get unique categories and their counts
                    if "multi" in format_type:
                        # Multi-label: try to parse lists/JSON
                        all_labels = []
                        for val in df[best_label].dropna():
                            if isinstance(val, list):
                                all_labels.extend(val)
                            elif isinstance(val, str):
                                try:
                                    parsed = json.loads(val)
                                    if isinstance(parsed, list):
                                        all_labels.extend(parsed)
                                except:
                                    pass
                        label_counts = pd.Series(all_labels).value_counts()
                    else:
                        # Single-label: direct value counts
                        label_counts = df[best_label].value_counts()

                    # Display categories table
                    if len(label_counts) > 0:
                        self.console.print(f"\n[bold]📊 Detected {len(label_counts)} Categories:[/bold]")

                        cat_table = Table(border_style="green", show_header=True, header_style="bold cyan")
                        cat_table.add_column("#", style="cyan", width=5)
                        cat_table.add_column("Category", style="white", width=30)
                        cat_table.add_column("Count", style="yellow", width=10, justify="right")
                        cat_table.add_column("Percentage", style="green", width=12, justify="right")

                        total = label_counts.sum()
                        for i, (cat, count) in enumerate(label_counts.head(20).items(), 1):
                            percentage = (count / total * 100) if total > 0 else 0
                            cat_table.add_row(
                                str(i),
                                str(cat)[:28],
                                f"{count:,}",
                                f"{percentage:.1f}%"
                            )

                        if len(label_counts) > 20:
                            cat_table.add_row("...", f"... and {len(label_counts) - 20} more", "...", "...")

                        self.console.print(cat_table)
                        self.console.print(f"[dim]Total samples: {total:,}[/dim]")
            except Exception as e:
                self.logger.debug(f"Could not analyze categories: {e}")

        if all_columns:
            self.console.print(f"\n[dim]Available columns: {', '.join(all_columns)}[/dim]")

        # Ask for label column with validation
        while True:
            label_column = Prompt.ask("\n[bold yellow]Category/label column[/bold yellow]", default=label_column_default)
            if label_column in all_columns:
                break
            self.console.print(f"[red]✗ Column '{label_column}' not found in dataset![/red]")
            self.console.print(f"[dim]Available columns: {', '.join(all_columns)}[/dim]")

        # Step 6: ID Column Selection with Sophisticated Strategy
        self.console.print("\n[bold cyan]━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[/bold cyan]")
        self.console.print("[bold cyan]  STEP 6:[/bold cyan] [bold white]Identifier Column Selection (Optional)[/bold white]")
        self.console.print("[bold cyan]━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[/bold cyan]")
        self.console.print("[dim]Optional: Select an ID column to track samples and link results to your original data.[/dim]\n")

        id_columns = self._detect_id_columns(all_columns)
        id_column = None

        if len(id_columns) > 1:
            self.console.print(f"[bold cyan]📋 Found {len(id_columns)} ID columns:[/bold cyan]")
            for i, col in enumerate(id_columns, 1):
                self.console.print(f"  {i}. [cyan]{col}[/cyan]")

            self.console.print("\n[bold]ID Strategy:[/bold]")
            self.console.print("[dim]IDs are used to track samples and link results to your original data.[/dim]")
            self.console.print("• [cyan]single[/cyan]: Use one column as ID")
            self.console.print("• [cyan]combine[/cyan]: Combine multiple columns (e.g., 'promesse_id+sentence_id')")
            self.console.print("• [cyan]none[/cyan]: Generate automatic IDs")

            id_strategy = Prompt.ask("ID strategy", choices=["single", "combine", "none"], default="single")

            if id_strategy == "none":
                self.console.print("[dim]An automatic ID will be generated[/dim]")
                id_column = None
            elif id_strategy == "combine":
                self.console.print("\n[bold]Select columns to combine:[/bold]")
                self.console.print("[dim]Enter column numbers separated by commas (e.g., '1,2')[/dim]")

                while True:
                    selection = Prompt.ask("Columns to combine")
                    try:
                        indices = [int(x.strip()) - 1 for x in selection.split(',')]
                        if all(0 <= i < len(id_columns) for i in indices):
                            selected_cols = [id_columns[i] for i in indices]
                            id_column = "+".join(selected_cols)
                            self.console.print(f"[green]✓ Will combine: {' + '.join(selected_cols)}[/green]")
                            break
                        else:
                            self.console.print("[red]Invalid column numbers. Try again.[/red]")
                    except (ValueError, IndexError):
                        self.console.print("[red]Invalid format. Use comma-separated numbers (e.g., '1,2')[/red]")
            else:  # single
                id_column = Prompt.ask("Which ID column to use?", choices=id_columns, default=id_columns[0])
        elif len(id_columns) == 1:
            self.console.print(f"[green]✓ ID column detected: '{id_columns[0]}'[/green]")
            if all_columns:
                self.console.print(f"[dim]  Available columns: {', '.join(all_columns)}[/dim]")
            id_column = Prompt.ask("Identifier column (optional)", default=id_columns[0])
        else:
            self.console.print("[dim]No ID columns detected - automatic IDs will be generated[/dim]")
            id_column = None

        if id_column:
            self.console.print(f"[green]✓ Identifier strategy: {id_column}[/green]")

        # Model selection will be done later when training mode is chosen
        # Store languages and text characteristics for later use
        model_to_use = None
        model_strategy = "multilingual"  # default
        language_model_mapping = {}  # For per-language models

        # Skip model selection - will be done in training mode
        if False and confirmed_languages and len(confirmed_languages) > 1:
            # Multiple languages detected - offer strategy choice
            self.console.print(f"[bold]📊 Dataset contains {len(confirmed_languages)} languages:[/bold]")

            if language_distribution:
                # Filter out metadata keys (like _reclassification_map)
                lang_counts = {k: v for k, v in language_distribution.items() if not k.startswith('_') and isinstance(v, (int, float))}

                for lang, count in sorted(lang_counts.items(), key=lambda x: x[1], reverse=True):
                    total = sum(lang_counts.values())
                    pct = (count / total * 100) if total > 0 else 0
                    self.console.print(f"  • {lang.upper()}: {count:,} texts ({pct:.1f}%)")
            else:
                for lang in sorted(confirmed_languages):
                    self.console.print(f"  • {lang.upper()}")

            self.console.print("\n[bold]Model Strategy Options:[/bold]")
            self.console.print("  [cyan]1. multilingual[/cyan] - Train ONE multilingual model for all languages")
            self.console.print("     ✓ Simpler, faster, handles cross-lingual patterns")
            self.console.print("     ✗ May have slightly lower performance per language")
            self.console.print()
            self.console.print("  [cyan]2. specialized[/cyan] - Train SEPARATE specialized models per language")
            self.console.print("     ✓ Best performance for each language")
            self.console.print("     ✗ More training time, requires language column or detection")
            self.console.print()
            self.console.print("  [cyan]3. hybrid[/cyan] - Multilingual model + fine-tuned per-language models")
            self.console.print("     ✓ Best of both worlds")
            self.console.print("     ✗ Most training time and complexity")

            model_strategy = Prompt.ask(
                "\n[bold yellow]Select model strategy[/bold yellow]",
                choices=["multilingual", "specialized", "hybrid"],
                default="multilingual"
            )

            self.console.print(f"\n[green]✓ Selected strategy: {model_strategy}[/green]")

            if model_strategy == "multilingual":
                # Get ONE multilingual model for all languages
                # Consider long-document models if user prefers them
                if text_length_stats.get('user_prefers_long_models', False):
                    model_to_use = self._get_long_document_model_recommendation(confirmed_languages)
                else:
                    model_to_use = self._get_model_recommendation_from_languages(confirmed_languages)

            elif model_strategy == "specialized":
                # Get specialized model for EACH language
                self.console.print("\n[bold]Selecting specialized models for each language:[/bold]")

                for lang in sorted(confirmed_languages):
                    # Consider long-document models if user prefers them
                    if text_length_stats.get('user_prefers_long_models', False):
                        lang_recommendations = self._get_long_document_models_for_language(lang)
                    else:
                        lang_recommendations = LanguageNormalizer.recommend_models({lang}, self.available_trainer_models)

                    if lang_recommendations:
                        self.console.print(f"\n[cyan]For {lang.upper()}:[/cyan]")
                        for i, rec in enumerate(lang_recommendations[:3], 1):
                            self.console.print(f"  {i}. {rec['model']} - {rec['reason']}")

                        choice = Prompt.ask(
                            f"Model for {lang.upper()} (1-{min(3, len(lang_recommendations))}, or enter model name)",
                            default="1"
                        )

                        if choice.isdigit() and 0 < int(choice) <= len(lang_recommendations):
                            language_model_mapping[lang] = lang_recommendations[int(choice) - 1]['model']
                        else:
                            language_model_mapping[lang] = choice

                        self.console.print(f"  [green]✓ {lang.upper()}: {language_model_mapping[lang]}[/green]")
                    else:
                        # Fallback to multilingual
                        self.console.print(f"[yellow]No specific model for {lang.upper()}, using multilingual[/yellow]")
                        if not model_to_use:
                            model_to_use = self._get_model_recommendation_from_languages(confirmed_languages)
                        language_model_mapping[lang] = model_to_use

            elif model_strategy == "hybrid":
                # First get multilingual base model
                self.console.print("\n[bold]1. Select base multilingual model:[/bold]")
                model_to_use = self._get_model_recommendation_from_languages(confirmed_languages)

                # Then get specialized models for fine-tuning
                self.console.print("\n[bold]2. Select specialized models for fine-tuning:[/bold]")
                for lang in sorted(confirmed_languages):
                    lang_recommendations = LanguageNormalizer.recommend_models({lang}, self.available_trainer_models)

                    if lang_recommendations:
                        self.console.print(f"\n[cyan]Fine-tuning model for {lang.upper()}:[/cyan]")
                        for i, rec in enumerate(lang_recommendations[:3], 1):
                            self.console.print(f"  {i}. {rec['model']}")

                        choice = Prompt.ask(
                            f"Model for {lang.upper()} (1-{min(3, len(lang_recommendations))}, or 'skip')",
                            default="1"
                        )

                        if choice.lower() != 'skip':
                            if choice.isdigit() and 0 < int(choice) <= len(lang_recommendations):
                                language_model_mapping[lang] = lang_recommendations[int(choice) - 1]['model']
                            else:
                                language_model_mapping[lang] = choice

                            self.console.print(f"  [green]✓ {lang.upper()}: {language_model_mapping[lang]}[/green]")

        elif confirmed_languages and len(confirmed_languages) == 1:
            # Single language - get specialized model
            lang = list(confirmed_languages)[0]
            self.console.print(f"[bold]Single language detected: {lang.upper()}[/bold]")

            # Consider long-document models if user prefers them
            if text_length_stats.get('user_prefers_long_models', False):
                lang_recommendations = self._get_long_document_models_for_language(lang)
            else:
                lang_recommendations = LanguageNormalizer.recommend_models({lang}, self.available_trainer_models)

            if lang_recommendations:
                self.console.print(f"\n[bold]🤖 Recommended Models for {lang.upper()}:[/bold]")
                for i, rec in enumerate(lang_recommendations[:5], 1):
                    self.console.print(f"  {i}. [cyan]{rec['model']}[/cyan] - {rec['reason']}")

                choice = Prompt.ask("Select model (1-5, or enter model name)", default="1")

                if choice.isdigit() and 0 < int(choice) <= len(lang_recommendations):
                    model_to_use = lang_recommendations[int(choice) - 1]['model']
                else:
                    model_to_use = choice

                self.console.print(f"[green]✓ Selected: {model_to_use}[/green]")
        else:
            # No languages detected - use default
            model_to_use = self._get_model_recommendation_from_languages(set())

        # Return all collected information
        return {
            'data_path': data_path,
            'text_column': text_column,
            'label_column': label_column,
            'id_column': id_column,
            'lang_column': lang_column,
            'confirmed_languages': confirmed_languages,
            'language_distribution': language_distribution,  # Exact counts per language
            'text_length_stats': text_length_stats,  # Text length statistics and long-document preference
            'model_strategy': model_strategy,  # multilingual, specialized, or hybrid
            'recommended_model': model_to_use,  # Main/base model
            'language_model_mapping': language_model_mapping,  # Per-language models (if specialized)
            'analysis': analysis
        }

    def _training_studio_dataset_wizard(self, builder: TrainingDatasetBuilder) -> Optional[TrainingDataBundle]:
        """
        Intelligent dataset wizard with comprehensive file analysis and guided setup.
        Now supports all formats with smart detection and recommendations.
        """

        # Step 1: Explain format options with Rich table
        self.console.print("\n[bold cyan]━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[/bold cyan]")
        self.console.print("[bold cyan]  STEP 1:[/bold cyan] [bold white]Dataset Format Selection[/bold white]")
        self.console.print("[bold cyan]━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[/bold cyan]")
        self.console.print("[dim]Choose the format that matches your annotated data structure.[/dim]\n")

        formats_table = Table(show_header=True, header_style="bold magenta", border_style="green", box=box.ROUNDED)
        formats_table.add_column("Format", style="cyan bold", width=18)
        formats_table.add_column("Description", style="white", width=50)
        formats_table.add_column("Example", style="dim", width=35)

        formats_table.add_row(
            "llm-json",
            "CSV/JSON with LLM annotations in a column\n✓ JSON objects containing labels/categories\n✓ Output from LLM annotation tools",
            "{'category': 'Tech', 'sentiment': 'pos'}"
        )
        formats_table.add_row(
            "[dim]category-csv[/dim]",
            "[dim]Simple CSV with text and label columns\n✓ Most common format\n✓ One row = one sample with its label[/dim]",
            "[dim]text,label\n'Hello',positive[/dim]"
        )
        formats_table.add_row(
            "[dim]binary-long[/dim]",
            "[dim]Long-format CSV with binary labels\n✓ Multiple rows per sample\n✓ Each row = one category with 0/1 value[/dim]",
            "[dim]id,text,category,value\n1,'Hi',pos,1[/dim]"
        )
        formats_table.add_row(
            "[dim]jsonl-single[/dim]",
            "[dim]JSONL file for single-label tasks\n✓ One JSON object per line\n✓ Each sample has one label only[/dim]",
            "[dim]{'text':'Hi','label':'positive'}[/dim]"
        )
        formats_table.add_row(
            "[dim]jsonl-multi[/dim]",
            "[dim]JSONL file for multi-label tasks\n✓ One JSON object per line\n✓ Each sample can have multiple labels[/dim]",
            "[dim]{'text':'Hi','labels':['pos','friendly']}[/dim]"
        )

        self.console.print(formats_table)
        self.console.print()

        # Add development notice for experimental formats
        self.console.print("[yellow]⚠️  Note:[/yellow] [bold red]category-csv, binary-long, jsonl-single, and jsonl-multi are currently under development and NOT accessible.[/bold red]")
        self.console.print("[dim]      These formats will be enabled in a future release after thorough testing.[/dim]")
        self.console.print()

        format_choice = Prompt.ask(
            "[bold yellow]Select dataset format[/bold yellow]",
            choices=["llm-json", "cancel", "back"],
            default="llm-json",
        )

        if format_choice == "cancel" or format_choice == "back":
            return None

        if format_choice == "llm-json":
            # Step 2: Dataset Selection
            self.console.print("\n[bold cyan]━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[/bold cyan]")
            self.console.print("[bold cyan]  STEP 2:[/bold cyan] [bold white]Dataset Selection[/bold white]")
            self.console.print("[bold cyan]━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[/bold cyan]")
            self.console.print("[dim]Select your annotated dataset file to prepare for training.[/dim]\n")

            # Show detected datasets if available
            if self.detected_datasets:
                datasets_table = Table(title="📊 Detected Datasets", border_style="cyan")
                datasets_table.add_column("#", style="cyan", width=3)
                datasets_table.add_column("Name", style="white", width=40)
                datasets_table.add_column("Format", style="yellow", width=8)
                datasets_table.add_column("Size", style="green", width=10)
                datasets_table.add_column("Folder", style="magenta", width=20)

                for i, ds in enumerate(self.detected_datasets, 1):  # Show ALL datasets, not just [:10]
                    # Calculate file size
                    try:
                        if hasattr(ds, 'path') and ds.path.exists():
                            size_bytes = ds.path.stat().st_size
                            if size_bytes < 1024:
                                size_str = f"{size_bytes} B"
                            elif size_bytes < 1024 * 1024:
                                size_str = f"{size_bytes / 1024:.1f} KB"
                            else:
                                size_str = f"{size_bytes / (1024 * 1024):.1f} MB"
                        else:
                            size_str = "—"
                    except Exception as e:
                        self.logger.debug(f"Could not get size for {ds.path}: {e}")
                        size_str = "—"

                    # Get folder name (parent directory name)
                    folder_name = ds.path.parent.name if hasattr(ds, 'path') and ds.path.parent.name else "data"

                    datasets_table.add_row(
                        str(i),
                        ds.path.name if hasattr(ds, 'path') else "—",
                        ds.format if hasattr(ds, 'format') else "—",
                        size_str,
                        folder_name
                    )

                self.console.print(datasets_table)
                self.console.print()
                self.console.print("[dim]💡 You can either:[/dim]")
                self.console.print("[dim]   • Enter the [cyan]#[/cyan] number from the table above (e.g., '1', '13')[/dim]")
                self.console.print("[dim]   • Enter an [cyan]absolute path[/cyan] to any file (e.g., '/Users/name/data/file.csv')[/dim]\n")

                dataset_choice = Prompt.ask("Dataset selection", default="1")

                # Parse choice
                if not dataset_choice or dataset_choice.strip() == "":
                    # Empty input - default to first dataset
                    self.console.print("[yellow]⚠️  No selection made, defaulting to first dataset[/yellow]")
                    csv_path = self.detected_datasets[0].path
                elif dataset_choice.isdigit():
                    idx = int(dataset_choice) - 1
                    if 0 <= idx < len(self.detected_datasets):
                        csv_path = self.detected_datasets[idx].path
                    else:
                        self.console.print("[red]Invalid dataset number[/red]")
                        return None
                else:
                    csv_path = Path(dataset_choice)
                    # Validate that it's a file, not a directory
                    if csv_path.is_dir():
                        self.console.print(f"[red]Error: '{csv_path}' is a directory, not a file[/red]")
                        return None
                    if not csv_path.exists():
                        self.console.print(f"[red]Error: File '{csv_path}' does not exist[/red]")
                        return None
            else:
                file_path_str = self._prompt_file_path("Annotated file path (CSV/JSON/Excel/Parquet)")
                csv_path = Path(file_path_str)

            self.console.print(f"[green]✓ Selected: {csv_path.name} ({csv_path.suffix[1:]})[/green]\n")

            # Step 3: File Structure Analysis
            self.console.print("\n[bold cyan]━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[/bold cyan]")
            self.console.print("[bold cyan]  STEP 3:[/bold cyan] [bold white]Analyzing Dataset Structure[/bold white]")
            self.console.print("[bold cyan]━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[/bold cyan]")
            self.console.print("[dim]🔍 Analyzing columns, detecting types, and extracting samples...[/dim]")
            analysis = DataDetector.analyze_file_intelligently(csv_path)

            # Show analysis results
            if analysis['issues']:
                self.console.print("\n[yellow]⚠️  Analysis warnings:[/yellow]")
                for issue in analysis['issues']:
                    self.console.print(f"  • {issue}")

            # Step 4: Column Selection
            self.console.print("\n[bold cyan]━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[/bold cyan]")
            self.console.print("[bold cyan]  STEP 4:[/bold cyan] [bold white]Column Selection[/bold white]")
            self.console.print("[bold cyan]━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[/bold cyan]")
            self.console.print("[bold]💡 What You Need to Select:[/bold]")
            self.console.print("   [cyan]• Text Column[/cyan]     - Contains the text data to train on (input for predictions)")
            self.console.print("   [cyan]• Annotation Column[/cyan] - Contains the JSON annotations (labels/categories for training)\n")

            # Auto-suggest text column with all available columns
            text_column_default = "sentence"
            all_columns = analysis.get('all_columns', [])

            # Read CSV to analyze ALL columns
            import pandas as pd

            # Final validation before reading
            if not csv_path or csv_path.is_dir():
                self.console.print(f"[red]Error: Invalid file path '{csv_path}'[/red]")
                return None
            if not csv_path.exists():
                self.console.print(f"[red]Error: File '{csv_path}' does not exist[/red]")
                return None

            df = pd.read_csv(csv_path)

            # Create comprehensive column overview table
            if all_columns:
                self.console.print(f"[bold]📊 Dataset Overview ({len(all_columns)} columns, {len(df):,} rows):[/bold]\n")

                # Create detailed columns table
                all_columns_table = Table(show_header=True, header_style="bold magenta", border_style="cyan", box=box.ROUNDED)
                all_columns_table.add_column("#", style="dim", width=3)
                all_columns_table.add_column("Column Name", style="cyan bold", width=30)
                all_columns_table.add_column("Type", style="yellow", width=12)
                all_columns_table.add_column("Sample Values", style="white", width=50)

                for idx, col in enumerate(all_columns, 1):
                    # Detect column type
                    col_type = "text"
                    if col in df.columns:
                        if df[col].dtype in ['int64', 'float64']:
                            col_type = "numeric"
                        elif pd.api.types.is_datetime64_any_dtype(df[col]):
                            col_type = "datetime"
                        else:
                            # Check if it's likely JSON
                            sample_val = df[col].dropna().iloc[0] if len(df[col].dropna()) > 0 else ""
                            if isinstance(sample_val, str) and (sample_val.startswith('{') or sample_val.startswith('[')):
                                col_type = "json/annotation"
                            else:
                                col_type = "text"

                        # Get sample values
                        samples = df[col].dropna().head(3).tolist()
                        if samples:
                            sample_str = ", ".join([str(s)[:30] + "..." if len(str(s)) > 30 else str(s) for s in samples])
                        else:
                            sample_str = "[empty]"
                    else:
                        sample_str = "—"

                    all_columns_table.add_row(
                        str(idx),
                        col,
                        col_type,
                        sample_str
                    )

                self.console.print(all_columns_table)

                # Now show AI suggestions
                self.console.print("\n[bold]💡 Helpful Suggestions[/bold] [dim](not required - you choose)[/dim]")
                self.console.print("[dim]These are suggestions based on column names and content analysis.[/dim]")
                self.console.print("[dim]You are free to select ANY column from the table above.[/dim]\n")

                suggestions_table = Table(show_header=True, header_style="bold magenta", border_style="green", box=box.SIMPLE)
                suggestions_table.add_column("Purpose", style="yellow bold", width=20)
                suggestions_table.add_column("Top Suggestion", style="green bold", width=25)
                suggestions_table.add_column("Why This Column?", style="white", width=45)

                # Text column row
                if analysis['text_column_candidates']:
                    best_text = analysis['text_column_candidates'][0]['name']
                    text_column_default = best_text
                    text_stats = analysis['text_column_candidates'][0]
                    avg_len = text_stats.get('avg_length', 0)
                    suggestions_table.add_row(
                        "📝 Text Data",
                        best_text,
                        f"Contains text (avg {avg_len:.0f} chars)"
                    )
                else:
                    suggestions_table.add_row("📝 Text Data", "—", "⚠️  No automatic suggestion")

                # Annotation column row
                annotation_column_default = "annotation"
                has_annotation_alternatives = False
                if analysis['annotation_column_candidates']:
                    best_annotation_info = analysis['annotation_column_candidates'][0]
                    best_annotation = best_annotation_info['name']
                    annotation_column_default = best_annotation
                    stats = analysis['annotation_stats'].get(best_annotation, {})
                    fill_rate = stats.get('fill_rate', 0)
                    is_json = stats.get('is_json', False)
                    match_type = best_annotation_info.get('match_type', 'name_pattern')

                    if fill_rate > 0:
                        # Build reason text
                        reason_parts = []
                        if is_json:
                            if match_type == 'json_content':
                                reason_parts.append("Auto-detected JSON annotations")
                            else:
                                reason_parts.append("Contains JSON annotations")
                        else:
                            reason_parts.append("Contains labels/categories")
                        reason_parts.append(f"{fill_rate*100:.1f}% filled")

                        suggestions_table.add_row(
                            "🏷️  Annotations",
                            best_annotation,
                            ", ".join(reason_parts)
                        )

                        # Mark if there are alternatives
                        if len(analysis['annotation_column_candidates']) > 1:
                            has_annotation_alternatives = True
                    else:
                        suggestions_table.add_row(
                            "🏷️  Annotations",
                            best_annotation,
                            "[red]⚠️  Column is EMPTY - cannot use[/red]"
                        )
                else:
                    suggestions_table.add_row("🏷️  Annotations", "—", "⚠️  No automatic suggestion")

                self.console.print(suggestions_table)

                # Show alternatives AFTER the table
                if has_annotation_alternatives:
                    alternatives = [c['name'] for c in analysis['annotation_column_candidates'][1:3]]
                    self.console.print(f"[dim]   Other annotation options: {', '.join(alternatives)}[/dim]")

                self.console.print()
            else:
                # Fallback if no columns detected
                if analysis['text_column_candidates']:
                    best_text = analysis['text_column_candidates'][0]['name']
                    text_column_default = best_text
                    self.console.print(f"\n[green]✓ Suggested text column: '{best_text}'[/green]")

                annotation_column_default = "annotation"
                if analysis['annotation_column_candidates']:
                    best_annotation = analysis['annotation_column_candidates'][0]['name']
                    annotation_column_default = best_annotation
                    stats = analysis['annotation_stats'].get(best_annotation, {})
                    fill_rate = stats.get('fill_rate', 0)
                    if fill_rate > 0:
                        self.console.print(f"[green]✓ Suggested annotation column: '{best_annotation}' ({fill_rate*100:.1f}% filled)[/green]")
                    else:
                        self.console.print(f"[red]⚠️  Suggested annotation column '{best_annotation}' is EMPTY - cannot be used for training![/red]")

            self.console.print("[bold yellow]📝 Your Turn - Select Columns:[/bold yellow]")
            self.console.print("[dim]   → Press [bold]Enter[/bold] to use the suggested column[/dim]")
            self.console.print("[dim]   → Or type ANY column name from the table above[/dim]")
            self.console.print("[dim]   → The suggestions are helpful, but not mandatory![/dim]\n")

            # Ask for text column with validation
            while True:
                text_column = Prompt.ask("[bold cyan]Text column[/bold cyan] (training input)", default=text_column_default)
                if text_column in all_columns:
                    break
                self.console.print(f"[red]✗ Column '{text_column}' not found in dataset![/red]")
                self.console.print(f"[dim]Available columns: {', '.join(all_columns)}[/dim]")

            # Ask for annotation column with validation
            while True:
                annotation_column = Prompt.ask("[bold cyan]Annotation column[/bold cyan] (training labels)", default=annotation_column_default)
                if annotation_column in all_columns:
                    break
                self.console.print(f"[red]✗ Column '{annotation_column}' not found in dataset![/red]")
                self.console.print(f"[dim]Available columns: {', '.join(all_columns)}[/dim]")

            # Show confirmation of selection
            self.console.print(f"\n[green]✓ Selected columns:[/green]")
            self.console.print(f"  [cyan]Text:[/cyan] '{text_column}' → Model will learn from this text")
            self.console.print(f"  [cyan]Annotations:[/cyan] '{annotation_column}' → Model will learn these labels")

            # Step 3b: CRITICAL - Text Length Analysis (MUST be done AFTER text column selection)
            self.console.print("\n[bold cyan]━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[/bold cyan]")
            text_length_stats = self.analyze_text_lengths(
                data_path=csv_path,
                text_column=text_column,  # Use the ACTUAL selected column, not temp
                display_results=True,
                step_label="STEP 3b: Text Length Analysis"
            )
            self.console.print("[bold cyan]━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[/bold cyan]")

            # Store stats for later use in model selection (no user choice yet)
            # User will choose strategy in model selection step

            # Step 5: Language Detection and Text Analysis (using sophisticated universal system)
            self.console.print("\n[bold cyan]━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[/bold cyan]")
            self.console.print("[bold cyan]  STEP 5:[/bold cyan] [bold white]Language Detection[/bold white]")
            self.console.print("[bold cyan]━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[/bold cyan]")
            self.console.print("[dim]Analyzing languages to recommend the best model.[/dim]\n")

            # Read CSV for analysis
            import pandas as pd
            df = pd.read_csv(csv_path)

            # Use the SAME sophisticated language detection as category-csv
            languages_found_in_column = set(analysis.get('languages_detected', {}).keys())
            confirmed_languages = set()
            lang_column = None
            language_distribution = {}  # Store exact language counts

            # Check if we have a language column with detected languages
            has_lang_column = bool(analysis.get('language_column_candidates'))

            if has_lang_column and languages_found_in_column:
                # Option 1: Language column exists - offer to use it or detect automatically
                self.console.print("[bold]🌍 Languages Found in Column:[/bold]")
                for lang, count in analysis['languages_detected'].items():
                    self.console.print(f"  • {lang.upper()}: {count:,} rows")

                lang_column_candidate = analysis['language_column_candidates'][0]
                self.console.print(f"\n[green]✓ Language column detected: '{lang_column_candidate}'[/green]")

                use_lang_column = Confirm.ask(
                    f"\n[bold]Use language column '{lang_column_candidate}'?[/bold]",
                    default=True
                )

                if use_lang_column:
                    confirmed_languages = languages_found_in_column
                    lang_column = lang_column_candidate
                    self.console.print(f"[green]✓ Using language column: {lang_column}[/green]")
                else:
                    # User said no to language column - apply automatic detection
                    self.console.print("\n[yellow]Language column not used. Applying automatic detection...[/yellow]")
                    has_lang_column = False  # Trigger auto-detection below
            else:
                # Option 2: No language column
                if not has_lang_column:
                    self.console.print("[yellow]ℹ️  No language column detected[/yellow]")
                apply_auto_detection = Confirm.ask("Apply automatic language detection on text content?", default=True)
                if not apply_auto_detection:
                    has_lang_column = True  # Skip auto-detection

            # Automatic language detection from text content (if no lang column used)
            if not lang_column and (not has_lang_column or 'apply_auto_detection' in locals()):
                self.console.print("\n[dim]🔍 Analyzing ALL texts to detect languages (this may take a moment)...[/dim]")

                try:
                    from llm_tool.utils.language_detector import LanguageDetector

                    if text_column in df.columns:
                        # Analyze ALL texts (not just sample) for precise distribution
                        all_texts = df[text_column].dropna().tolist()

                        if all_texts:
                            detector = LanguageDetector()
                            lang_counts = {}
                            detected_languages_per_text = []  # Store language for each text

                            # Progress indicator
                            from tqdm import tqdm
                            self.console.print(f"[dim]Analyzing {len(all_texts)} texts...[/dim]")

                            for text in tqdm(all_texts, desc="Detecting languages", disable=not HAS_RICH):
                                if text and len(str(text).strip()) > 10:
                                    try:
                                        detected = detector.detect(str(text))
                                        if detected:
                                            # Handle both dict and string returns
                                            if isinstance(detected, dict):
                                                lang = detected.get('language')
                                                confidence = detected.get('confidence', 0)
                                                # Use confidence threshold (optional)
                                                if lang and confidence >= 0.7:  # 70% confidence threshold
                                                    lang_counts[lang] = lang_counts.get(lang, 0) + 1
                                                    detected_languages_per_text.append(lang)
                                                else:
                                                    detected_languages_per_text.append(None)  # Low confidence
                                            elif isinstance(detected, str):
                                                lang_counts[detected] = lang_counts.get(detected, 0) + 1
                                                detected_languages_per_text.append(detected)
                                        else:
                                            detected_languages_per_text.append(None)
                                    except Exception as e:
                                        self.logger.debug(f"Language detection failed for text: {e}")
                                        detected_languages_per_text.append(None)
                                else:
                                    detected_languages_per_text.append(None)  # Empty or too short text

                            if lang_counts:
                                # Store exact distribution
                                language_distribution = lang_counts
                                total = sum(lang_counts.values())

                                self.console.print(f"\n[bold]🌍 Languages Detected from Content ({total:,} texts analyzed):[/bold]")

                                # Create detailed table
                                lang_table = Table(border_style="cyan", show_header=True, header_style="bold")
                                lang_table.add_column("Language", style="cyan", width=12)
                                lang_table.add_column("Count", style="yellow", justify="right", width=12)
                                lang_table.add_column("Percentage", style="green", justify="right", width=12)

                                for lang, count in sorted(lang_counts.items(), key=lambda x: x[1], reverse=True):
                                    percentage = (count / total * 100) if total > 0 else 0
                                    lang_table.add_row(
                                        lang.upper(),
                                        f"{count:,}",
                                        f"{percentage:.1f}%"
                                    )

                                self.console.print(lang_table)

                                # Detect low-percentage languages (likely detection errors)
                                LOW_PERCENTAGE_THRESHOLD = 1.0  # Languages with < 1% are considered low
                                majority_languages = {}  # Languages above threshold
                                minority_languages = {}  # Languages below threshold (likely errors)

                                for lang, count in lang_counts.items():
                                    percentage = (count / total * 100) if total > 0 else 0
                                    if percentage >= LOW_PERCENTAGE_THRESHOLD:
                                        majority_languages[lang] = count
                                    else:
                                        minority_languages[lang] = count

                                confirmed_languages = set(lang_counts.keys())

                                # Handle low-percentage languages if detected
                                if minority_languages:
                                    self.console.print(f"\n[yellow]⚠ Warning: {len(minority_languages)} language(s) detected with very low percentage (< {LOW_PERCENTAGE_THRESHOLD}%):[/yellow]")
                                    for lang, count in sorted(minority_languages.items(), key=lambda x: x[1], reverse=True):
                                        percentage = (count / total * 100)
                                        self.console.print(f"  • {lang.upper()}: {count} texts ({percentage:.2f}%)")

                                    self.console.print("\n[dim]These are likely detection errors. You have options:[/dim]")
                                    self.console.print("  [cyan]1. exclude[/cyan] - Exclude ALL low-percentage languages from training")
                                    self.console.print("  [cyan]2. keep[/cyan] - Keep ALL detected languages (not recommended)")
                                    self.console.print("  [cyan]3. select[/cyan] - Manually select which languages to keep")
                                    self.console.print("  [cyan]4. correct[/cyan] - Force ALL minority languages to a single language (quick fix)")

                                    minority_action = Prompt.ask(
                                        "\n[bold yellow]How to handle low-percentage languages?[/bold yellow]",
                                        choices=["exclude", "keep", "select", "correct"],
                                        default="correct"
                                    )

                                    if minority_action == "correct":
                                        # Quick correction: force all minority languages to one language
                                        self.console.print("\n[bold cyan]🔧 Quick Language Correction[/bold cyan]\n")

                                        # Show available languages
                                        all_supported_langs = [
                                            'en', 'fr', 'es', 'de', 'it', 'pt', 'nl', 'ru', 'zh', 'ja',
                                            'ar', 'pl', 'tr', 'ko', 'hi', 'sv', 'no', 'da', 'fi', 'cs',
                                            'el', 'he', 'ro', 'uk', 'bg', 'hr', 'vi', 'th', 'id', 'fa'
                                        ]

                                        # Suggest the majority language
                                        majority_lang = max(majority_languages.items(), key=lambda x: x[1])[0] if majority_languages else 'en'

                                        self.console.print(f"[bold]Available languages:[/bold]")
                                        self.console.print(f"  • Majority language detected: [green]{majority_lang.upper()}[/green] ({majority_languages.get(majority_lang, 0)} texts)")
                                        self.console.print(f"  • All supported: {', '.join([l.upper() for l in all_supported_langs])}")

                                        correction_target = Prompt.ask(
                                            f"\n[bold yellow]Force ALL minority languages to which language?[/bold yellow]",
                                            default=majority_lang
                                        ).lower().strip()

                                        if correction_target not in all_supported_langs:
                                            self.console.print(f"[yellow]Warning: '{correction_target}' not in standard list, but will be used anyway[/yellow]")

                                        # Update language_distribution and confirmed_languages
                                        total_corrected = sum(minority_languages.values())

                                        # Move all minority counts to the target language
                                        for minority_lang in minority_languages.keys():
                                            if minority_lang in language_distribution:
                                                del language_distribution[minority_lang]

                                        # Add corrected texts to target language
                                        if correction_target in language_distribution:
                                            language_distribution[correction_target] += total_corrected
                                        else:
                                            language_distribution[correction_target] = total_corrected

                                        # Update confirmed languages
                                        confirmed_languages = set([correction_target] + list(majority_languages.keys()))

                                        # CRITICAL FIX: Update detected_languages_per_text with corrections
                                        if 'detected_languages_per_text' in locals() and detected_languages_per_text:
                                            for i in range(len(detected_languages_per_text)):
                                                if detected_languages_per_text[i] in minority_languages:
                                                    detected_languages_per_text[i] = correction_target

                                        self.console.print(f"\n[green]✓ Corrected {total_corrected} texts from {len(minority_languages)} languages to {correction_target.upper()}[/green]")

                                        # Display updated distribution
                                        update_table = Table(title="Updated Language Distribution", border_style="green")
                                        update_table.add_column("Language", style="cyan", justify="center")
                                        update_table.add_column("Count", justify="right")
                                        update_table.add_column("Percentage", justify="right")

                                        new_total = sum(language_distribution.values())
                                        for lang, count in sorted(language_distribution.items(), key=lambda x: x[1], reverse=True):
                                            if count > 0:  # Only show non-zero counts
                                                percentage = (count / new_total) * 100 if new_total > 0 else 0
                                                update_table.add_row(lang.upper(), f"{count:,}", f"{percentage:.1f}%")

                                        self.console.print(update_table)

                                    elif minority_action == "exclude":
                                        # Exclude low-percentage languages
                                        for lang in minority_languages.keys():
                                            language_distribution[lang] = 0  # Mark as excluded

                                        # CRITICAL FIX: Mark excluded language texts as None
                                        if 'detected_languages_per_text' in locals() and detected_languages_per_text:
                                            for i in range(len(detected_languages_per_text)):
                                                if detected_languages_per_text[i] in minority_languages:
                                                    detected_languages_per_text[i] = None

                                        confirmed_languages = set(majority_languages.keys())
                                        excluded_count = sum(minority_languages.values())
                                        self.console.print(f"\n[yellow]✗ Excluded {excluded_count} texts from {len(minority_languages)} low-percentage language(s)[/yellow]")
                                        self.console.print(f"[green]✓ Final languages: {', '.join([l.upper() for l in sorted(confirmed_languages)])}[/green]")

                                    elif minority_action == "keep":
                                        self.console.print("[yellow]⚠ Keeping all detected languages (including low-percentage ones)[/yellow]")

                                    elif minority_action == "select":
                                        # Manual selection of languages to keep
                                        self.console.print("\n[bold cyan]📝 Language Selection:[/bold cyan]")
                                        self.console.print(f"[dim]Select which languages to keep for training (from all {len(lang_counts)} detected)[/dim]\n")

                                        # Show all languages sorted by count
                                        self.console.print("[bold]All Detected Languages:[/bold]")
                                        for i, (lang, count) in enumerate(sorted(lang_counts.items(), key=lambda x: x[1], reverse=True), 1):
                                            percentage = (count / total * 100)
                                            status = "[green]✓ majority[/green]" if lang in majority_languages else "[yellow]⚠ minority[/yellow]"
                                            self.console.print(f"  {i:2d}. {lang.upper():5s} - {count:6,} texts ({percentage:5.2f}%) {status}")

                                        self.console.print("\n[bold yellow]Select languages to KEEP:[/bold yellow]")
                                        self.console.print("[dim]Enter language codes separated by commas (e.g., 'fr,en,de')[/dim]")
                                        self.console.print("[dim]Press Enter without typing to keep ALL languages[/dim]")

                                        selected_langs = Prompt.ask("\n[bold]Languages to keep[/bold]", default="")

                                        if selected_langs.strip():
                                            # User selected specific languages
                                            selected_set = set([l.strip().lower() for l in selected_langs.split(',') if l.strip()])

                                            # Validate that selected languages exist
                                            invalid_langs = selected_set - set(lang_counts.keys())
                                            if invalid_langs:
                                                self.console.print(f"[yellow]⚠ Warning: These languages were not detected: {', '.join(invalid_langs)}[/yellow]")
                                                selected_set = selected_set - invalid_langs

                                            # Exclude non-selected languages
                                            for lang in lang_counts.keys():
                                                if lang not in selected_set:
                                                    language_distribution[lang] = 0  # Mark as excluded

                                            # CRITICAL FIX: Mark non-selected language texts as None
                                            if 'detected_languages_per_text' in locals() and detected_languages_per_text:
                                                for i in range(len(detected_languages_per_text)):
                                                    if detected_languages_per_text[i] and detected_languages_per_text[i] not in selected_set:
                                                        detected_languages_per_text[i] = None

                                            confirmed_languages = selected_set
                                            kept_count = sum([lang_counts[lang] for lang in selected_set])
                                            excluded_count = total - kept_count

                                            self.console.print(f"\n[green]✓ Kept {len(selected_set)} language(s): {', '.join([l.upper() for l in sorted(selected_set)])}[/green]")
                                            self.console.print(f"[dim]  → {kept_count:,} texts kept, {excluded_count:,} texts excluded[/dim]")
                                        else:
                                            # User pressed Enter - keep all
                                            self.console.print("[green]✓ Keeping all detected languages[/green]")

                                # Final confirmation (allow override even after selection)
                                lang_list = ', '.join([l.upper() for l in sorted(confirmed_languages)])
                                lang_confirmed = Confirm.ask(
                                    f"\n[bold]Final languages: {lang_list}. Is this correct?[/bold]",
                                    default=True
                                )

                                if not lang_confirmed:
                                    self.console.print("\n[yellow]Override with manual selection[/yellow]")
                                    manual_langs = Prompt.ask("Enter language codes (comma-separated, e.g., en,fr,de)")
                                    confirmed_languages = set([l.strip().lower() for l in manual_langs.split(',') if l.strip()])

                                    # Update distribution to exclude non-selected languages
                                    for lang in lang_counts.keys():
                                        if lang not in confirmed_languages:
                                            language_distribution[lang] = 0

                                    # CRITICAL FIX: Mark non-confirmed language texts as None
                                    if 'detected_languages_per_text' in locals() and detected_languages_per_text:
                                        for i in range(len(detected_languages_per_text)):
                                            if detected_languages_per_text[i] and detected_languages_per_text[i] not in confirmed_languages:
                                                detected_languages_per_text[i] = None

                                    self.console.print(f"[green]✓ Manual override: {', '.join([l.upper() for l in sorted(confirmed_languages)])}[/green]")
                                else:
                                    self.console.print("[green]✓ Languages confirmed from content analysis[/green]")

                                # CRITICAL FIX: Add detected language column to DataFrame and save
                                if 'detected_languages_per_text' in locals() and detected_languages_per_text:
                                    # Create a temporary DataFrame for non-null texts
                                    temp_df = df[df[text_column].notna()].copy()

                                    # Ensure same length
                                    if len(detected_languages_per_text) == len(temp_df):
                                        temp_df['language'] = detected_languages_per_text

                                        # Map detected languages to the full DataFrame
                                        df['language'] = None
                                        df.loc[df[text_column].notna(), 'language'] = detected_languages_per_text

                                        # Set lang_column to use this new column
                                        lang_column = 'language'

                                        # Save updated DataFrame back to CSV
                                        df.to_csv(csv_path, index=False)
                                        self.console.print(f"[dim]✓ Added 'language' column to dataset ({len([l for l in detected_languages_per_text if l])} texts with detected language)[/dim]")
                            else:
                                # Fallback: ask user
                                self.console.print("[yellow]Could not detect languages automatically[/yellow]")
                                manual_langs = Prompt.ask("Expected language codes (e.g., en,fr,de)", default="")
                                if manual_langs.strip():
                                    confirmed_languages = set([l.strip().lower() for l in manual_langs.split(',') if l.strip()])
                        else:
                            self.console.print("[yellow]Not enough text samples for language detection[/yellow]")
                            manual_langs = Prompt.ask("Expected language codes (optional, e.g., en,fr,de)", default="")
                            if manual_langs.strip():
                                confirmed_languages = set([l.strip().lower() for l in manual_langs.split(',') if l.strip()])

                except Exception as e:
                    self.logger.debug(f"Language detection from content failed: {e}")
                    self.console.print("[yellow]Automatic detection failed. Please specify manually[/yellow]")
                    manual_langs = Prompt.ask("Expected language codes (optional, e.g., en,fr,de)", default="")
                    if manual_langs.strip():
                        confirmed_languages = set([l.strip().lower() for l in manual_langs.split(',') if l.strip()])

            # Model selection will be done later when training mode is selected
            # Store languages for later use

            # Step 6: Annotation Data Preview
            self.console.print("\n[bold cyan]━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[/bold cyan]")
            self.console.print("[bold cyan]  STEP 6:[/bold cyan] [bold white]Annotation Data Preview[/bold white]")
            self.console.print("[bold cyan]━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[/bold cyan]")
            self.console.print("[dim]🔍 Analyzing all annotation data to show you what labels/categories will be trained...[/dim]\n")

            # df already loaded above for language detection

            all_keys_values = {}  # {key: set_of_unique_values}
            total_samples = 0
            malformed_count = 0

            for idx, row in df.iterrows():
                annotation_val = row.get(annotation_column)
                if pd.isna(annotation_val) or annotation_val == '':
                    continue

                total_samples += 1
                try:
                    if isinstance(annotation_val, str):
                        # Try standard JSON first
                        try:
                            annotation_dict = json.loads(annotation_val)
                        except json.JSONDecodeError:
                            # Try Python literal (handles single quotes with escapes)
                            import ast
                            annotation_dict = ast.literal_eval(annotation_val)
                    elif isinstance(annotation_val, dict):
                        annotation_dict = annotation_val
                    else:
                        continue

                    # Extract keys and values
                    for key, value in annotation_dict.items():
                        if key not in all_keys_values:
                            all_keys_values[key] = set()

                        if isinstance(value, list):
                            for v in value:
                                if v is not None and v != '':
                                    all_keys_values[key].add(str(v))
                        elif value is not None and value != '':
                            all_keys_values[key].add(str(value))

                except (json.JSONDecodeError, AttributeError, TypeError, ValueError, SyntaxError) as e:
                    malformed_count += 1
                    continue

            # Display comprehensive preview with Rich table
            if all_keys_values:
                self.console.print(f"\n[bold cyan]📊 Complete Annotation Data Preview[/bold cyan]")
                self.console.print(f"[dim]Analyzed {total_samples} samples ({malformed_count} malformed)[/dim]\n")

                preview_table = Table(show_header=True, header_style="bold magenta", border_style="cyan", box=box.ROUNDED)
                preview_table.add_column("Key", style="yellow bold", width=20)
                preview_table.add_column("Unique Values", style="white", width=15, justify="center")
                preview_table.add_column("Sample Values", style="green", width=60)

                for key in sorted(all_keys_values.keys()):
                    values_set = all_keys_values[key]
                    num_values = len(values_set)

                    # Show first 10 values as sample
                    sample_values = sorted(values_set)[:10]
                    sample_str = ', '.join([f"'{v}'" for v in sample_values])
                    if num_values > 10:
                        sample_str += f" ... (+{num_values - 10} more)"

                    preview_table.add_row(
                        key,
                        str(num_values),
                        sample_str
                    )

                self.console.print(preview_table)
                self.console.print()

                # Show selection options
                self.console.print("[bold]💡 Training Options:[/bold]")
                self.console.print("  [dim]• You can choose to train on [cyan]ALL[/cyan] keys/values[/dim]")
                self.console.print("  [dim]• Or select [cyan]specific keys[/cyan] to train (asked later)[/dim]")
                self.console.print("  [dim]• Or select [cyan]specific values[/cyan] for each key (asked later)[/dim]\n")
            else:
                self.console.print("[yellow]⚠️  No valid annotation data found[/yellow]\n")

            # Step 6.5: Value Filtering (Optional) - CRITICAL FOR DATA QUALITY
            if all_keys_values:
                self.console.print("\n[bold cyan]━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[/bold cyan]")
                self.console.print("[bold cyan]  STEP 6.5:[/bold cyan] [bold white]Value Filtering (Optional)[/bold white]")
                self.console.print("[bold cyan]━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[/bold cyan]")
                self.console.print("[dim]📋 You can exclude specific values from your training data.[/dim]")
                self.console.print("[dim]   For example: Remove 'null' values, or exclude rare categories.[/dim]\n")

                filter_values = Confirm.ask(
                    "[bold yellow]Do you want to exclude any specific values from training?[/bold yellow]",
                    default=False
                )

                excluded_values = {}  # {key: [list_of_excluded_values]}
                rows_to_remove = []  # List of indices to remove from df

                if filter_values:
                    self.console.print("\n[bold]🔍 Value Filtering Configuration[/bold]\n")

                    # Ask for each key
                    for key in sorted(all_keys_values.keys()):
                        values_set = all_keys_values[key]
                        num_values = len(values_set)

                        if num_values == 0:
                            continue

                        # Display key and its values
                        self.console.print(f"\n[cyan]Key:[/cyan] [bold]{key}[/bold] ({num_values} values)")

                        # Create table for values with counts
                        values_table = Table(show_header=True, header_style="bold magenta", border_style="dim", box=box.SIMPLE)
                        values_table.add_column("Value", style="yellow", width=30)
                        values_table.add_column("Count", style="white", width=10, justify="right")
                        values_table.add_column("Percentage", style="green", width=12, justify="right")

                        # Count occurrences of each value in the dataset
                        value_counts = {}
                        for idx, row in df.iterrows():
                            annotation_val = row.get(annotation_column)
                            if pd.isna(annotation_val) or annotation_val == '':
                                continue

                            try:
                                if isinstance(annotation_val, str):
                                    try:
                                        annotation_dict = json.loads(annotation_val)
                                    except json.JSONDecodeError:
                                        import ast
                                        annotation_dict = ast.literal_eval(annotation_val)
                                elif isinstance(annotation_val, dict):
                                    annotation_dict = annotation_val
                                else:
                                    continue

                                if key in annotation_dict:
                                    val = annotation_dict[key]
                                    if isinstance(val, list):
                                        for v in val:
                                            if v is not None and v != '':
                                                v_str = str(v)
                                                value_counts[v_str] = value_counts.get(v_str, 0) + 1
                                    elif val is not None and val != '':
                                        v_str = str(val)
                                        value_counts[v_str] = value_counts.get(v_str, 0) + 1
                            except:
                                continue

                        # Display values with counts
                        sorted_values = sorted(values_set, key=lambda v: value_counts.get(v, 0), reverse=True)
                        for val in sorted_values:
                            count = value_counts.get(val, 0)
                            percentage = (count / total_samples * 100) if total_samples > 0 else 0
                            values_table.add_row(
                                val,
                                str(count),
                                f"{percentage:.1f}%"
                            )

                        self.console.print(values_table)

                        # Ask if user wants to exclude any values for this key
                        exclude_for_key = Confirm.ask(
                            f"[bold yellow]Exclude any values from '{key}'?[/bold yellow]",
                            default=False
                        )

                        if exclude_for_key:
                            self.console.print(f"[dim]Enter values to exclude (comma-separated), or type 'cancel' to skip[/dim]")
                            exclude_input = Prompt.ask(
                                f"[yellow]Values to exclude from '{key}'[/yellow]",
                                default=""
                            )

                            if exclude_input.lower() != 'cancel' and exclude_input.strip():
                                excluded_list = [v.strip() for v in exclude_input.split(',') if v.strip()]
                                # Validate that excluded values exist
                                valid_excluded = [v for v in excluded_list if v in values_set]
                                invalid_excluded = [v for v in excluded_list if v not in values_set]

                                if invalid_excluded:
                                    self.console.print(f"[yellow]⚠️  Warning: These values don't exist: {', '.join(invalid_excluded)}[/yellow]")

                                if valid_excluded:
                                    excluded_values[key] = valid_excluded
                                    self.console.print(f"[green]✓ Will exclude: {', '.join(valid_excluded)}[/green]")

                    # Now filter the DataFrame based on excluded values
                    if excluded_values:
                        self.console.print(f"\n[bold cyan]🔄 Filtering labels from dataset...[/bold cyan]")
                        self.console.print(f"[dim]Note: Removing excluded labels from samples, not the samples themselves.[/dim]\n")

                        original_count = len(df)
                        labels_removed_count = 0
                        samples_modified = 0

                        # Filter labels from each row (NOT remove rows)
                        for idx, row in df.iterrows():
                            annotation_val = row.get(annotation_column)
                            if pd.isna(annotation_val) or annotation_val == '':
                                continue

                            try:
                                # Parse annotation
                                if isinstance(annotation_val, str):
                                    try:
                                        annotation_dict = json.loads(annotation_val)
                                    except json.JSONDecodeError:
                                        import ast
                                        annotation_dict = ast.literal_eval(annotation_val)
                                elif isinstance(annotation_val, dict):
                                    annotation_dict = annotation_dict.copy()
                                else:
                                    continue

                                # Remove excluded values from annotation (NOT the row)
                                modified = False
                                for key, excluded_vals in excluded_values.items():
                                    if key in annotation_dict:
                                        val = annotation_dict[key]

                                        if isinstance(val, list):
                                            # Remove excluded values from list
                                            original_list = val.copy()
                                            val = [v for v in val if str(v) not in excluded_vals]
                                            if len(val) != len(original_list):
                                                modified = True
                                                labels_removed_count += len(original_list) - len(val)
                                            annotation_dict[key] = val if val else None

                                        elif val is not None and str(val) in excluded_vals:
                                            # Replace excluded value with None
                                            annotation_dict[key] = None
                                            modified = True
                                            labels_removed_count += 1

                                # Update the annotation in the DataFrame
                                if modified:
                                    samples_modified += 1
                                    # Convert back to JSON string if it was originally a string
                                    if isinstance(row[annotation_column], str):
                                        df.at[idx, annotation_column] = json.dumps(annotation_dict)
                                    else:
                                        df.at[idx, annotation_column] = annotation_dict

                            except Exception as e:
                                self.logger.warning(f"Error filtering row {idx}: {e}")
                                continue

                        # IMPORTANT: Do NOT remove samples even if they have no valid labels remaining
                        # Reason: Label filtering happens BEFORE key selection for training.
                        # A sample with all null/None labels might still be useful when training
                        # on specific keys later (e.g., user might select keys where null is valid).
                        # The training code will naturally skip samples without valid labels for selected keys.
                        removed_count = 0
                        filtered_count = len(df)

                        self.console.print(f"[green]✓ Label filtering complete:[/green]")
                        self.console.print(f"  • [cyan]Samples kept:[/cyan] {original_count} → {filtered_count}")
                        self.console.print(f"  • [cyan]Samples modified:[/cyan] {samples_modified}")
                        self.console.print(f"  • [cyan]Labels removed:[/cyan] {labels_removed_count}")
                        if removed_count > 0:
                            self.console.print(f"  • [yellow]Samples removed (empty):[/yellow] {removed_count}")
                        self.console.print()

                        # Recalculate all_keys_values with filtered data
                        all_keys_values = {}
                        total_samples = 0
                        malformed_count = 0

                        for idx, row in df.iterrows():
                            annotation_val = row.get(annotation_column)
                            if pd.isna(annotation_val) or annotation_val == '':
                                continue

                            total_samples += 1
                            try:
                                if isinstance(annotation_val, str):
                                    try:
                                        annotation_dict = json.loads(annotation_val)
                                    except json.JSONDecodeError:
                                        import ast
                                        annotation_dict = ast.literal_eval(annotation_val)
                                elif isinstance(annotation_val, dict):
                                    annotation_dict = annotation_val
                                else:
                                    continue

                                # Extract keys and values (excluding the filtered ones)
                                for key, value in annotation_dict.items():
                                    if key not in all_keys_values:
                                        all_keys_values[key] = set()

                                    if isinstance(value, list):
                                        for v in value:
                                            if v is not None and v != '':
                                                all_keys_values[key].add(str(v))
                                    elif value is not None and value != '':
                                        all_keys_values[key].add(str(value))

                            except (json.JSONDecodeError, AttributeError, TypeError, ValueError, SyntaxError) as e:
                                malformed_count += 1
                                continue

                        # Display updated summary
                        self.console.print("[bold]📊 Updated Data Summary:[/bold]")
                        summary_table = Table(show_header=True, header_style="bold magenta", border_style="cyan", box=box.ROUNDED)
                        summary_table.add_column("Key", style="yellow bold", width=25)
                        summary_table.add_column("Values (After Filtering)", style="white", width=50)

                        for key in sorted(all_keys_values.keys()):
                            values_set = all_keys_values[key]
                            num_values = len(values_set)
                            sample_str = ', '.join([f"'{v}'" for v in sorted(values_set)[:5]])
                            if num_values > 5:
                                sample_str += f" ... (+{num_values - 5} more)"

                            # Show what was excluded
                            if key in excluded_values:
                                excluded_str = f"[dim red](excluded: {', '.join(excluded_values[key])})[/dim red]"
                                summary_table.add_row(
                                    f"{key}\n{excluded_str}",
                                    f"[green]{num_values} values[/green]: {sample_str}"
                                )
                            else:
                                summary_table.add_row(
                                    key,
                                    f"{num_values} values: {sample_str}"
                                )

                        self.console.print(summary_table)
                        self.console.print()
                else:
                    self.console.print("[dim]✓ No values excluded - using all data[/dim]\n")

            # Step 7: Training Strategy Selection (SIMPLIFIED)
            self.console.print("\n[bold cyan]━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[/bold cyan]")
            self.console.print("[bold cyan]  STEP 7:[/bold cyan] [bold white]Training Strategy Selection[/bold white]")
            self.console.print("[bold cyan]━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[/bold cyan]")

            # Extract annotation keys and values from data
            annotation_keys_found = analysis.get('annotation_keys_found', set())
            sample_annotation = analysis.get('sample_data', {}).get(annotation_column, [])
            real_example_data = None

            if sample_annotation and len(sample_annotation) > 0:
                first_sample = sample_annotation[0]
                try:
                    if isinstance(first_sample, str):
                        real_example_data = json.loads(first_sample)
                    elif isinstance(first_sample, dict):
                        real_example_data = first_sample
                except:
                    pass

            # Show sample annotation for context
            if real_example_data:
                self.console.print("[bold]📄 Example annotation from your data:[/bold]")
                example_str = json.dumps(real_example_data, ensure_ascii=False, indent=2)
                self.console.print(f"[dim]{example_str}[/dim]\n")

            # Initialize
            detected_keys = []
            annotation_keys = None
            mode = "single-label"  # Will be derived from choice
            training_approach = "multi-class"  # Default

            # Step 6a: Show all annotation keys and their values
            if all_keys_values:
                detected_keys = sorted(all_keys_values.keys())
                self.console.print(f"[bold]📝 Annotation Keys Detected in Your Data:[/bold]\n")

                # Show all keys and their values
                for key in detected_keys:
                    num_values = len(all_keys_values[key])
                    values_preview = ', '.join([f"'{v}'" for v in sorted(all_keys_values[key])[:5]])
                    if num_values > 5:
                        values_preview += f" ... (+{num_values-5} more)"
                    self.console.print(f"  • [cyan]{key}[/cyan] ({num_values} values): {values_preview}")

                self.console.print("\n[dim]Options:[/dim]")
                self.console.print(f"  • [cyan]Leave blank[/cyan] → Use ALL {len(detected_keys)} keys with ALL their values")
                self.console.print(f"  • [cyan]Enter specific keys[/cyan] → Use only selected keys with ALL their values")
                if detected_keys:
                    self.console.print(f"    Example: '{detected_keys[0]}' → Use only {detected_keys[0]} key\n")
            elif analysis.get('annotation_keys_found'):
                detected_keys = sorted(analysis['annotation_keys_found'])
                self.console.print(f"\n[green]✓ Detected keys: {', '.join(detected_keys)}[/green]")
                self.console.print("[dim]Leave blank to use all keys, or specify which ones to include[/dim]\n")

            # Step 6b: Ask which keys to include
            keys_input = Prompt.ask("[bold yellow]Annotation keys to include[/bold yellow] (comma separated, or BLANK for ALL)", default="")
            annotation_keys = [key.strip() for key in keys_input.split(",") if key.strip()] or None

            # Step 6c: Ask multi-class vs one-vs-all (ALWAYS, not just for single key)
            # Determine which keys will be trained
            keys_to_train = annotation_keys if annotation_keys else detected_keys

            # Validate that all selected keys exist in all_keys_values
            invalid_keys = [key for key in keys_to_train if key not in all_keys_values]
            if invalid_keys:
                self.console.print(f"\n[bold red]❌ Error: Invalid keys selected[/bold red]")
                self.console.print(f"[yellow]The following keys do not exist in the dataset:[/yellow]")
                for key in invalid_keys:
                    self.console.print(f"  • [red]{key}[/red]")
                self.console.print(f"\n[bold cyan]Available keys:[/bold cyan]")
                for key in sorted(all_keys_values.keys()):
                    self.console.print(f"  • [green]{key}[/green]")
                self.console.print()
                return None

            # Calculate total number of models for each approach
            total_values_count = 0
            for key in keys_to_train:
                if key in all_keys_values:
                    total_values_count += len(all_keys_values[key])

            num_keys = len(keys_to_train)

            # ALWAYS ask the training approach question, even for binary classification
            # User may want one-vs-all even with 2 values
            if True:  # Always ask
                self.console.print(f"\n[bold cyan]🎯 Training Approach[/bold cyan]\n")

                if annotation_keys and len(annotation_keys) == 1:
                    # Single key selected
                    selected_key = annotation_keys[0]
                    num_unique_values = len(all_keys_values[selected_key])
                    values_list = sorted(all_keys_values[selected_key])
                    values_str = ', '.join([f"'{v}'" for v in values_list[:5]])
                    if num_unique_values > 5:
                        values_str += f" ... (+{num_unique_values-5} more)"

                    self.console.print(f"[bold]Selected:[/bold] '{selected_key}' ({num_unique_values} values)")
                    self.console.print(f"[dim]Values: {values_str}[/dim]\n")
                else:
                    # Multiple keys or ALL
                    self.console.print(f"[bold]Selected:[/bold] {'ALL' if not annotation_keys else len(annotation_keys)} keys ({num_keys} total)")
                    self.console.print(f"[dim]Total unique values across all keys: {total_values_count}[/dim]\n")

                # Create comparison table
                approach_table = Table(show_header=True, header_style="bold magenta", border_style="cyan", box=box.ROUNDED)
                approach_table.add_column("Approach", style="cyan bold", width=18)
                approach_table.add_column("What It Does", style="white", width=60)

                if annotation_keys and len(annotation_keys) == 1:
                    # Single key - simple explanation
                    selected_key = annotation_keys[0]
                    num_unique_values = len(all_keys_values[selected_key])
                    values_list = sorted(all_keys_values[selected_key])

                    approach_table.add_row(
                        "multi-class",
                        f"🎯 Trains ONE model for '{selected_key}'\n\n"
                        f"• Chooses between all {num_unique_values} values\n"
                        f"• Example: '{values_list[0]}' vs '{values_list[1]}' vs ...\n"
                        f"• Predicts exactly ONE value per text\n"
                        f"• [bold green]Total: 1 model[/bold green]\n\n"
                        "[bold cyan]Best for:[/bold cyan] Mutually exclusive categories"
                    )
                    approach_table.add_row(
                        "one-vs-all",
                        f"⚡ Trains {num_unique_values} binary models for '{selected_key}'\n\n"
                        f"• Model 1: '{values_list[0]}' vs NOT '{values_list[0]}'\n"
                        f"• Model 2: '{values_list[1]}' vs NOT '{values_list[1]}'\n"
                        f"• ... (one model per value)\n"
                        f"• [bold yellow]Total: {num_unique_values} models[/bold yellow]\n\n"
                        "[bold cyan]Best for:[/bold cyan] Imbalanced data, multiple labels per text"
                    )
                else:
                    # Multiple keys or ALL - offer hybrid and custom modes
                    # Analyze keys to determine hybrid strategy
                    keys_small = []  # ≤5 values
                    keys_large = []  # >5 values
                    for key in keys_to_train:
                        num_values = len(all_keys_values[key])
                        if num_values <= 5:
                            keys_small.append((key, num_values))
                        else:
                            keys_large.append((key, num_values))

                    hybrid_multiclass_count = len(keys_small)
                    hybrid_onevsall_count = sum(num_vals for _, num_vals in keys_large)
                    total_hybrid_models = hybrid_multiclass_count + hybrid_onevsall_count

                    approach_table.add_row(
                        "multi-class",
                        f"🎯 Trains ONE model PER KEY (not per value)\n\n"
                        f"• {num_keys} models total (one per annotation key)\n"
                        f"• Each model learns ALL values of ITS key\n"
                        f"• Example: One model for 'political_party' learns BQ, CAQ, CPC, etc.\n"
                        f"• Example: Another model for 'sentiment' learns positive, negative, neutral\n"
                        f"• [bold green]Total: {num_keys} models (one per key)[/bold green]\n\n"
                        "[bold cyan]Best for:[/bold cyan] Standard classification with mutually exclusive categories per key"
                    )
                    approach_table.add_row(
                        "one-vs-all",
                        f"⚡ Trains ONE model PER VALUE (not per key)\n\n"
                        f"• {total_values_count} binary models total (one per unique value)\n"
                        f"• Each model: 'value X' vs NOT 'value X'\n"
                        f"• Example: Separate model for 'political_party_BQ' (binary: BQ or not)\n"
                        f"• Example: Separate model for 'sentiment_positive' (binary: positive or not)\n"
                        f"• [bold yellow]Total: {total_values_count} models (one per value)[/bold yellow]\n\n"
                        "[bold cyan]Best for:[/bold cyan] Imbalanced data, or when texts can have multiple labels"
                    )
                    approach_table.add_row(
                        "hybrid",
                        f"🔀 SMART: Adapts strategy PER KEY based on number of values\n\n"
                        f"• Automatic strategy selection (threshold: 5 values):\n"
                        f"  - Keys with ≤5 values → Multi-class (1 model per key)\n"
                        f"  - Keys with >5 values → One-vs-all (1 model per value)\n"
                        f"• For your data:\n"
                        f"  - {hybrid_multiclass_count} keys use multi-class ({', '.join([k for k, _ in keys_small[:3]])}{'...' if len(keys_small) > 3 else ''})\n"
                        f"  - {len(keys_large)} keys use one-vs-all ({', '.join([k for k, _ in keys_large[:3]])}{'...' if len(keys_large) > 3 else ''})\n"
                        f"• [bold magenta]Total: {total_hybrid_models} models[/bold magenta]\n\n"
                        "[bold cyan]Best for:[/bold cyan] Mixed dataset with both simple and complex keys (RECOMMENDED)"
                    )
                    approach_table.add_row(
                        "custom",
                        f"⚙️  CUSTOM: You choose the strategy for EACH key individually\n\n"
                        f"• You'll be asked for each of the {num_keys} keys\n"
                        f"• Choose multi-class or one-vs-all per key\n"
                        f"• Example: multi-class for 'sentiment', one-vs-all for 'themes'\n"
                        f"• [bold blue]Total: Variable (depends on your choices)[/bold blue]\n\n"
                        "[bold cyan]Best for:[/bold cyan] Advanced users who want fine-grained control"
                    )

                self.console.print(approach_table)
                self.console.print()

                # Determine available choices and default based on context
                if annotation_keys and len(annotation_keys) == 1:
                    # Single key: no hybrid or custom modes
                    available_choices = ["multi-class", "one-vs-all", "back"]
                    default_approach = "multi-class"
                else:
                    # Multiple keys: all modes available
                    available_choices = ["multi-class", "one-vs-all", "hybrid", "custom", "back"]
                    default_approach = "hybrid"

                training_approach = Prompt.ask(
                    "[bold yellow]Training approach[/bold yellow]",
                    choices=available_choices,
                    default=default_approach
                )

                if training_approach == "back":
                    return None

                # Store per-key strategy decisions
                key_strategies = {}  # {key_name: 'multi-class' or 'one-vs-all'}

                if training_approach == "hybrid":
                    # Automatic: ≤5 values = multi-class, >5 values = one-vs-all
                    self.console.print("\n[bold cyan]📊 Hybrid Strategy Assignment:[/bold cyan]\n")

                    # Calculate total models for hybrid approach
                    total_hybrid_models = 0
                    for key in keys_to_train:
                        num_values = len(all_keys_values[key])
                        if num_values <= 5:
                            key_strategies[key] = 'multi-class'
                            total_hybrid_models += 1
                            self.console.print(f"  • [green]{key}[/green] ({num_values} values) → [bold]multi-class[/bold] (1 model)")
                        else:
                            key_strategies[key] = 'one-vs-all'
                            total_hybrid_models += num_values
                            self.console.print(f"  • [yellow]{key}[/yellow] ({num_values} values) → [bold]one-vs-all[/bold] ({num_values} models)")

                    self.console.print(f"\n[dim]Total models: {total_hybrid_models}[/dim]\n")

                elif training_approach == "custom":
                    # User chooses per key
                    self.console.print("\n[bold cyan]⚙️  Custom Strategy Selection:[/bold cyan]")
                    self.console.print("[dim]Choose the training strategy for each key individually.[/dim]\n")

                    total_custom_models = 0
                    for key in keys_to_train:
                        num_values = len(all_keys_values[key])
                        values_preview = ', '.join([f"'{v}'" for v in sorted(all_keys_values[key])[:3]])
                        if num_values > 3:
                            values_preview += f" ... (+{num_values-3} more)"

                        self.console.print(f"[bold]{key}[/bold] ({num_values} values)")
                        self.console.print(f"[dim]  Values: {values_preview}[/dim]")
                        self.console.print(f"  • [green]multi-class[/green]: 1 model learns all {num_values} values")
                        self.console.print(f"  • [yellow]one-vs-all[/yellow]: {num_values} binary models (one per value)")

                        key_choice = Prompt.ask(
                            f"  Strategy for '{key}'",
                            choices=["multi-class", "one-vs-all", "m", "o"],
                            default="multi-class" if num_values <= 5 else "one-vs-all"
                        )

                        # Normalize shortcuts
                        if key_choice == "m":
                            key_choice = "multi-class"
                        elif key_choice == "o":
                            key_choice = "one-vs-all"

                        key_strategies[key] = key_choice

                        if key_choice == "multi-class":
                            total_custom_models += 1
                            self.console.print(f"  ✓ Will train [green]1 model[/green] for {key}\n")
                        else:
                            total_custom_models += num_values
                            self.console.print(f"  ✓ Will train [yellow]{num_values} models[/yellow] for {key}\n")

                    self.console.print(f"[bold cyan]Total models to train: {total_custom_models}[/bold cyan]\n")

                elif training_approach == "multi-class":
                    # All keys use multi-class
                    for key in keys_to_train:
                        key_strategies[key] = 'multi-class'

                elif training_approach == "one-vs-all":
                    # All keys use one-vs-all
                    for key in keys_to_train:
                        key_strategies[key] = 'one-vs-all'

            # Step 6c: Data Split Configuration
            self.console.print("\n[bold cyan]━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[/bold cyan]")
            self.console.print("[bold cyan]  STEP 6c:[/bold cyan] [bold white]Data Split Configuration[/bold white]")
            self.console.print("[bold cyan]━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[/bold cyan]")

            split_config = self._configure_data_splits(
                keys_to_train=keys_to_train,
                all_keys_values=all_keys_values,
                training_approach=training_approach,
                key_strategies=key_strategies,
                total_samples=len(df)
            )

            if split_config is None:
                return None

            # Display split configuration summary
            self._display_split_summary(
                split_config=split_config,
                keys_to_train=keys_to_train,
                all_keys_values=all_keys_values,
                key_strategies=key_strategies
            )

            # Note: split_config will be stored in bundle.metadata after bundle is created

            # Step 6d: Label naming strategy
            self.console.print("\n[bold]🏷️  Label Naming Strategy:[/bold]")
            self.console.print("[dim]This determines how label names appear in your training files and model predictions.[/dim]\n")

            # Generate examples based on SELECTED keys (not random example data)
            # Build concrete transformation examples
            transformation_examples = []
            for key in keys_to_train[:2]:  # Show 2 examples for clarity
                if key in all_keys_values:
                    values = sorted(all_keys_values[key])[:2]  # First 2 values
                    if values:
                        for val in values:
                            transformation_examples.append({
                                'key': key,
                                'value': val,
                                'key_value': f"{key}_{val}",
                                'value_only': val
                            })

            # Create comparison table
            strategy_table = Table(show_header=True, header_style="bold magenta", border_style="cyan", box=box.ROUNDED)
            strategy_table.add_column("Strategy", style="cyan bold", width=15)
            strategy_table.add_column("Format", style="white", width=25)
            strategy_table.add_column("When to Use", style="white", width=40)

            # Build key_value example string
            if transformation_examples:
                kv_format_examples = [f"'{ex['key_value']}'" for ex in transformation_examples[:3]]
                kv_format = f"key_value\nExample: {', '.join(kv_format_examples)}"
            else:
                kv_format = "key_value\nExample: 'sentiment_positive'"

            # Build value_only example string
            if transformation_examples:
                vo_format_examples = [f"'{ex['value_only']}'" for ex in transformation_examples[:3]]
                vo_format = f"value_only\nExample: {', '.join(vo_format_examples)}"
            else:
                vo_format = "value_only\nExample: 'positive'"

            strategy_table.add_row(
                "key_value",
                "Includes key prefix\n[dim](key_value)[/dim]",
                "✓ Training [bold]multiple keys[/bold]\n"
                "✓ Values might overlap between keys\n"
                "✓ [green]Recommended for most cases[/green]"
            )

            strategy_table.add_row(
                "value_only",
                "Only the value\n[dim](no prefix)[/dim]",
                "✓ Training [bold]single key only[/bold]\n"
                "✓ Values are unique across dataset\n"
                "⚠️  [yellow]Can cause conflicts with multiple keys[/yellow]"
            )

            self.console.print(strategy_table)
            self.console.print()

            # Show concrete transformation if we have examples
            if transformation_examples:
                self.console.print("[bold]📋 How Your Data Will Be Transformed:[/bold]\n")

                transform_table = Table(show_header=True, header_style="bold magenta", border_style="green", box=box.SIMPLE)
                transform_table.add_column("Original (key → value)", style="cyan", width=35)
                transform_table.add_column("key_value format", style="green", width=25)
                transform_table.add_column("value_only format", style="yellow", width=20)

                for ex in transformation_examples[:4]:  # Show max 4 examples
                    transform_table.add_row(
                        f"{ex['key']} → {ex['value']}",
                        ex['key_value'],
                        ex['value_only']
                    )

                self.console.print(transform_table)
                self.console.print()

            # Show warning if multiple keys and value_only
            if len(keys_to_train) > 1:
                self.console.print("[bold yellow]💡 Recommendation:[/bold yellow]")
                self.console.print(f"[dim]You selected {len(keys_to_train)} keys. Use [bold cyan]key_value[/bold cyan] to avoid label conflicts.")
                self.console.print(f"[dim]Example: If both 'affiliation' and 'gender' have value 'no', they would conflict with [yellow]value_only[/yellow].[/dim]\n")
            else:
                self.console.print("[dim]💡 With a single key, both strategies work fine. [cyan]key_value[/cyan] is still recommended for consistency.[/dim]\n")

            label_strategy = Prompt.ask("Label naming strategy", choices=["key_value", "value_only", "back"], default="key_value")
            if label_strategy == "back":
                return None

            # Derive mode based on approach
            if training_approach == "one-vs-all":
                mode = "multi-label"  # one-vs-all uses multi-label infrastructure
            else:
                mode = "single-label"  # multi-class uses single-label infrastructure

            # Step 8: Additional Columns (ID, Language)
            self.console.print("\n[bold cyan]━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[/bold cyan]")
            self.console.print("[bold cyan]  STEP 8:[/bold cyan] [bold white]Additional Columns (Optional)[/bold white]")
            self.console.print("[bold cyan]━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[/bold cyan]")
            self.console.print("[dim]Optional: Select ID and language columns if available in your dataset.[/dim]\n")

            # Use ID column already detected in analysis
            id_column = None
            if analysis['id_column_candidates']:
                id_column = analysis['id_column_candidates'][0]
                self.console.print(f"[green]✓ ID column detected: '{id_column}'[/green]")
                # Allow user to override if they want
                if all_columns:
                    self.console.print(f"[dim]  Available columns: {', '.join(all_columns)}[/dim]")
                while True:
                    override_id = Prompt.ask("\n[bold yellow]Identifier column (optional)[/bold yellow]", default=id_column)
                    if not override_id or override_id in all_columns:
                        if override_id and override_id != id_column:
                            id_column = override_id
                        break
                    self.console.print(f"[red]✗ Column '{override_id}' not found in dataset![/red]")
                    self.console.print(f"[dim]Available columns: {', '.join(all_columns)}[/dim]")
            else:
                self.console.print("[dim]No ID columns detected - automatic IDs will be generated[/dim]")
                if all_columns:
                    self.console.print(f"[dim]  Available columns: {', '.join(all_columns)}[/dim]")
                while True:
                    id_column_input = Prompt.ask("\n[bold yellow]Identifier column (optional)[/bold yellow]", default="")
                    if not id_column_input or id_column_input in all_columns:
                        if id_column_input:
                            id_column = id_column_input
                        break
                    self.console.print(f"[red]✗ Column '{id_column_input}' not found in dataset![/red]")
                    self.console.print(f"[dim]Available columns: {', '.join(all_columns)}[/dim]")

            # Language column handling - check if already processed in Step 5
            # Skip if we already did language detection (either with column or auto-detection)
            language_already_processed = 'lang_column' in locals() and confirmed_languages

            if language_already_processed:
                # Language was already handled in Step 5
                if lang_column:
                    self.console.print(f"\n[green]✓ Language column from Step 5: '{lang_column}'[/green]")
                else:
                    self.console.print(f"\n[green]✓ Languages detected in Step 5: {', '.join([l.upper() for l in sorted(confirmed_languages)])}[/green]")
                    self.console.print(f"[dim]  (Using automatic language detection - no specific column)[/dim]")
            elif analysis['language_column_candidates']:
                # Language column detected but Step 5 was skipped - ask user
                lang_column_candidate = analysis['language_column_candidates'][0]
                self.console.print(f"\n[green]✓ Language column detected: '{lang_column_candidate}'[/green]")
                if all_columns:
                    self.console.print(f"[dim]  Available columns: {', '.join(all_columns)}[/dim]")
                while True:
                    override_lang = Prompt.ask("\n[bold yellow]Language column (optional)[/bold yellow]", default=lang_column_candidate)
                    if not override_lang or override_lang in all_columns:
                        lang_column = override_lang if override_lang else lang_column_candidate
                        break
                    self.console.print(f"[red]✗ Column '{override_lang}' not found in dataset![/red]")
                    self.console.print(f"[dim]Available columns: {', '.join(all_columns)}[/dim]")

            # Handle training approach with key_strategies support
            if 'training_approach' in locals() and training_approach == "one-vs-all":
                # Convert to multi-label format for one-vs-all training
                request = TrainingDataRequest(
                    input_path=csv_path,
                    format="llm_json",
                    text_column=text_column,
                    annotation_column=annotation_column,
                    annotation_keys=annotation_keys,
                    label_strategy=label_strategy,
                    mode="multi-label",  # Use multi-label to trigger one-vs-all training
                    id_column=id_column or None,
                    lang_column=lang_column or None,
                    key_strategies={k: 'one-vs-all' for k in (annotation_keys or [])} if 'key_strategies' not in locals() else None
                )
                bundle = builder.build(request)

                # Mark this as one-vs-all for distributed training
                if bundle:
                    bundle.metadata['training_approach'] = 'one-vs-all'
                    bundle.metadata['original_strategy'] = 'single-label'
            else:
                # Standard mode (can be multi-class, hybrid, or custom)
                # Pass key_strategies if available (from hybrid/custom mode)
                request = TrainingDataRequest(
                    input_path=csv_path,
                    format="llm_json",
                    text_column=text_column,
                    annotation_column=annotation_column,
                    annotation_keys=annotation_keys,
                    label_strategy=label_strategy,
                    mode=mode,
                    id_column=id_column or None,
                    lang_column=lang_column or None,
                    key_strategies=key_strategies if 'key_strategies' in locals() else None
                )
                bundle = builder.build(request)

            # Store language metadata in bundle for later use (model selection will happen in training mode)
            if bundle:
                if confirmed_languages:
                    bundle.metadata['confirmed_languages'] = confirmed_languages
                if language_distribution:
                    bundle.metadata['language_distribution'] = language_distribution
                # Save training approach if user made a choice (multi-label/one-vs-all)
                if 'training_approach' in locals() and training_approach:
                    bundle.metadata['training_approach'] = training_approach
                # Store annotation keys (categories) for benchmark mode
                # Use keys_to_train (which contains all keys when user selects ALL)
                if 'keys_to_train' in locals() and keys_to_train:
                    bundle.metadata['categories'] = keys_to_train
                elif 'annotation_keys' in locals() and annotation_keys:
                    bundle.metadata['categories'] = annotation_keys
                # Store source file and annotation column for benchmark mode
                bundle.metadata['source_file'] = str(csv_path)
                bundle.metadata['annotation_column'] = annotation_column
                # Store split configuration if it exists
                if 'split_config' in locals() and split_config:
                    bundle.metadata['split_config'] = split_config
                # Text length stats for intelligent model selection later
                # ONLY calculate if not already done (avoid duplicate analysis)
                if 'text_length_stats' in locals() and text_length_stats:
                    # Already calculated with user interaction - reuse it
                    bundle.metadata['text_length_stats'] = text_length_stats
                elif text_column in df.columns:
                    # Not calculated yet - do it now without UI
                    text_length_stats = self.analyze_text_lengths(
                        df=df,
                        text_column=text_column,
                        display_results=False  # Silent calculation
                    )
                    bundle.metadata['text_length_stats'] = text_length_stats

            return bundle

        if format_choice == "category-csv":
            # DEVELOPMENT MODE: This format is not yet available
            self.console.print("\n[bold red]❌ Error: category-csv format is currently under development[/bold red]")
            self.console.print("[yellow]This format will be available in a future release after thorough testing.[/yellow]")
            self.console.print("[dim]Please use 'llm-json' format instead.[/dim]\n")
            return None

            # Ask user for training strategy (mono-label vs multi-label)
            self.console.print("\n[bold cyan]📊 Training Strategy Selection[/bold cyan]\n")
            self.console.print("[dim]Choose how to handle the labels in your dataset:[/dim]\n")

            strategy_table = Table(show_header=True, header_style="bold magenta", border_style="cyan", box=box.ROUNDED)
            strategy_table.add_column("Strategy", style="cyan bold", width=18)
            strategy_table.add_column("Description", style="white", width=60)

            strategy_table.add_row(
                "single-label",
                "🎯 Each sample has ONE label/category\n"
                "✓ Best for: classification tasks (sentiment, topic, etc.)\n"
                "✓ Example: each text is either 'positive' OR 'negative'"
            )
            strategy_table.add_row(
                "multi-label",
                "🏷️  Each sample can have MULTIPLE labels\n"
                "✓ Best for: tagging, multiple categories per text\n"
                "✓ Example: a text can be 'politics' AND 'economy' AND 'urgent'"
            )

            self.console.print(strategy_table)
            self.console.print()

            mode = Prompt.ask(
                "[bold yellow]Training strategy[/bold yellow]",
                choices=["single-label", "multi-label", "back"],
                default="single-label"
            )

            if mode == "back":
                return None

            # If single-label, ALWAYS ask about training approach (even for binary/2 classes)
            training_approach = "multi-class"  # Default
            if mode == "single-label":
                # Count unique labels
                import pandas as pd
                df = pd.read_csv(selection['data_path'])
                label_column = selection['label_column']
                num_unique_labels = df[label_column].nunique()

                # Always ask, even for binary classification (user may want one-vs-all)
                self.console.print(f"\n[bold cyan]🎯 Training Approach for {num_unique_labels} Categories[/bold cyan]\n")
                if num_unique_labels == 2:
                    self.console.print("[dim]Even with 2 categories, you can choose between multi-class or one-vs-all:[/dim]\n")
                else:
                    self.console.print("[dim]Choose how to train with multiple categories:[/dim]\n")

                approach_table = Table(show_header=True, header_style="bold magenta", border_style="cyan", box=box.ROUNDED)
                approach_table.add_column("Approach", style="cyan bold", width=18)
                approach_table.add_column("Description", style="white", width=60)

                approach_table.add_row(
                    "multi-class",
                    f"🎯 ONE model predicting among {num_unique_labels} categories\n"
                    "✓ Faster training (1 model only)\n"
                    "✓ Model learns relationships between categories\n"
                    "✓ Best for: general classification with balanced data"
                )
                approach_table.add_row(
                    "one-vs-all",
                    f"⚡ {num_unique_labels} binary models (one per category)\n"
                    "✓ Each model: 'Category X' vs 'NOT Category X'\n"
                    "✓ Better for: imbalanced data or category-specific tuning\n"
                    "✓ Longer training but more flexible"
                )

                self.console.print(approach_table)
                self.console.print()

                training_approach = Prompt.ask(
                    "[bold yellow]Training approach[/bold yellow]",
                    choices=["multi-class", "one-vs-all", "back"],
                    default="multi-class"
                )

                if training_approach == "back":
                    return None

            # If one-vs-all, convert to multi-label format (one binary file per category)
            if training_approach == "one-vs-all":
                # Convert single-label multi-class to multi-label one-vs-all format
                # This will create one binary file per category
                request = TrainingDataRequest(
                    input_path=selection['data_path'],
                    format="category_csv",
                    text_column=selection['text_column'],
                    label_column=selection['label_column'],
                    id_column=selection.get('id_column'),
                    lang_column=selection.get('lang_column'),
                    mode="multi-label",  # Use multi-label to trigger one-vs-all training
                )
                bundle = builder.build(request)

                # Mark this as one-vs-all for distributed training
                if bundle:
                    bundle.metadata['training_approach'] = 'one-vs-all'
                    bundle.metadata['original_strategy'] = 'single-label-multiclass'
            else:
                # Standard multi-class: one model for all categories
                request = TrainingDataRequest(
                    input_path=selection['data_path'],
                    format="category_csv",
                    text_column=selection['text_column'],
                    label_column=selection['label_column'],
                    id_column=selection.get('id_column'),
                    lang_column=selection.get('lang_column'),
                    mode=mode,
                )
                bundle = builder.build(request)

            # Store recommended model and metadata in bundle for later use
            if bundle:
                if selection.get('recommended_model'):
                    bundle.recommended_model = selection['recommended_model']
                if selection.get('confirmed_languages'):
                    bundle.metadata['confirmed_languages'] = selection['confirmed_languages']
                if selection.get('language_distribution'):
                    bundle.metadata['language_distribution'] = selection['language_distribution']
                if selection.get('text_length_stats'):
                    bundle.metadata['text_length_stats'] = selection['text_length_stats']
                # Store split configuration if it exists
                if 'split_config' in locals() and split_config:
                    bundle.metadata['split_config'] = split_config

            return bundle

        if format_choice == "binary-long":
            # DEVELOPMENT MODE: This format is not yet available
            self.console.print("\n[bold red]❌ Error: binary-long format is currently under development[/bold red]")
            self.console.print("[yellow]This format will be available in a future release after thorough testing.[/yellow]")
            self.console.print("[dim]Please use 'llm-json' format instead.[/dim]\n")
            return None

            # Use sophisticated universal selector
            selection = self._training_studio_intelligent_dataset_selector(format_type="binary-long")
            if not selection:
                return None

            # Binary-long specific: need category and value columns
            category_column = Prompt.ask("\n[bold yellow]Category column[/bold yellow]", default="category")
            value_column = Prompt.ask("[bold yellow]Value column (0/1)[/bold yellow]", default="value")

            request = TrainingDataRequest(
                input_path=selection['data_path'],
                format="binary_long_csv",
                text_column=selection['text_column'],
                category_column=category_column,
                value_column=value_column,
                id_column=selection.get('id_column'),
                lang_column=selection.get('lang_column'),
                mode="multi-label",
            )
            bundle = builder.build(request)

            # Store recommended model and metadata in bundle for later use
            if bundle:
                if selection.get('recommended_model'):
                    bundle.recommended_model = selection['recommended_model']
                if selection.get('confirmed_languages'):
                    bundle.metadata['confirmed_languages'] = selection['confirmed_languages']
                if selection.get('language_distribution'):
                    bundle.metadata['language_distribution'] = selection['language_distribution']
                if selection.get('text_length_stats'):
                    bundle.metadata['text_length_stats'] = selection['text_length_stats']
                # Store split configuration if it exists
                if 'split_config' in locals() and split_config:
                    bundle.metadata['split_config'] = split_config

            return bundle

        if format_choice == "jsonl-single":
            # DEVELOPMENT MODE: This format is not yet available
            self.console.print("\n[bold red]❌ Error: jsonl-single format is currently under development[/bold red]")
            self.console.print("[yellow]This format will be available in a future release after thorough testing.[/yellow]")
            self.console.print("[dim]Please use 'llm-json' format instead.[/dim]\n")
            return None

            # Use sophisticated universal selector
            selection = self._training_studio_intelligent_dataset_selector(format_type="jsonl-single")
            if not selection:
                return None

            request = TrainingDataRequest(
                input_path=selection['data_path'],
                format="jsonl_single",
                text_column=selection['text_column'],
                label_column=selection['label_column'],
                mode="single-label",
            )
            bundle = builder.build(request)

            # Store recommended model and metadata in bundle for later use
            if bundle:
                if selection.get('recommended_model'):
                    bundle.recommended_model = selection['recommended_model']
                if selection.get('confirmed_languages'):
                    bundle.metadata['confirmed_languages'] = selection['confirmed_languages']
                if selection.get('language_distribution'):
                    bundle.metadata['language_distribution'] = selection['language_distribution']
                if selection.get('text_length_stats'):
                    bundle.metadata['text_length_stats'] = selection['text_length_stats']
                # Store split configuration if it exists
                if 'split_config' in locals() and split_config:
                    bundle.metadata['split_config'] = split_config

            return bundle

        # jsonl-multi (should not be reached - format is not in choices list)
        if format_choice == "jsonl-multi":
            # DEVELOPMENT MODE: This format is not yet available
            self.console.print("\n[bold red]❌ Error: jsonl-multi format is currently under development[/bold red]")
            self.console.print("[yellow]This format will be available in a future release after thorough testing.[/yellow]")
            self.console.print("[dim]Please use 'llm-json' format instead.[/dim]\n")
            return None

        # Fallback: unrecognized format
        self.console.print(f"\n[bold red]❌ Error: Unknown format '{format_choice}'[/bold red]")
        self.console.print("[dim]Supported formats: llm-json[/dim]\n")
        return None

    def _display_model_details(self, model_id: str, MODEL_METADATA: dict):
        """Display complete model information including full description."""
        from rich.panel import Panel
        from rich.text import Text

        meta = MODEL_METADATA.get(model_id, {})
        if not meta:
            self.console.print(f"[red]Model '{model_id}' not found in metadata[/red]")
            return

        # Create detailed info panel
        info = Text()
        info.append(f"Model: ", style="bold cyan")
        info.append(f"{model_id}\n\n", style="bold white")

        info.append(f"Languages: ", style="bold yellow")
        langs = ', '.join(meta.get('languages', ['?']))
        info.append(f"{langs}\n", style="white")

        info.append(f"Max Tokens: ", style="bold blue")
        info.append(f"{meta.get('max_length', '?')}\n", style="white")

        info.append(f"Size: ", style="bold magenta")
        info.append(f"{meta.get('size', '?')}\n\n", style="white")

        info.append(f"Description:\n", style="bold green")
        # Full description, not truncated
        full_desc = meta.get('description', 'No description available')
        info.append(full_desc, style="dim white")

        panel = Panel(info, title="📋 Model Details", border_style="cyan", expand=False)
        self.console.print(panel)

    def _run_benchmark_mode(
        self,
        bundle: TrainingDataBundle,
        languages: set,
        train_by_language: bool,
        text_length_avg: float,
        prefers_long_models: bool
    ) -> Optional[Dict[str, Any]]:
        """
        Execute complete benchmark mode workflow.

        Steps:
        1. Multi-model selection (≥2 per language or ≥2 multilingual)
        2. Class imbalance analysis
        3. Category selection
        4. Benchmark execution (quick training 3-5 epochs)
        5. Results display and ranking
        6. Final model selection

        Args:
            bundle: Training data bundle
            languages: Set of detected languages
            train_by_language: Whether training per-language
            text_length_avg: Average text length
            prefers_long_models: Whether long models preferred

        Returns:
            Dict with selected models or None to stop
        """
        from llm_tool.utils.model_display import get_recommended_models, MODEL_METADATA
        from llm_tool.utils.benchmark_utils import (
            analyze_categories_imbalance,
            select_benchmark_categories,
            format_imbalance_summary,
            create_benchmark_dataset,
            compare_model_results
        )
        from llm_tool.trainers.model_trainer import ModelTrainer, TrainingConfig
        from rich.prompt import IntPrompt
        from rich.table import Table
        from rich import box
        import tempfile
        from pathlib import Path
        import json

        self.console.print("\n[bold cyan]═══════════════════════════════════════════════════════════════[/bold cyan]")
        self.console.print("[bold cyan]           🎯 BENCHMARK MODE - Model Comparison                [/bold cyan]")
        self.console.print("[bold cyan]═══════════════════════════════════════════════════════════════[/bold cyan]\n")

        # Load original source file from bundle metadata (not the transformed JSONL)
        source_file = bundle.metadata.get('source_file')
        if not source_file:
            self.console.print("[red]❌ Cannot run benchmark: source file not found in bundle metadata[/red]")
            self.console.print("[dim]Bundle metadata keys:[/dim] " + ", ".join(bundle.metadata.keys()))
            return None

        source_path = Path(source_file)
        if not source_path.exists():
            self.console.print(f"[red]❌ Source file not found: {source_path}[/red]")
            return None

        # Get annotation column from metadata
        annotation_column = bundle.metadata.get('annotation_column')
        if not annotation_column:
            self.console.print("[red]❌ Annotation column not found in bundle metadata[/red]")
            return None

        # Load data based on file format
        try:
            file_ext = source_path.suffix.lower()
            if file_ext == '.csv':
                original_dataframe = pd.read_csv(source_path)
            elif file_ext in ['.xlsx', '.xls']:
                original_dataframe = pd.read_excel(source_path)
            elif file_ext == '.parquet':
                original_dataframe = pd.read_parquet(source_path)
            elif file_ext in ['.json', '.jsonl']:
                original_dataframe = pd.read_json(source_path, lines=(file_ext == '.jsonl'))
            else:
                self.console.print(f"[red]❌ Unsupported file format: {file_ext}[/red]")
                return None
        except Exception as e:
            self.console.print(f"[red]❌ Error loading data: {e}[/red]")
            return None

        self.logger.debug(f"Loaded source file: {source_path}")
        self.logger.debug(f"Using annotation column: {annotation_column}")

        # ======================== STEP 1: Multi-Model Selection ========================
        self.console.print("[bold]STEP 1: Select Models to Benchmark[/bold]\n")

        selected_models_benchmark = []
        models_by_language_benchmark = {}

        if train_by_language:
            # Select multiple models per language
            self.console.print(f"[yellow]You'll select at least 2 models for each language: {', '.join(sorted(languages))}[/yellow]\n")

            for lang in sorted(languages):
                self.console.print(f"\n[bold yellow]{'─'*60}[/bold yellow]")
                self.console.print(f"[bold yellow]🎯 Selecting models for {lang} texts[/bold yellow]")
                self.console.print(f"[bold yellow]{'─'*60}[/bold yellow]\n")

                lang_models = []

                # Get recommendations
                lang_recommended = get_recommended_models(
                    languages={lang},
                    avg_text_length=text_length_avg,
                    requires_long_model=prefers_long_models,
                    top_n=10
                )

                while True:
                    # Show models
                    if lang_recommended:
                        self.console.print(f"[bold cyan]🎯 Top 10 Recommended Models for {lang}:[/bold cyan]\n")

                        models_table = Table(show_header=True, header_style="bold magenta", border_style="cyan", box=box.ROUNDED)
                        models_table.add_column("#", style="yellow", width=3)
                        models_table.add_column("Model ID", style="cyan", width=45)
                        models_table.add_column("Languages", style="green", width=15)
                        models_table.add_column("Max Tokens", style="blue", width=11)
                        models_table.add_column("Size", style="magenta", width=10)
                        models_table.add_column("Description", style="white", width=46)

                        for idx, model_id in enumerate(lang_recommended[:10], 1):
                            meta = MODEL_METADATA.get(model_id, {})
                            langs = ', '.join(meta.get('languages', ['?']))
                            max_len = str(meta.get('max_length', '?'))
                            size = meta.get('size', '?')
                            desc = meta.get('description', '')[:44] + '..' if len(meta.get('description', '')) > 44 else meta.get('description', '')
                            models_table.add_row(str(idx), model_id, langs, max_len, size, desc)

                        self.console.print(models_table)
                        # Default to next model in recommendations based on how many already selected
                        default_idx = min(len(lang_models), len(lang_recommended) - 1)
                        default_model = lang_recommended[default_idx]
                    else:
                        default_model = 'bert-base-uncased'

                    if lang_models:
                        self.console.print(f"\n[green]✓ Already selected {len(lang_models)} model(s) for {lang}:[/green]")
                        for m in lang_models:
                            self.console.print(f"  • {m}")

                    # Show selection hint
                    self.console.print(f"\n[dim]💡 Tip: Type 'info X' (e.g., 'info 1') to see full details of a model[/dim]")

                    model_input = Prompt.ask(
                        f"\n[bold yellow]{'Add' if lang_models else 'Select'} model #{len(lang_models)+1} for {lang}[/bold yellow]",
                        default=default_model
                    )

                    # Check if user wants info on a model
                    if model_input.lower().startswith('info '):
                        info_target = model_input[5:].strip()
                        if info_target.isdigit():
                            info_idx = int(info_target) - 1
                            if lang_recommended and 0 <= info_idx < len(lang_recommended):
                                self._display_model_details(lang_recommended[info_idx], MODEL_METADATA)
                            else:
                                self.console.print(f"[red]Invalid model number: {info_target}[/red]")
                        else:
                            self._display_model_details(info_target, MODEL_METADATA)
                        continue  # Ask again for selection

                    # Parse selection
                    if model_input.isdigit():
                        idx = int(model_input) - 1
                        if lang_recommended and 0 <= idx < len(lang_recommended):
                            selected_model = lang_recommended[idx]
                        else:
                            selected_model = default_model
                    else:
                        selected_model = model_input

                    # Validate model exists (check in MODEL_METADATA or HuggingFace format)
                    if selected_model not in MODEL_METADATA and '/' not in selected_model:
                        self.console.print(f"[yellow]⚠️  Model '{selected_model}' not found in metadata[/yellow]")
                        # Ask if they want to use it anyway
                        use_anyway = Confirm.ask(
                            f"[yellow]Use '{selected_model}' anyway? (may fail if invalid)[/yellow]",
                            default=False
                        )
                        if not use_anyway:
                            continue  # Ask for selection again

                    lang_models.append(selected_model)
                    self.console.print(f"[green]✓ Added: {selected_model}[/green]")

                    # Display full model details after selection
                    self._display_model_details(selected_model, MODEL_METADATA)

                    # Ask to add more (require at least 2)
                    if len(lang_models) >= 2:
                        add_more = Confirm.ask(
                            f"\n[cyan]Add another model for {lang}? (Current: {len(lang_models)})[/cyan]",
                            default=False
                        )
                        if not add_more:
                            break
                    else:
                        self.console.print(f"[yellow]⚠️  At least 2 models required. Please select one more.[/yellow]")

                models_by_language_benchmark[lang] = lang_models
                self.console.print(f"\n[green]✓ {len(lang_models)} models selected for {lang}[/green]")

        else:
            # Select multiple multilingual or single-language models
            self.console.print("[yellow]Select at least 2 models to benchmark[/yellow]\n")

            # Determine recommendation language
            if len(languages) > 1:
                languages_for_recommendation = {'MULTI'}
            else:
                languages_for_recommendation = languages

            recommended_models_list = get_recommended_models(
                languages=languages_for_recommendation,
                avg_text_length=text_length_avg,
                requires_long_model=prefers_long_models,
                top_n=10
            )

            while True:
                # Show models
                if recommended_models_list:
                    self.console.print("[bold cyan]🎯 Top 10 Recommended Models:[/bold cyan]\n")

                    models_table = Table(show_header=True, header_style="bold magenta", border_style="cyan", box=box.ROUNDED)
                    models_table.add_column("#", style="yellow", width=3)
                    models_table.add_column("Model ID", style="cyan", width=45)
                    models_table.add_column("Languages", style="green", width=15)
                    models_table.add_column("Max Tokens", style="blue", width=11)
                    models_table.add_column("Size", style="magenta", width=10)
                    models_table.add_column("Description", style="white", width=46)

                    for idx, model_id in enumerate(recommended_models_list[:10], 1):
                        meta = MODEL_METADATA.get(model_id, {})
                        langs = ', '.join(meta.get('languages', ['?']))
                        max_len = str(meta.get('max_length', '?'))
                        size = meta.get('size', '?')
                        desc = meta.get('description', '')[:44] + '..' if len(meta.get('description', '')) > 44 else meta.get('description', '')
                        models_table.add_row(str(idx), model_id, langs, max_len, size, desc)

                    self.console.print(models_table)

                if selected_models_benchmark:
                    self.console.print(f"\n[green]✓ Already selected {len(selected_models_benchmark)} model(s):[/green]")
                    for m in selected_models_benchmark:
                        self.console.print(f"  • {m}")

                # Default to next model in recommendations based on how many already selected
                if recommended_models_list:
                    default_idx = min(len(selected_models_benchmark), len(recommended_models_list) - 1)
                    default_model = recommended_models_list[default_idx]
                else:
                    default_model = 'bert-base-uncased'

                # Show selection hint
                self.console.print(f"\n[dim]💡 Tip: Type 'info X' (e.g., 'info 1') to see full details of a model[/dim]")

                model_input = Prompt.ask(
                    f"\n[bold yellow]{'Add' if selected_models_benchmark else 'Select'} model #{len(selected_models_benchmark)+1}[/bold yellow]",
                    default=default_model
                )

                # Check if user wants info on a model
                if model_input.lower().startswith('info '):
                    info_target = model_input[5:].strip()
                    if info_target.isdigit():
                        info_idx = int(info_target) - 1
                        if recommended_models_list and 0 <= info_idx < len(recommended_models_list):
                            self._display_model_details(recommended_models_list[info_idx], MODEL_METADATA)
                        else:
                            self.console.print(f"[red]Invalid model number: {info_target}[/red]")
                    else:
                        self._display_model_details(info_target, MODEL_METADATA)
                    continue  # Ask again for selection

                # Parse selection
                if model_input.isdigit():
                    idx = int(model_input) - 1
                    if recommended_models_list and 0 <= idx < len(recommended_models_list):
                        selected_model = recommended_models_list[idx]
                    else:
                        selected_model = default_model
                else:
                    selected_model = model_input

                # Validate model exists (check in MODEL_METADATA or HuggingFace format)
                if selected_model not in MODEL_METADATA and '/' not in selected_model:
                    self.console.print(f"[yellow]⚠️  Model '{selected_model}' not found in metadata[/yellow]")
                    # Ask if they want to use it anyway
                    use_anyway = Confirm.ask(
                        f"[yellow]Use '{selected_model}' anyway? (may fail if invalid)[/yellow]",
                        default=False
                    )
                    if not use_anyway:
                        continue  # Ask for selection again

                # Check for duplicates
                if selected_model in selected_models_benchmark:
                    self.console.print(f"[yellow]⚠️  Model '{selected_model}' is already selected. Please choose a different model.[/yellow]")
                    continue

                selected_models_benchmark.append(selected_model)
                self.console.print(f"[green]✓ Added: {selected_model}[/green]")

                # Display full model details after selection
                self._display_model_details(selected_model, MODEL_METADATA)

                # Ask to add more (require at least 2)
                if len(selected_models_benchmark) >= 2:
                    add_more = Confirm.ask(
                        f"\n[cyan]Add another model? (Current: {len(selected_models_benchmark)})[/cyan]",
                        default=False
                    )
                    if not add_more:
                        break
                else:
                    self.console.print(f"[yellow]⚠️  At least 2 models required. Please select one more.[/yellow]")

        # Deduplicate models and track changes
        if train_by_language:
            for lang in models_by_language_benchmark:
                original_count = len(models_by_language_benchmark[lang])
                # Remove duplicates while preserving order
                models_by_language_benchmark[lang] = list(dict.fromkeys(models_by_language_benchmark[lang]))
                deduped_count = len(models_by_language_benchmark[lang])
                if deduped_count < original_count:
                    self.console.print(f"\n[dim]  • {lang}: Removed {original_count - deduped_count} duplicate(s), {deduped_count} unique model(s) remaining[/dim]")
        else:
            original_count = len(selected_models_benchmark)
            selected_models_benchmark = list(dict.fromkeys(selected_models_benchmark))
            deduped_count = len(selected_models_benchmark)
            if deduped_count < original_count:
                self.console.print(f"\n[dim]  • Removed {original_count - deduped_count} duplicate(s), {deduped_count} unique model(s) remaining[/dim]")

        # Summary
        self.console.print("\n[bold green]✓ Model Selection Complete[/bold green]")
        if train_by_language:
            total_models = sum(len(models) for models in models_by_language_benchmark.values())
            for lang, models in sorted(models_by_language_benchmark.items()):
                self.console.print(f"  • {lang}: [cyan]{len(models)} model(s)[/cyan]")
                for m in models:
                    self.console.print(f"    - {m}")
            if total_models < 2:
                self.console.print(f"\n[red]❌ Only {total_models} unique model(s) - benchmark requires at least 2 different models[/red]")
                return None
        else:
            self.console.print(f"  • [cyan]{len(selected_models_benchmark)} unique model(s)[/cyan]")
            for m in selected_models_benchmark:
                self.console.print(f"    - {m}")
            if len(selected_models_benchmark) < 2:
                self.console.print(f"\n[red]❌ Only {len(selected_models_benchmark)} unique model(s) - benchmark requires at least 2 different models[/red]")
                return None

        # ======================== STEP 2: Training Epochs ========================
        # Reinforced learning is enabled by default with standard parameters
        enable_benchmark_rl = True
        rl_f1_threshold = 0.70
        rl_oversample_factor = 2.0
        rl_class_weight_factor = 2.0
        self.console.print("\n[bold cyan]═══════════════════════════════════════════════════════════════[/bold cyan]")
        self.console.print("[bold cyan]           ⏱️  STEP 3: Training Epochs (Benchmark)              [/bold cyan]")
        self.console.print("[bold cyan]═══════════════════════════════════════════════════════════════[/bold cyan]\n")

        self.console.print("[bold]What are Epochs?[/bold]")
        self.console.print("  • [cyan]One epoch[/cyan] = One complete pass through your entire training dataset")
        self.console.print("  • [cyan]More epochs[/cyan] = Model sees and learns from data more times")
        self.console.print("  • [cyan]Typical range[/cyan]: 3-15 epochs for BERT-like models\n")

        self.console.print("[bold]Guidelines:[/bold]")
        self.console.print("  • [green]Small dataset (<1000 samples)[/green]: 10-15 epochs recommended")
        self.console.print("  • [green]Medium dataset (1000-10000)[/green]: 5-10 epochs recommended")
        self.console.print("  • [green]Large dataset (>10000)[/green]: 3-5 epochs recommended\n")

        self.console.print("[bold green]💾 Automatic Best Model Checkpointing:[/bold green]")
        self.console.print("  • [cyan]Don't worry about setting too many epochs![/cyan]")
        self.console.print("  • The [bold]BEST model[/bold] is automatically saved during training")
        self.console.print("  • System monitors [yellow]validation F1 score[/yellow] after each epoch")
        self.console.print("  • Only the checkpoint with [bold green]highest F1[/bold green] is kept")
        self.console.print("  • Early stopping prevents overfitting automatically\n")

        self.console.print("[dim]💡 Example: You set 15 epochs, but best F1 was at epoch 8 → Model from epoch 8 is used[/dim]\n")

        benchmark_epochs = IntPrompt.ask("[bold yellow]Number of epochs[/bold yellow]", default=10)

        # Store RL params
        # CRITICAL: Initialize reinforced_epochs with a default value to ensure global_max_epochs calculation works
        # Default to same as base epochs (user can override manually)
        benchmark_rl_params = {
            'f1_threshold': rl_f1_threshold,
            'oversample_factor': rl_oversample_factor,
            'class_weight_factor': rl_class_weight_factor,
            'reinforced_epochs': benchmark_epochs  # Default: same as base epochs (will be overridden if manually configured)
        }

        # Calculate and display total epochs (always show, even if RL disabled)
        from ..trainers.reinforced_params import get_reinforced_params

        if enable_benchmark_rl:
            self.console.print("\n[bold yellow]⚠️  Reinforced Learning Epoch Calculation[/bold yellow]\n")
            self.console.print("[dim]When F1 < {:.2f}, reinforced learning adds extra epochs.[/dim]".format(rl_f1_threshold))
            self.console.print("[dim]The table below shows the MAXIMUM possible epochs (worst case: F1 = 0.0)[/dim]\n")
        else:
            self.console.print("\n[bold cyan]📊 Total Training Epochs[/bold cyan]\n")
            self.console.print("[dim]Reinforced learning is disabled. All models will train for the same number of epochs.[/dim]\n")

        # Create table showing epoch calculation (always show)
        epoch_table = Table(show_header=True, header_style="bold magenta", border_style="cyan", box=box.ROUNDED)
        epoch_table.add_column("Model", style="yellow", width=40)
        epoch_table.add_column("Base Epochs", style="cyan", justify="center", width=12)
        if enable_benchmark_rl:
            epoch_table.add_column("Max Reinforced", style="red", justify="center", width=15)
            epoch_table.add_column("Max Total", style="green bold", justify="center", width=12)
        else:
            epoch_table.add_column("Total Epochs", style="green bold", justify="center", width=12)

        max_total_epochs = benchmark_epochs

        # Get all models to calculate epochs for
        models_to_calculate = []
        if train_by_language:
            for lang, models in models_by_language_benchmark.items():
                models_to_calculate.extend(models)
        else:
            models_to_calculate = selected_models_benchmark

        for model_id in models_to_calculate:
            model_name = model_id.split('/')[-1] if '/' in model_id else model_id

            if enable_benchmark_rl:
                # Calculate potential reinforced epochs (worst case: F1 = 0.0)
                reinforced_params = get_reinforced_params(
                    model_name=model_name,
                    best_f1_1=0.0,  # Worst case scenario
                    original_lr=5e-5,
                    num_classes=2
                )
                max_reinforced_epochs = reinforced_params.get('n_epochs', 0)
                total_possible = benchmark_epochs + max_reinforced_epochs

                if total_possible > max_total_epochs:
                    max_total_epochs = total_possible

                epoch_table.add_row(
                    model_id,
                    str(benchmark_epochs),
                    str(max_reinforced_epochs),
                    str(total_possible)
                )
            else:
                # No reinforced learning - just show base epochs
                epoch_table.add_row(
                    model_id,
                    str(benchmark_epochs),
                    str(benchmark_epochs)
                )

        self.console.print(epoch_table)
        self.console.print()

        # Ask for confirmation
        if enable_benchmark_rl:
            epochs_confirmed = Confirm.ask(
                f"[bold yellow]Continue with these epoch settings? (Max {max_total_epochs} epochs per model)[/bold yellow]",
                default=True
            )
        else:
            epochs_confirmed = Confirm.ask(
                f"[bold yellow]Continue with {benchmark_epochs} epoch(s) per model?[/bold yellow]",
                default=True
            )

        # Store manual reinforced epochs if configured
        manual_reinforced_epochs = None

        if not epochs_confirmed:
            # Ask what the user wants to configure
            self.console.print("\n[yellow]What would you like to configure?[/yellow]")

            # ──────────────────────────────────────────────────────────────
            # Step 1: Base Epochs Configuration (optional)
            # ──────────────────────────────────────────────────────────────
            modify_base = Confirm.ask(
                "[bold yellow]Modify base epochs?[/bold yellow]",
                default=True
            )

            if modify_base:
                benchmark_epochs = IntPrompt.ask(
                    "[bold yellow]Base epochs for benchmark[/bold yellow]",
                    default=benchmark_epochs
                )
                self.console.print(f"[green]✓ Base epochs set to: {benchmark_epochs}[/green]\n")
            else:
                self.console.print(f"[green]✓ Keeping base epochs at: {benchmark_epochs}[/green]\n")

            # ──────────────────────────────────────────────────────────────
            # Step 2: Reinforced Learning Epochs Configuration (independent)
            # ──────────────────────────────────────────────────────────────
            # NOTE: This section executes REGARDLESS of whether base epochs
            # were modified above. Both configurations are independent.
            if enable_benchmark_rl:
                configure_rl_epochs = Confirm.ask(
                    "[bold yellow]Configure reinforced learning epochs manually?[/bold yellow]\n"
                    "[dim](Default: auto-calculated based on model performance)[/dim]",
                    default=False
                )

                if configure_rl_epochs:
                    self.console.print("\n[bold cyan]ℹ️  Reinforced Learning Epochs:[/bold cyan]")
                    self.console.print("[dim]These epochs will be used for ALL models when F1 < {:.2f}[/dim]".format(rl_f1_threshold))
                    self.console.print("[dim]Auto-calculation typically uses 8-20 epochs based on model type[/dim]\n")

                    manual_reinforced_epochs = IntPrompt.ask(
                        "[bold yellow]Reinforced epochs[/bold yellow]",
                        default=10
                    )

                    self.console.print(f"[green]✓ Manual reinforced epochs set to: {manual_reinforced_epochs}[/green]\n")
                else:
                    self.console.print("[green]✓ Reinforced learning epochs will be auto-calculated[/green]\n")

        # Update RL params with manual reinforced epochs if configured
        if manual_reinforced_epochs is not None:
            benchmark_rl_params['reinforced_epochs'] = manual_reinforced_epochs

        # ======================== STEP 4: Category Selection ========================
        self.console.print("\n[bold]STEP 4: Select Categories for Benchmark[/bold]\n")
        self.console.print("[dim]Analyzing training data structure...[/dim]\n")

        import json

        # First, check bundle metadata for categories (for multi-class approach)
        metadata_categories = []
        if hasattr(bundle, 'metadata') and bundle.metadata:
            metadata_categories = bundle.metadata.get('categories', [])
            if metadata_categories:
                self.console.print(f"[cyan]✓ Found {len(metadata_categories)} categories from training configuration[/cyan]")
                for cat in metadata_categories[:10]:  # Show first 10
                    self.console.print(f"  • {cat}")
                if len(metadata_categories) > 10:
                    self.console.print(f"  ... and {len(metadata_categories) - 10} more")
                self.console.print()

        # If no metadata categories, analyze the actual data
        if not metadata_categories:
            self.console.print("[dim]No categories in metadata, analyzing annotations...[/dim]\n")

            unique_categories = set()
            for idx, row in original_dataframe.iterrows():
                annotation = row[annotation_column]

                # Parse if string
                if isinstance(annotation, str):
                    try:
                        annotation = json.loads(annotation)
                    except:
                        continue

                if isinstance(annotation, dict):
                    unique_categories.update(annotation.keys())

            metadata_categories = list(unique_categories)

            if metadata_categories:
                self.console.print(f"[cyan]✓ Found {len(metadata_categories)} unique categor{'y' if len(metadata_categories) == 1 else 'ies'} in annotations[/cyan]\n")

        num_categories_in_data = len(metadata_categories)

        if num_categories_in_data == 0:
            self.console.print("[red]❌ No categories found in training data[/red]")
            self.console.print("[yellow]This may indicate an issue with the data conversion.[/yellow]")
            self.console.print("[dim]Benchmark requires category information for analysis.[/dim]\n")
            return None

        selected_benchmark_categories = []

        if num_categories_in_data == 1:
            # Only one category: Use the full dataset, no category selection needed
            self.console.print(f"[green]Single category detected: {metadata_categories[0]}[/green]")
            self.console.print(f"[dim]Benchmarking on full dataset (no filtering needed)[/dim]\n")

            # No category filtering needed
            selected_benchmark_categories = None  # Signal to use full dataset

        else:
            # Multiple categories: Analyze and select representative ones
            self.console.print(f"[yellow]Multiple categories detected ({num_categories_in_data} total)[/yellow]")
            self.console.print("[dim]Performing class imbalance analysis to suggest representative categories...[/dim]\n")

            # Analyze categories
            # CRITICAL: Only analyze categories that were selected for training
            imbalance_analysis = analyze_categories_imbalance(
                data=original_dataframe,
                annotation_column=annotation_column,
                filter_categories=metadata_categories  # Only analyze training-selected categories
            )

            if not imbalance_analysis:
                self.console.print("[red]❌ No categories found in annotations[/red]")
                return None

            # Select suggested categories
            suggested_categories = select_benchmark_categories(imbalance_analysis, num_categories=3)

            # Display analysis with explanation
            self.console.print("[bold cyan]📊 Class Imbalance Analysis[/bold cyan]\n")

            self.console.print("[bold]🎯 Why This Analysis?[/bold]")
            self.console.print("[dim]To choose the best model, we need to test how each model performs on:[/dim]")
            self.console.print("[dim]  • [cyan]Balanced categories[/cyan] - Equal class distribution (easier, baseline performance)[/dim]")
            self.console.print("[dim]  • [yellow]Imbalanced categories[/yellow] - Skewed class distribution (harder, real-world scenario)[/dim]")
            self.console.print("[dim]This reveals which model handles both easy and challenging data best.[/dim]\n")

            self.console.print("[bold]📋 Category Selection Strategy:[/bold]")
            self.console.print("[dim]The system automatically selects a mix of:[/dim]")
            self.console.print("[dim]  • Categories with different imbalance ratios (2:1, 5:1, 10:1+)[/dim]")
            self.console.print("[dim]  • Different sample sizes (small vs large datasets)[/dim]")
            self.console.print("[dim]  • Different numbers of classes (binary vs multi-class)[/dim]")
            self.console.print("[dim]This comprehensive test ensures you pick the model that performs well across all scenarios.[/dim]\n")

            categories_table = Table(show_header=True, header_style="bold magenta", border_style="cyan", box=box.ROUNDED)
            categories_table.add_column("Category", style="yellow", width=30)
            categories_table.add_column("Profile", style="cyan", width=15)
            categories_table.add_column("Metrics", style="white", width=70)

            for profile, profile_cats in suggested_categories.items():
                for cat in profile_cats:
                    if cat in imbalance_analysis:
                        metrics = imbalance_analysis[cat]
                        categories_table.add_row(
                            cat,
                            profile.capitalize(),
                            format_imbalance_summary(metrics)
                        )

            self.console.print(categories_table)

            # Explain metrics
            self.console.print("\n[bold]📏 Understanding the Metrics:[/bold]")
            self.console.print("[dim]  • [cyan]Ratio[/cyan] - Largest class / Smallest class (e.g., 5.3:1 means majority class is 5.3× larger)[/dim]")
            self.console.print("[dim]  • [cyan]Gini[/cyan] - Inequality coefficient (0=perfect balance, 1=extreme imbalance)[/dim]")
            self.console.print("[dim]  • [green]Balanced[/green]: Ratio < 2:1, Gini < 0.2 | [yellow]Moderate[/yellow]: Ratio 2-5:1, Gini 0.2-0.4 | [red]Imbalanced[/red]: Ratio > 5:1, Gini > 0.4[/dim]\n")

            # Collect all suggested
            all_suggested = []
            for cats in suggested_categories.values():
                all_suggested.extend(cats)

            # User choice
            self.console.print("[bold]Select categories for benchmark:[/bold]")
            self.console.print("  • Press [cyan]ENTER[/cyan] to use all suggested categories")
            self.console.print("  • Or enter [cyan]category names[/cyan] (comma-separated)")
            self.console.print("  • Or enter [cyan]'all'[/cyan] to see all available categories\n")

            choice = Prompt.ask("Categories", default="suggested")

            if choice in ["suggested", ""]:
                selected_benchmark_categories = all_suggested
            elif choice == "all":
                # Show all categories
                self.console.print("\n[bold cyan]All Available Categories:[/bold cyan]\n")

                all_cats_table = Table(show_header=True, header_style="bold magenta", border_style="green", box=box.ROUNDED)
                all_cats_table.add_column("#", style="yellow", width=3)
                all_cats_table.add_column("Category", style="cyan", width=30)
                all_cats_table.add_column("Classes", style="green", width=8)
                all_cats_table.add_column("Samples", style="blue", width=8)
                all_cats_table.add_column("Imbalance", style="white", width=15)

                sorted_cats = sorted(imbalance_analysis.items(), key=lambda x: x[1]['total_samples'], reverse=True)

                for idx, (cat, metrics) in enumerate(sorted_cats, 1):
                    ratio = metrics.get('imbalance_ratio', 1.0)
                    imb_level = "Balanced" if ratio < 2 else "Moderate" if ratio < 5 else "Imbalanced"

                    all_cats_table.add_row(
                        str(idx),
                        cat,
                        str(metrics.get('num_classes', 0)),
                        str(metrics.get('total_samples', 0)),
                        f"{imb_level} ({ratio:.1f}:1)"
                    )

                self.console.print(all_cats_table)

                self.console.print("\n[yellow]Enter category names or numbers (comma-separated):[/yellow]")
                selection = Prompt.ask("Selection")

                # Parse selection
                selected_benchmark_categories = []
                for item in selection.split(','):
                    item = item.strip()
                    if item.isdigit():
                        idx = int(item) - 1
                        if 0 <= idx < len(sorted_cats):
                            selected_benchmark_categories.append(sorted_cats[idx][0])
                    else:
                        if item in imbalance_analysis:
                            selected_benchmark_categories.append(item)
            else:
                selected_benchmark_categories = [c.strip() for c in choice.split(',')]

            if not selected_benchmark_categories:
                self.console.print("[red]❌ No categories selected[/red]")
                return None

            self.console.print(f"\n[green]✓ Selected {len(selected_benchmark_categories)} categories:[/green]")
            for cat in selected_benchmark_categories:
                if cat in imbalance_analysis:
                    metrics = imbalance_analysis[cat]
                    self.console.print(f"  • {cat} ({metrics['total_samples']} samples, {metrics['num_classes']} classes)")

        # ======================== STEP 5: Execute Benchmark ========================
        self.console.print("\n[bold]STEP 5: Running Benchmark[/bold]\n")

        # Collect all models to test (with language mapping if train_by_language)
        all_models_to_test = []
        model_to_language_map = {}  # Track which language each model should use

        if train_by_language:
            for lang, models in models_by_language_benchmark.items():
                for model in models:
                    all_models_to_test.append(model)
                    model_to_language_map[model] = lang  # Remember this model is for this language
        else:
            all_models_to_test = selected_models_benchmark

        self.console.print(f"  • Models to test: [cyan]{len(all_models_to_test)}[/cyan]")
        if selected_benchmark_categories is not None:
            self.console.print(f"  • Categories: [cyan]{len(selected_benchmark_categories)}[/cyan]")
        else:
            self.console.print(f"  • Dataset: [cyan]Full training dataset[/cyan]")

        # Display epochs with reinforced learning info if enabled
        if enable_benchmark_rl:
            reinforced_epochs = benchmark_rl_params.get('reinforced_epochs', None)
            if reinforced_epochs is not None:
                # Manual reinforced epochs configured
                max_epochs = benchmark_epochs + reinforced_epochs
                self.console.print(f"  • Epochs per model: [cyan]{benchmark_epochs}[/cyan] (up to [yellow]{max_epochs}[/yellow] with reinforced learning)")
            else:
                # Auto-calculated reinforced epochs (typically 8-20)
                self.console.print(f"  • Epochs per model: [cyan]{benchmark_epochs}[/cyan] (up to [yellow]{benchmark_epochs}+auto[/yellow] with reinforced learning)")
            self.console.print(f"  • Reinforced learning: [cyan]Enabled[/cyan] (F1 < {benchmark_rl_params.get('f1_threshold', 0.70):.2f})")

            # Estimate time considering potential reinforced learning
            # Conservative estimate: assume some models will trigger RL
            estimated_avg_epochs = benchmark_epochs + (reinforced_epochs // 2 if reinforced_epochs else 5)
            estimated_minutes = len(all_models_to_test) * estimated_avg_epochs // 2
        else:
            self.console.print(f"  • Epochs per model: [cyan]{benchmark_epochs}[/cyan]")
            estimated_minutes = len(all_models_to_test) * benchmark_epochs // 2

        self.console.print(f"  • Estimated time: [yellow]~{estimated_minutes} minutes[/yellow]\n")

        proceed = Confirm.ask("[bold yellow]Proceed with benchmark?[/bold yellow]", default=True)
        if not proceed:
            return None

        # Prepare benchmark dataset
        with tempfile.TemporaryDirectory() as tmpdir:
            # For single-label, use the bundle's primary file directly
            # For multi-label with category filtering, create filtered dataset
            if selected_benchmark_categories is None:
                # Single-label: Use existing training file
                benchmark_file = bundle.primary_file
                self.console.print(f"[green]✓ Using full training dataset: {bundle.primary_file.name}[/green]\n")
            else:
                # Multi-label: Create filtered dataset
                self.console.print("\n[dim]Creating filtered benchmark dataset...[/dim]")
                benchmark_file = Path(tmpdir) / "benchmark_data.jsonl"

                # Create filtered dataset
                import json
                benchmark_rows = []

                for idx, row in original_dataframe.iterrows():
                    annotation = row[annotation_column]

                    # Parse if string
                    if isinstance(annotation, str):
                        try:
                            annotation = json.loads(annotation)
                        except:
                            continue

                    if not isinstance(annotation, dict):
                        continue

                    # Filter to selected categories
                    filtered_annotation = {
                        k: v for k, v in annotation.items()
                        if k in selected_benchmark_categories
                    }

                    if not filtered_annotation:
                        continue

                    # Transform to multi-label format: list of "key_value" strings
                    # E.g., {'sentiment': 'positive', 'theme': 'politics'} → ['sentiment_positive', 'theme_politics']
                    # CRITICAL: Exclude 'null' string values
                    label_list = []
                    for key, value in filtered_annotation.items():
                        if isinstance(value, str) and value and value != 'null':
                            # Combine key and value into single label string
                            label_list.append(f"{key}_{value}")
                        elif isinstance(value, list):
                            # Handle list values (shouldn't happen in this flow, but be defensive)
                            for v in value:
                                if isinstance(v, str) and v and v != 'null':
                                    label_list.append(f"{key}_{v}")

                    if not label_list:
                        continue

                    # Create row
                    benchmark_row = {
                        'text': row[bundle.text_column],
                        'labels': label_list  # Always a list of label strings
                    }

                    # Add language if available
                    for lang_col in ['language', 'lang']:
                        if lang_col in row.index:
                            benchmark_row['lang'] = row[lang_col]
                            break

                    benchmark_rows.append(benchmark_row)

                # Save as JSONL
                with open(benchmark_file, 'w', encoding='utf-8') as f:
                    for row in benchmark_rows:
                        f.write(json.dumps(row, ensure_ascii=False) + '\n')

                self.console.print(f"[green]✓ Benchmark dataset created: {len(benchmark_rows)} samples[/green]\n")

            # Run benchmark for each model
            benchmark_results = {}

            # CRITICAL: Reuse the session ID from the training session (self.current_session_id)
            # This ensures benchmark and full training use THE SAME session folder.
            # The session_id format is: {user_name}_{YYYYMMDD_HHMMSS} (created at line 6558)
            if hasattr(self, 'current_session_id') and self.current_session_id:
                benchmark_session_id = self.current_session_id
                self.logger.info(f"✓ Benchmark reusing existing session: {benchmark_session_id}")
            else:
                # Fallback: create session_id if not yet initialized (should not happen in Training Arena)
                import datetime
                benchmark_session_id = datetime.datetime.now().strftime("training_session_%Y%m%d_%H%M%S")
                self.current_session_id = benchmark_session_id
                self.logger.warning(f"⚠️  Created new session_id for benchmark (expected to reuse existing): {benchmark_session_id}")

            # CRITICAL: Display session information to user
            self.logger.info("="*80)
            self.logger.info("SESSION MANAGEMENT - BENCHMARK")
            self.logger.info(f"  benchmark_session_id: {benchmark_session_id}")
            self.logger.info(f"  Models will be saved to: models/{benchmark_session_id}/benchmark/")
            self.logger.info(f"  Logs will be saved to: logs/training_arena/{benchmark_session_id}/training_metrics/benchmark/")
            self.logger.info("="*80)
            self.console.print(f"\n[cyan]📂 Session ID:[/cyan] [bold]{benchmark_session_id}[/bold]")
            self.console.print(f"[dim]All benchmark models will be saved to: models/{benchmark_session_id}/benchmark/[/dim]\n")

            # ============================================================
            # CRITICAL: Save initial benchmark metadata for session tracking
            # This enables session persistence and resume capability even if
            # training is interrupted or user chooses to exit benchmark early
            # ============================================================
            try:
                self.logger.info("💾 Saving initial benchmark metadata for session tracking...")

                # Build comprehensive benchmark configuration for metadata
                benchmark_model_config = {
                    'training_mode': 'benchmark',
                    'benchmark_enabled': True,
                    'selected_models': selected_models_benchmark if not train_by_language else list(all_models_to_test),
                    'models_by_language': models_by_language_benchmark if train_by_language else {},
                    'train_by_language': train_by_language,
                    'benchmark_categories': selected_benchmark_categories,
                    'benchmark_epochs': benchmark_epochs,
                    'reinforced_learning_enabled': enable_benchmark_rl,
                    'rl_f1_threshold': benchmark_rl_params.get('f1_threshold', 0.70),
                    'rl_oversample_factor': benchmark_rl_params.get('oversample_factor', 2.0),
                    'rl_class_weight_factor': benchmark_rl_params.get('class_weight_factor', 2.0),
                    'reinforced_epochs': benchmark_rl_params.get('reinforced_epochs'),
                    'epochs': benchmark_epochs,
                    'batch_size': 16,
                    'learning_rate': 2e-5
                }

                # Save metadata immediately (before any training starts)
                initial_metadata_path = self._save_training_metadata(
                    bundle=bundle,
                    mode='benchmark',
                    model_config=benchmark_model_config,
                    execution_status={
                        'status': 'benchmark_starting',
                        'started_at': datetime.now().isoformat(),
                        'completed_at': None,
                        'models_trained': [],
                        'models_to_test': list(all_models_to_test),
                        'best_model': None,
                        'best_f1': None,
                        'benchmark_phase': 'initialization'
                    },
                    session_id=benchmark_session_id,
                    training_context={
                        'benchmark_mode': True,
                        'user_choices': {
                            'enable_benchmark': True,
                            'num_models_selected': len(all_models_to_test),
                            'selected_categories': selected_benchmark_categories
                        }
                    }
                )
                self.console.print(f"[dim]💾 Session metadata saved: {initial_metadata_path.name}[/dim]\n")
                self.logger.info(f"✓ Initial benchmark metadata saved: {initial_metadata_path}")

            except Exception as e:
                self.logger.error(f"Failed to save initial benchmark metadata: {e}")
                self.console.print(f"[yellow]⚠️  Warning: Could not save session metadata: {e}[/yellow]\n")
                # Continue anyway - metadata saving should not block training

            # Initialize global progress tracking for benchmark
            import time
            global_start_time = time.time()
            global_total_models = len(all_models_to_test)

            # Calculate total epochs accounting for all categories
            # Each model must be trained on each category, so total = models × categories × epochs
            num_categories = 1 if selected_benchmark_categories is None else len(selected_benchmark_categories)
            global_total_epochs = global_total_models * num_categories * benchmark_epochs

            # Calculate maximum possible epochs (if all models trigger reinforced learning)
            if enable_benchmark_rl and benchmark_rl_params.get('reinforced_epochs') is not None:
                global_max_epochs = global_total_models * num_categories * (benchmark_epochs + benchmark_rl_params['reinforced_epochs'])
            else:
                global_max_epochs = global_total_epochs

            global_completed_epochs = 0

            # ============================================================
            # CRITICAL: Validate and filter insufficient labels BEFORE training
            # ============================================================
            try:
                benchmark_file, was_filtered = self._validate_and_filter_insufficient_labels(
                    input_file=str(benchmark_file),
                    strategy=bundle.strategy,
                    min_samples=2,
                    auto_remove=False,  # Ask user for confirmation
                    train_by_language=train_by_language  # CRITICAL: Language-aware validation for multilingual
                )
                if was_filtered:
                    self.console.print(f"[green]✓ Using filtered benchmark dataset[/green]\n")
            except ValueError as e:
                # User cancelled or validation failed
                self.console.print(f"[red]{e}[/red]")
                return None
            except Exception as e:
                self.logger.warning(f"Label validation failed: {e}")
                # Continue with original file if validation fails
                pass

            # Run benchmark for each model
            for idx, model_id in enumerate(all_models_to_test, 1):
                self.console.print(f"\n[bold yellow]{'═' * 70}[/bold yellow]")
                self.console.print(f"[bold yellow]🔬 Testing Model {idx}/{len(all_models_to_test)}: {model_id}[/bold yellow]")
                self.console.print(f"[bold yellow]{'═' * 70}[/bold yellow]\n")

                try:
                    # Create temp output dir
                    model_output_dir = Path(tmpdir) / f"model_{idx}"
                    model_output_dir.mkdir(exist_ok=True)

                    # Train configuration
                    config = TrainingConfig()
                    config.num_epochs = benchmark_epochs
                    config.batch_size = 16
                    config.early_stopping_patience = max(2, benchmark_epochs // 5)
                    config.output_dir = str(model_output_dir)

                    trainer = ModelTrainer(config=config)

                    # Create progress callback to track completed epochs
                    def progress_callback(**metrics):
                        """Callback to increment global completed epochs counter"""
                        nonlocal global_completed_epochs
                        global_completed_epochs += 1

                    # Prepare training params
                    train_params = {
                        'input_file': str(benchmark_file),
                        'model_name': model_id,
                        'num_epochs': benchmark_epochs,
                        'text_column': 'text',
                        'label_column': 'labels',
                        'training_strategy': bundle.strategy,
                        'output_dir': str(model_output_dir),
                        'is_benchmark': True,  # Flag to enable benchmark mode log structure
                        'session_id': benchmark_session_id,  # Unified session ID for all models in benchmark
                        'progress_callback': progress_callback,  # Add callback for epoch tracking
                        # Global progress tracking parameters
                        'global_total_models': global_total_models,
                        'global_current_model': idx,
                        'global_total_epochs': global_total_epochs,
                        'global_max_epochs': global_max_epochs,
                        'global_completed_epochs': global_completed_epochs,
                        'global_start_time': global_start_time,
                        # Pass training_approach from bundle metadata (one-vs-all vs multi-class)
                        'training_approach': bundle.metadata.get('training_approach') if hasattr(bundle, 'metadata') else None
                    }

                    # Add language filtering for per-language models
                    if model_id in model_to_language_map:
                        # This is a language-specific model - only train on its language
                        model_lang = model_to_language_map[model_id]
                        train_params['confirmed_languages'] = [model_lang]
                        train_params['filter_by_language'] = model_lang  # Filter data to only this language
                    elif hasattr(bundle, 'metadata') and bundle.metadata.get('confirmed_languages'):
                        # Multilingual model - use all languages
                        train_params['confirmed_languages'] = bundle.metadata['confirmed_languages']

                    # Add reinforced learning params if enabled
                    if enable_benchmark_rl:
                        train_params['reinforced_learning'] = True
                        train_params['rl_f1_threshold'] = benchmark_rl_params.get('f1_threshold', 0.70)
                        train_params['rl_oversample_factor'] = benchmark_rl_params.get('oversample_factor', 2.0)
                        train_params['rl_class_weight_factor'] = benchmark_rl_params.get('class_weight_factor', 2.0)
                        # Pass manual reinforced epochs if configured
                        if benchmark_rl_params.get('reinforced_epochs') is not None:
                            train_params['reinforced_epochs'] = benchmark_rl_params['reinforced_epochs']

                    # Train
                    result = trainer.train(train_params)

                    # NOTE: global_completed_epochs is tracked internally by the trainer via display.global_completed_epochs
                    # Each epoch increments the counter automatically, accounting for all categories and reinforced learning
                    # No manual increment needed here - the next model will receive the updated count via the display object

                    benchmark_results[model_id] = result

                    # Extract metrics with backward compatibility for different key names
                    f1_score = result.get('f1_macro', result.get('f1', result.get('best_f1_macro', 0)))
                    accuracy = result.get('accuracy', result.get('best_accuracy', 0))

                    self.console.print(f"\n[green]✓ Training Complete[/green]")
                    self.console.print(f"  • Overall F1-Score: [bold green]{f1_score:.3f}[/bold green]")
                    self.console.print(f"  • Overall Accuracy: [bold green]{accuracy:.3f}[/bold green]")
                    if 'training_time' in result:
                        self.console.print(f"  • Time: [cyan]{result['training_time']:.1f}s[/cyan]")

                    # Display per-category scores if available (multi-label benchmark)
                    if 'trained_models' in result and result['trained_models']:
                        self.console.print(f"\n  [dim]Per-Category Scores:[/dim]")
                        # Get category details from model trainer
                        # trained_models is a dict of {model_name: model_path} but we need metrics
                        # This will be enhanced in the results display section

                except Exception as e:
                    self.console.print(f"\n[red]❌ Error during training: {str(e)}[/red]")
                    # Add placeholder result
                    benchmark_results[model_id] = {
                        'best_f1_macro': 0.0,
                        'accuracy': 0.0,
                        'training_time': 0,
                        'error': str(e)
                    }

        # ======================== STEP 6: Display Results ========================
        self.console.print("\n[bold cyan]═══════════════════════════════════════════════════════════════[/bold cyan]")
        self.console.print("[bold cyan]         📊 STEP 6: BENCHMARK RESULTS                           [/bold cyan]")
        self.console.print("[bold cyan]═══════════════════════════════════════════════════════════════[/bold cyan]\n")

        # Display ranking methodology explanation
        self.console.print("[bold yellow]📋 How Models Are Ranked:[/bold yellow]")
        self.console.print("\n[bold]Sophisticated Combined Metric System[/bold] (mirrors epoch selection):\n")
        self.console.print("  [cyan]1. Combined Score[/cyan] (Primary Criterion)")
        self.console.print("     • Binary Classification: [green]70% × F1_minority + 30% × F1_macro[/green]")
        self.console.print("       → Prioritizes minority class detection (e.g., detecting defects, fraud)")
        self.console.print("     • Multi-Class: [green]F1_macro[/green] (balanced across all classes)\n")

        self.console.print("  [cyan]2. Language Balance Penalty[/cyan] (for multilingual data)")
        self.console.print("     • Measures performance consistency across languages")
        self.console.print("     • Penalty = [yellow]min(CV × 0.2, 0.2)[/yellow] where CV = coefficient of variation")
        self.console.print("     • Example: Model with F1=90% (EN) + F1=30% (FR) → [red]penalized[/red]")
        self.console.print("     • Example: Model with F1=70% (EN) + F1=65% (FR) → [green]minimal penalty[/green]\n")

        self.console.print("  [cyan]3. Tiebreakers[/cyan]")
        self.console.print("     • [green]Accuracy[/green] (when combined scores equal)")
        self.console.print("     • [green]Training Time[/green] (faster is better when score + accuracy equal)\n")

        self.console.print("[dim]💡 This ensures models are ranked the same way best epochs are selected during training[/dim]\n")

        # Check if we have multi-category results
        has_category_details = any('category_metrics' in result and result['category_metrics']
                                   for result in benchmark_results.values())

        if has_category_details and selected_benchmark_categories:
            # Display detailed per-category results
            self.console.print("[bold]Overall Rankings:[/bold]\n")

            # Create comparison DataFrame with sophisticated ranking
            comparison_df = compare_model_results(benchmark_results, use_sophisticated_ranking=True)

            # Overall results table
            results_table = Table(show_header=True, header_style="bold magenta", border_style="green", box=box.ROUNDED, title="[bold]Ranked Results[/bold]")
            results_table.add_column("Rank", style="yellow", width=6)
            results_table.add_column("Model", style="cyan", width=35)
            results_table.add_column("Combined\nScore", style="bold green", width=10, justify="right")
            results_table.add_column("Avg F1", style="green", width=10, justify="right")
            results_table.add_column("Avg Acc", style="green", width=10, justify="right")
            results_table.add_column("Time (s)", style="blue", width=10, justify="right")

            for _, row in comparison_df.iterrows():
                # Add emoji for top 3
                if row['rank'] == 1:
                    rank_str = "🥇 1"
                elif row['rank'] == 2:
                    rank_str = "🥈 2"
                elif row['rank'] == 3:
                    rank_str = "🥉 3"
                else:
                    rank_str = f"   {row['rank']}"

                # Highlight combined score if different from f1_macro
                combined_score = row.get('combined_score', row['f1_macro'])
                if abs(combined_score - row['f1_macro']) > 0.001:
                    combined_str = f"[bold]{combined_score:.3f}[/bold]"
                else:
                    combined_str = f"{combined_score:.3f}"

                results_table.add_row(
                    rank_str,
                    row['model'],
                    combined_str,
                    f"{row['f1_macro']:.3f}",
                    f"{row['accuracy']:.3f}",
                    f"{row['training_time']:.1f}"
                )

            self.console.print(results_table)

            # Per-category breakdown
            self.console.print(f"\n[bold]Performance by Category:[/bold]\n")

            for category in selected_benchmark_categories:
                self.console.print(f"[bold cyan]Category: {category}[/bold cyan]")

                cat_table = Table(show_header=True, header_style="bold yellow", border_style="blue", box=box.SIMPLE)
                cat_table.add_column("Model", style="cyan", width=35)
                cat_table.add_column("F1-Score", style="green", width=12)
                cat_table.add_column("Accuracy", style="green", width=12)
                cat_table.add_column("Precision", style="blue", width=12)
                cat_table.add_column("Recall", style="blue", width=12)

                # Collect scores for this category across all models
                category_scores = []
                for model_id, result in benchmark_results.items():
                    if 'category_metrics' in result:
                        # Find the model that corresponds to this category
                        for model_name, metrics in result['category_metrics'].items():
                            # Check if this model is for the current category
                            # Model names typically include category: "sentiment_simple_EN" or similar
                            if category.lower() in model_name.lower():
                                category_scores.append({
                                    'model': model_id,
                                    'f1': metrics.get('f1_macro', 0),
                                    'accuracy': metrics.get('accuracy', 0),
                                    'precision': metrics.get('precision', 0),
                                    'recall': metrics.get('recall', 0)
                                })
                                break

                # Sort by F1 score
                category_scores.sort(key=lambda x: x['f1'], reverse=True)

                # Display
                for score_data in category_scores:
                    cat_table.add_row(
                        score_data['model'],
                        f"{score_data['f1']:.3f}",
                        f"{score_data['accuracy']:.3f}",
                        f"{score_data['precision']:.3f}",
                        f"{score_data['recall']:.3f}"
                    )

                self.console.print(cat_table)
                self.console.print()  # Empty line between categories

        else:
            # Simple display for single-category or no details available
            # Create comparison DataFrame with sophisticated ranking
            comparison_df = compare_model_results(benchmark_results, use_sophisticated_ranking=True)

            # Display results with combined score
            results_table = Table(show_header=True, header_style="bold magenta", border_style="green", box=box.ROUNDED, title="[bold]Ranked Results[/bold]")
            results_table.add_column("Rank", style="yellow", width=6)
            results_table.add_column("Model", style="cyan", width=45)
            results_table.add_column("Combined\nScore", style="bold green", width=10, justify="right")
            results_table.add_column("F1-Macro", style="green", width=10, justify="right")
            results_table.add_column("Accuracy", style="green", width=10, justify="right")
            results_table.add_column("Time (s)", style="blue", width=10, justify="right")

            for _, row in comparison_df.iterrows():
                # Add emoji for top 3
                if row['rank'] == 1:
                    rank_str = "🥇 1"
                elif row['rank'] == 2:
                    rank_str = "🥈 2"
                elif row['rank'] == 3:
                    rank_str = "🥉 3"
                else:
                    rank_str = f"   {row['rank']}"

                # Highlight combined score if different from f1_macro
                combined_score = row.get('combined_score', row['f1_macro'])
                if abs(combined_score - row['f1_macro']) > 0.001:
                    # Different → show in bold
                    combined_str = f"[bold]{combined_score:.3f}[/bold]"
                else:
                    combined_str = f"{combined_score:.3f}"

                results_table.add_row(
                    rank_str,
                    row['model'],
                    combined_str,
                    f"{row['f1_macro']:.3f}",
                    f"{row['accuracy']:.3f}",
                    f"{row['training_time']:.1f}"
                )

            self.console.print(results_table)

            # Display ranking explanations for top 3 models
            self.console.print("\n[bold cyan]📊 Top 3 Models - Ranking Details:[/bold cyan]\n")
            for _, row in comparison_df.head(3).iterrows():
                emoji = "🥇" if row['rank'] == 1 else "🥈" if row['rank'] == 2 else "🥉"
                self.console.print(f"{emoji} [bold]{row['model']}[/bold]")
                if 'ranking_explanation' in row and row['ranking_explanation']:
                    self.console.print(f"   → {row['ranking_explanation']}")

                # Show class-specific F1 if binary classification
                if 'f1_class_1' in row and row['f1_class_1'] > 0:
                    self.console.print(f"   → F1_class_0: {row['f1_class_0']:.3f} | F1_class_1: {row['f1_class_1']:.3f}")

                # Show language penalty if applicable
                if 'language_balance_penalty' in row and row['language_balance_penalty'] > 0:
                    self.console.print(f"   → Language imbalance penalty: [yellow]-{row['language_balance_penalty']:.1%}[/yellow]")

                self.console.print()

        # ======================== Consolidate Session CSVs ========================
        # Create consolidated CSV files at session root
        try:
            from llm_tool.utils.benchmark_utils import consolidate_session_csvs

            # Session directory is in logs/training_arena/{session_id}/training_metrics
            session_dir = Path("logs/training_arena") / benchmark_session_id / "training_metrics"

            if session_dir.exists():
                self.console.print("\n[bold cyan]📊 Consolidating session metrics...[/bold cyan]")
                consolidated_files = consolidate_session_csvs(session_dir, benchmark_session_id)

                if consolidated_files:
                    self.console.print("[green]✓ Created consolidated CSV files:[/green]")
                    if 'training' in consolidated_files:
                        self.console.print(f"  • Training metrics: {consolidated_files['training'].name}")
                    if 'best' in consolidated_files:
                        self.console.print(f"  • Best models: {consolidated_files['best'].name}")
        except Exception as e:
            self.console.print(f"[yellow]⚠ Warning: Could not consolidate CSVs: {e}[/yellow]")

        # ======================== STEP 7: Final Choice ========================
        self.console.print("\n[bold cyan]═══════════════════════════════════════════════════════════════[/bold cyan]")
        self.console.print("[bold cyan]         🎯 STEP 7: Final Model Selection                       [/bold cyan]")
        self.console.print("[bold cyan]═══════════════════════════════════════════════════════════════[/bold cyan]\n")

        self.console.print("[bold]Based on benchmark results, you can:[/bold]")
        self.console.print("  [cyan]1.[/cyan] [bold]Use top-ranked model(s)[/bold] (recommended)")
        self.console.print("  [cyan]2.[/cyan] Manually select model(s)")
        self.console.print("  [cyan]3.[/cyan] Stop here (benchmark only, no full training)\n")

        choice = Prompt.ask(
            "[bold yellow]What would you like to do?[/bold yellow]",
            choices=["1", "2", "3", "top", "manual", "stop"],
            default="1"
        )

        if choice in ["3", "stop"]:
            self.console.print("\n[green]✓ Benchmark complete. Exiting without full training.[/green]")

            # ============================================================
            # CRITICAL: Update metadata with final benchmark results
            # This ensures the benchmark-only session is fully tracked
            # ============================================================
            try:
                self.logger.info("💾 Updating benchmark metadata with final results...")

                # Extract best model from results
                best_model = comparison_df.iloc[0]['model'] if not comparison_df.empty else None
                best_f1 = comparison_df.iloc[0]['f1_macro'] if not comparison_df.empty else None

                # Build final benchmark model config
                final_benchmark_config = {
                    'training_mode': 'benchmark',
                    'benchmark_enabled': True,
                    'selected_models': selected_models_benchmark if not train_by_language else list(all_models_to_test),
                    'models_by_language': models_by_language_benchmark if train_by_language else {},
                    'train_by_language': train_by_language,
                    'benchmark_categories': selected_benchmark_categories,
                    'benchmark_epochs': benchmark_epochs,
                    'reinforced_learning_enabled': enable_benchmark_rl,
                    'epochs': benchmark_epochs,
                    'batch_size': 16,
                    'learning_rate': 2e-5,
                    'actual_models_trained': list(benchmark_results.keys()),
                    'best_model_from_benchmark': best_model,
                    'benchmark_rankings': comparison_df.to_dict('records') if not comparison_df.empty else []
                }

                # Save complete metadata
                final_metadata_path = self._save_training_metadata(
                    bundle=bundle,
                    mode='benchmark',
                    model_config=final_benchmark_config,
                    execution_status={
                        'status': 'benchmark_completed_no_training',
                        'started_at': datetime.now().isoformat(),
                        'completed_at': datetime.now().isoformat(),
                        'models_trained': list(benchmark_results.keys()),
                        'best_model': best_model,
                        'best_f1': best_f1,
                        'benchmark_phase': 'completed',
                        'user_choice': 'stop_after_benchmark'
                    },
                    session_id=benchmark_session_id,
                    training_context={
                        'benchmark_mode': True,
                        'benchmark_results': {
                            model_id: {
                                'best_f1_macro': result.get('best_f1_macro', 0),
                                'accuracy': result.get('accuracy', 0),
                                'training_time': result.get('training_time', 0)
                            }
                            for model_id, result in benchmark_results.items()
                        },
                        'user_choices': {
                            'enable_benchmark': True,
                            'stopped_after_benchmark': True,
                            'num_models_tested': len(benchmark_results)
                        }
                    }
                )
                self.console.print(f"[dim]💾 Final metadata saved: {final_metadata_path.name}[/dim]")
                self.logger.info(f"✓ Final benchmark metadata saved: {final_metadata_path}")

            except Exception as e:
                self.logger.error(f"Failed to save final benchmark metadata: {e}")
                self.console.print(f"[yellow]⚠️  Warning: Could not save final metadata: {e}[/yellow]")
                # Continue to summaries even if metadata fails

            # Generate comprehensive summary files for benchmark-only session
            try:
                from llm_tool.utils.training_summary_generator import generate_training_summaries

                self.console.print("\n[bold cyan]📊 Generating Comprehensive Benchmark Summaries...[/bold cyan]")
                csv_path, jsonl_path = generate_training_summaries(benchmark_session_id)

                self.console.print("[green]✓ Benchmark summaries generated successfully:[/green]")
                self.console.print(f"  • CSV Summary: [cyan]{csv_path.name}[/cyan]")
                self.console.print(f"  • JSONL Summary: [cyan]{jsonl_path.name}[/cyan]")
                self.console.print(f"\n[dim]Full paths:[/dim]")
                self.console.print(f"  • {csv_path}")
                self.console.print(f"  • {jsonl_path}")

            except Exception as e:
                self.logger.error(f"Failed to generate benchmark summaries: {e}")
                self.console.print(f"[yellow]⚠️  Could not generate comprehensive summaries: {e}[/yellow]")

            return None

        # Select final models
        final_model_name = None
        final_models_by_language = None

        if choice in ["1", "top"]:
            self.console.print("\n[bold green]✓ Using top-ranked model(s)[/bold green]")

            if train_by_language:
                # Select best model per language
                final_models_by_language = {}
                for lang in languages:
                    lang_models = models_by_language_benchmark[lang]
                    # Find best model for this language
                    lang_results = {m: benchmark_results[m] for m in lang_models}
                    best_model = max(lang_results, key=lambda m: lang_results[m].get('best_f1_macro', 0))
                    final_models_by_language[lang] = best_model
                    self.console.print(f"  • {lang}: [cyan]{best_model}[/cyan] (F1: {benchmark_results[best_model]['best_f1_macro']:.3f})")
            else:
                # Take best model overall
                final_model_name = comparison_df.iloc[0]['model']
                self.console.print(f"  • Selected: [cyan]{final_model_name}[/cyan] (F1: {comparison_df.iloc[0]['f1_macro']:.3f})")

        elif choice in ["2", "manual"]:
            self.console.print("\n[bold]Manual Selection:[/bold]")

            if train_by_language:
                final_models_by_language = {}
                for lang in sorted(languages):
                    lang_models = models_by_language_benchmark[lang]

                    self.console.print(f"\n[yellow]Models for {lang}:[/yellow]")
                    for idx, model in enumerate(lang_models, 1):
                        result = benchmark_results[model]
                        self.console.print(f"  {idx}. {model} (F1: {result.get('best_f1_macro', 0):.3f})")

                    choice_idx = IntPrompt.ask(f"Select model for {lang}", default=1)
                    idx_adj = choice_idx - 1
                    if 0 <= idx_adj < len(lang_models):
                        final_models_by_language[lang] = lang_models[idx_adj]
                    else:
                        final_models_by_language[lang] = lang_models[0]

                    self.console.print(f"  [green]✓ {lang}: {final_models_by_language[lang]}[/green]")
            else:
                self.console.print("\n[yellow]Available models:[/yellow]")
                for idx, model in enumerate(selected_models_benchmark, 1):
                    result = benchmark_results[model]
                    self.console.print(f"  {idx}. {model} (F1: {result.get('best_f1_macro', 0):.3f})")

                choice_idx = IntPrompt.ask("Select model", default=1)
                idx_adj = choice_idx - 1
                if 0 <= idx_adj < len(selected_models_benchmark):
                    final_model_name = selected_models_benchmark[idx_adj]
                else:
                    final_model_name = selected_models_benchmark[0]

                self.console.print(f"  [green]✓ Selected: {final_model_name}[/green]")

        # Return results
        result = {}
        if final_model_name:
            result['model_name'] = final_model_name
        if final_models_by_language:
            result['models_by_language'] = final_models_by_language
            result['train_by_language'] = True

        return result

    def _training_studio_render_bundle_summary(self, bundle: TrainingDataBundle) -> None:
        table = Table(title="Dataset Summary", border_style="green")
        table.add_column("Property", style="cyan")
        table.add_column("Value", style="white")

        # Use training_approach from metadata if available, otherwise fallback to bundle.strategy
        training_approach = bundle.metadata.get('training_approach') if hasattr(bundle, 'metadata') else None
        strategy_display = training_approach if training_approach else bundle.strategy
        table.add_row("Strategy", strategy_display)
        table.add_row("Primary file", str(bundle.primary_file) if bundle.primary_file else "—")
        table.add_row("Text column", bundle.text_column)
        table.add_row("Label column", bundle.label_column)
        table.add_row("Training files", str(len(bundle.training_files)))

        if bundle.metadata.get("label_distribution"):
            distribution = ", ".join(f"{k}: {v}" for k, v in bundle.metadata["label_distribution"].items())
            table.add_row("Label distribution", distribution)
        if bundle.metadata.get("categories"):
            table.add_row("Categories", ", ".join(bundle.metadata["categories"]))
        if bundle.metadata.get("analysis"):
            analysis = bundle.metadata["analysis"]
            table.add_row("Annotated rows", str(analysis.get("annotated_rows", "n/a")))

        self.console.print(table)

    def _log_training_data_distributions(self, bundle: TrainingDataBundle, training_context: Optional[Dict[str, Any]] = None) -> None:
        """
        Log comprehensive distribution information for ALL training datasets created.

        This function is called AFTER training/benchmark completion and logs:
        - ALL datasets created (multiclass, onevsall, multilabel)
        - What was used for benchmark vs normal training
        - Train/val/test splits
        - Label distributions
        - Language distributions
        - Imbalance warnings
        - Complete training context (mode, models, results)

        Args:
            bundle: TrainingDataBundle containing all created dataset files
            training_context: Optional dict with training/benchmark information:
                - mode: Training mode (quick, benchmark, custom, distributed)
                - training_result: Results from training
                - runtime_params: Runtime parameters used
                - models_trained: List of models that were trained
                - benchmark_results: Results if benchmark mode was used
        """
        import json

        # Defensive check: Ensure session attributes are initialized
        if not hasattr(self, 'current_session_manager') or not self.current_session_manager:
            self.logger.warning("_log_training_data_distributions called without session_manager initialized. Skipping logging.")
            return
        if not hasattr(self, 'current_session_id') or not self.current_session_id:
            self.logger.warning("_log_training_data_distributions called without session_id initialized. Skipping logging.")
            return

        if training_context:
            self.console.print(f"\n[bold cyan]📊 Generating comprehensive training session report...[/bold cyan]")
            self.console.print(f"[dim]Mode: {training_context.get('mode', 'unknown')} | Models: {len(training_context.get('models_trained', []))}[/dim]")
        else:
            self.console.print("\n[dim]📊 Logging comprehensive training data distributions...[/dim]")

        # Collect all dataset files (primary + training_files)
        all_files = []
        if bundle.primary_file:
            # Use descriptive name based on strategy
            primary_name = f"multilabel_combined" if bundle.strategy == 'multi-label' else "combined_dataset"
            all_files.append((primary_name, bundle.primary_file))
        for key, path in bundle.training_files.items():
            all_files.append((key, path))

        if not all_files:
            self.logger.warning("No training data files found in bundle to log")
            return

        # Log distribution for each dataset file
        datasets_logged = 0
        for dataset_name, dataset_path in all_files:
            try:
                # Load JSONL file
                records = []
                with open(dataset_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            records.append(json.loads(line))

                if not records:
                    self.logger.warning(f"Dataset {dataset_name} is empty: {dataset_path}")
                    continue

                # Build comprehensive metadata including training context
                dataset_metadata = {
                    'file_path': str(dataset_path),
                    'file_size_mb': round(dataset_path.stat().st_size / (1024 * 1024), 2),
                    'num_records': len(records),
                    'strategy': bundle.strategy,
                    'training_approach': bundle.metadata.get('training_approach', ''),
                    'text_column': bundle.text_column,
                    'label_column': bundle.label_column,
                    'source_file': bundle.metadata.get('source_file', ''),
                    'categories': bundle.metadata.get('categories', []),
                    'confirmed_languages': bundle.metadata.get('confirmed_languages', []),
                    'split_config': bundle.metadata.get('split_config', {}),
                }

                # Add training context if provided (mode, benchmark info, etc.)
                if training_context:
                    dataset_metadata.update({
                        'training_mode': training_context.get('mode'),
                        'models_trained': training_context.get('models_trained', []),
                        'was_used_in_benchmark': training_context.get('mode') == 'benchmark',
                        'benchmark_results': training_context.get('benchmark_results') if training_context.get('mode') == 'benchmark' else None,
                    })

                # Log distribution with complete metadata
                self.current_session_manager.log_distribution(
                    dataset_name=dataset_name,
                    train_samples=records,  # All samples (split happens during training)
                    val_samples=[],  # Splitting happens during training
                    test_samples=[],
                    label_key=dataset_name,
                    metadata=dataset_metadata
                )
                datasets_logged += 1

            except Exception as e:
                self.logger.warning(f"Could not log distribution for {dataset_name}: {e}")
                continue

        # Finalize session and generate comprehensive reports
        try:
            warnings_count, datasets_with_warnings = self.current_session_manager.finalize(training_context=training_context)

            # Display summary to user
            if training_context:
                self.console.print(f"\n[green]✓ Complete training session report generated:[/green]")
                self.console.print(f"  • [cyan]Session ID:[/cyan] {self.current_session_id}")
                self.console.print(f"  • [cyan]Training Mode:[/cyan] {training_context.get('mode', 'unknown')}")
                self.console.print(f"  • [cyan]Datasets logged:[/cyan] {datasets_logged}")
                self.console.print(f"  • [cyan]Models trained:[/cyan] {len(training_context.get('models_trained', []))}")
                if training_context.get('mode') == 'benchmark':
                    self.console.print(f"  • [cyan]Benchmark:[/cyan] Results included in reports")
            else:
                self.console.print(f"\n[green]✓ Training data distribution reports generated:[/green]")
                self.console.print(f"  • [cyan]Session ID:[/cyan] {self.current_session_id}")
                self.console.print(f"  • [cyan]Datasets logged:[/cyan] {datasets_logged}")

            self.console.print(f"\n  📋 [cyan]Reports:[/cyan]")
            self.console.print(f"     - Model Catalog:      {self.current_session_manager.training_data_logs_dir / 'model_catalog.csv'} ← ALL models with full details")
            self.console.print(f"     - Session Summary:    {self.current_session_manager.session_dir / 'SESSION_SUMMARY.txt'} ← Complete overview")
            self.console.print(f"     - Quick overview:     {self.current_session_manager.training_data_logs_dir / 'quick_summary.csv'}")
            self.console.print(f"     - Detailed breakdown: {self.current_session_manager.training_data_logs_dir / 'split_summary.csv'}")
            self.console.print(f"     - Complete data:      {self.current_session_manager.training_data_logs_dir / 'distribution_report.json'}")

            if training_context:
                self.console.print(f"\n  [dim]💡 Reports include complete training context: mode, models trained, and benchmark results.[/dim]")
            else:
                self.console.print(f"\n  [dim]💡 Note: Data is currently PRE-SPLIT. The train/val/test split\n"
                                 f"     will be applied during model training according to your configuration.[/dim]")

            if warnings_count > 0:
                self.console.print(f"\n[yellow]⚠️  {warnings_count} validation warning(s) detected across {datasets_with_warnings} dataset(s)[/yellow]")
                self.console.print(f"[dim]  View details in: {self.current_session_manager.warnings_log}[/dim]")
            else:
                self.console.print(f"\n[green]✓ All data validation checks passed[/green]")

        except Exception as e:
            self.logger.warning(f"Could not finalize training data session: {e}")
            self.console.print(f"[yellow]⚠️  Could not generate final reports: {e}[/yellow]")

    def _configure_data_splits(self, keys_to_train: List[str], all_keys_values: Dict[str, set],
                               training_approach: str, key_strategies: Dict[str, str],
                               total_samples: int) -> Optional[Dict[str, Any]]:
        """
        Configure train/test/validation split ratios.

        Args:
            total_samples: Total number of samples in the dataset

        Returns:
            split_config dict or None if user cancels
        """
        from rich.prompt import Prompt, Confirm, FloatPrompt
        from rich.table import Table
        from rich import box

        self.console.print("\n[bold]📊 Data Split Configuration[/bold]\n")
        self.console.print("[dim]Configure how your data will be split for training, validation, and testing.[/dim]\n")

        # Tableau explicatif
        split_table = Table(show_header=True, header_style="bold magenta", border_style="cyan", box=box.ROUNDED)
        split_table.add_column("Set", style="cyan bold", width=15)
        split_table.add_column("Purpose", style="white", width=60)

        split_table.add_row(
            "Training",
            "Used to train the model (learn patterns from data)"
        )
        split_table.add_row(
            "Validation",
            "Used DURING training to:\n"
            "  • Monitor performance at each epoch\n"
            "  • Select best model checkpoint\n"
            "  • Enable early stopping\n"
            "  • Activate reinforced learning if needed"
        )
        split_table.add_row(
            "Test (Optional)",
            "Reserved for FINAL evaluation AFTER training:\n"
            "  • Provides unbiased performance metrics\n"
            "  • Never used during training\n"
            "  • Only evaluated once at the very end"
        )

        self.console.print(split_table)
        self.console.print()

        # Dataset size information
        self.console.print(f"[bold]📈 Dataset Size:[/bold] {total_samples:,} samples\n")

        # Question 1: Use separate test set for final evaluation?
        # Provide recommendation based on dataset size
        if total_samples < 1000:
            self.console.print("[yellow]⚠️  With fewer than 1,000 samples, it's recommended to skip the separate test set.[/yellow]")
            self.console.print("[dim]   Reason: You need as much data as possible for training and validation.[/dim]\n")
            use_test_set_default = False
        elif total_samples < 5000:
            self.console.print("[dim]💡 With your dataset size, a separate test set is optional but not critical.[/dim]\n")
            use_test_set_default = False
        else:
            self.console.print("[dim]✓ Your dataset is large enough to benefit from a separate test set.[/dim]\n")
            use_test_set_default = False

        use_test_set = Confirm.ask(
            "[bold yellow]Keep a separate test set for final evaluation?[/bold yellow]",
            default=use_test_set_default
        )

        self.console.print()

        # Question: Uniform or custom splits?
        self.console.print("\n[bold]Split Mode:[/bold]\n")
        self.console.print("  • [cyan]uniform[/cyan]: Same ratios for all keys/values")
        self.console.print("  • [cyan]custom[/cyan]:  Different ratios per key or value\n")

        split_mode = Prompt.ask(
            "[bold yellow]Split mode[/bold yellow]",
            choices=["uniform", "custom", "u", "c", "back"],
            default="uniform"
        )

        if split_mode == "back":
            return None

        # Normalize shortcuts
        if split_mode == "u":
            split_mode = "uniform"
        elif split_mode == "c":
            split_mode = "custom"

        split_config = {
            'use_test_set': use_test_set,
            'mode': split_mode
        }

        # UNIFORM MODE
        if split_mode == "uniform":
            split_config['uniform'] = self._configure_uniform_splits(use_test_set)
            if split_config['uniform'] is None:
                return None

        # CUSTOM MODE
        else:
            custom_config = self._configure_custom_splits(
                keys_to_train=keys_to_train,
                all_keys_values=all_keys_values,
                training_approach=training_approach,
                key_strategies=key_strategies,
                use_test_set=use_test_set
            )

            if custom_config is None:
                return None

            split_config.update(custom_config)

        # Display summary
        self._display_split_summary(split_config, keys_to_train, all_keys_values, key_strategies)

        return split_config

    def _configure_uniform_splits(self, use_test_set: bool) -> Optional[Dict[str, float]]:
        """Configure uniform split ratios.

        Args:
            use_test_set: If True, configure train/val/test. If False, configure train/val only.
        """
        from rich.prompt import FloatPrompt

        if use_test_set:
            self.console.print("\n[bold]📈 Configure Split Ratios (Train / Validation / Test)[/bold]\n")
            self.console.print("[dim]Ratios must sum to 1.0[/dim]\n")

            train_ratio = FloatPrompt.ask("  Training ratio", default=0.7)
            # Calculate remaining ratio for val + test
            remaining_ratio = round(1.0 - train_ratio, 10)
            # Default: split remaining evenly between val and test (but favor validation slightly)
            default_val = round(min(0.2, remaining_ratio * 0.67), 10)
            default_test = round(remaining_ratio - default_val, 10)

            validation_ratio = FloatPrompt.ask("  Validation ratio", default=default_val)
            # Update test default based on what's left
            remaining_for_test = round(1.0 - train_ratio - validation_ratio, 10)
            test_ratio = FloatPrompt.ask("  Test ratio", default=max(0.0, remaining_for_test))

        else:
            self.console.print("\n[bold]📈 Configure Split Ratios (Train / Validation)[/bold]\n")
            self.console.print("[dim]Ratios must sum to 1.0. Validation will be used for training evaluation.[/dim]\n")

            train_ratio = FloatPrompt.ask("  Training ratio", default=0.8)
            # Calculate default validation as remaining ratio
            default_validation = round(1.0 - train_ratio, 10)
            validation_ratio = FloatPrompt.ask("  Validation ratio", default=default_validation)
            test_ratio = 0.0

        # Validate and normalize
        try:
            train_ratio, validation_ratio, test_ratio = self._validate_split_ratios(
                train_ratio, validation_ratio, test_ratio
            )
        except ValueError as e:
            self.console.print(f"[red]Error: {e}[/red]")
            return None

        return {
            'train_ratio': train_ratio,
            'validation_ratio': validation_ratio,
            'test_ratio': test_ratio
        }

    def _configure_custom_splits(self, keys_to_train: List[str], all_keys_values: Dict[str, set],
                                  training_approach: str, key_strategies: Dict[str, str],
                                  use_test_set: bool) -> Optional[Dict[str, Any]]:
        """Configure custom split ratios per key or value.

        Args:
            use_test_set: If True, configure train/val/test. If False, configure train/val only.
        """
        from rich.prompt import Confirm, FloatPrompt

        custom_config = {}

        # Configure defaults first
        self.console.print("\n[bold]Default Ratios[/bold]")
        self.console.print("[dim]Applied to keys/values not configured below[/dim]\n")

        if use_test_set:
            default_train = FloatPrompt.ask("  Default train ratio", default=0.7)
            # Calculate remaining for val + test
            remaining = 1.0 - default_train
            default_val_calc = min(0.2, remaining * 0.67)
            default_test_calc = remaining - default_val_calc

            default_validation = FloatPrompt.ask("  Default validation ratio", default=default_val_calc)
            remaining_for_test = 1.0 - default_train - default_validation
            default_test = FloatPrompt.ask("  Default test ratio", default=max(0.0, remaining_for_test))
        else:
            default_train = FloatPrompt.ask("  Default train ratio", default=0.8)
            default_val_calc = 1.0 - default_train
            default_validation = FloatPrompt.ask("  Default validation ratio", default=default_val_calc)
            default_test = 0.0

        # Validate defaults
        try:
            default_train, default_validation, default_test = self._validate_split_ratios(
                default_train, default_validation, default_test
            )
        except ValueError as e:
            self.console.print(f"[red]Error in defaults: {e}[/red]")
            return None

        custom_config['defaults'] = {
            'train_ratio': default_train,
            'validation_ratio': default_validation,
            'test_ratio': default_test
        }

        # Determine if we configure by key or by value
        if training_approach == "multi-class":
            # Configure by key
            custom_config['custom_by_key'] = self._configure_custom_by_key(
                keys_to_train, all_keys_values, use_test_set,
                default_train, default_validation, default_test
            )

        elif training_approach == "one-vs-all":
            # Configure by value
            custom_config['custom_by_value'] = self._configure_custom_by_value(
                keys_to_train, all_keys_values, use_test_set,
                default_train, default_validation, default_test
            )

        elif training_approach in ["hybrid", "custom"]:
            # Mix: some keys, some values
            custom_by_key = {}
            custom_by_value = {}

            for key in keys_to_train:
                strategy = key_strategies.get(key, 'multi-class')

                if strategy == 'multi-class':
                    # Configure this key
                    self.console.print(f"\n[bold cyan]{key}[/bold cyan] ([green]multi-class[/green])")
                    customize = Confirm.ask(f"  Customize split for '{key}'?", default=False)

                    if customize:
                        config = self._ask_split_ratios(use_test_set, default_train, default_validation, default_test)
                        if config:
                            custom_by_key[key] = config
                            self.console.print(f"  [green]✓ {key}: {config['train_ratio']:.1%} / {config['validation_ratio']:.1%} / {config['test_ratio']:.1%}[/green]")
                        else:
                            self.console.print(f"  [dim]Using defaults[/dim]")
                    else:
                        self.console.print(f"  [dim]Using defaults[/dim]")

                else:  # one-vs-all
                    # Configure values for this key
                    self.console.print(f"\n[bold yellow]{key}[/bold yellow] ([yellow]one-vs-all[/yellow])")
                    customize = Confirm.ask(f"  Customize splits for values in '{key}'?", default=False)

                    if customize:
                        values = sorted(all_keys_values[key])
                        for value in values:
                            full_name = f"{key}_{value}"

                            customize_value = Confirm.ask(f"    Customize '{value}'?", default=False)

                            if customize_value:
                                config = self._ask_split_ratios(use_test_set, default_train, default_validation, default_test)
                                if config:
                                    custom_by_value[full_name] = config
                                    self.console.print(f"    [green]✓ {value}: {config['train_ratio']:.1%} / {config['validation_ratio']:.1%} / {config['test_ratio']:.1%}[/green]")
                                else:
                                    self.console.print(f"    [dim]Using defaults[/dim]")
                            else:
                                self.console.print(f"    [dim]Using defaults[/dim]")

            if custom_by_key:
                custom_config['custom_by_key'] = custom_by_key
            if custom_by_value:
                custom_config['custom_by_value'] = custom_by_value

        return custom_config

    def _configure_custom_by_key(self, keys_to_train: List[str], all_keys_values: Dict[str, set],
                                  use_test_set: bool, default_train: float,
                                  default_validation: float, default_test: float) -> Dict[str, Dict[str, float]]:
        """Configure custom splits per key.

        Args:
            use_test_set: If True, configure train/val/test. If False, configure train/val only.
        """
        from rich.prompt import Confirm

        custom_by_key = {}

        self.console.print("\n[bold cyan]⚙️  Custom Configuration (per key)[/bold cyan]\n")

        for key in keys_to_train:
            num_values = len(all_keys_values[key])
            self.console.print(f"[bold]{key}[/bold] ({num_values} values)")

            customize = Confirm.ask(f"  Customize split for '{key}'?", default=False)

            if customize:
                config = self._ask_split_ratios(use_test_set, default_train, default_validation, default_test)
                if config:
                    custom_by_key[key] = config
                    self.console.print(f"  [green]✓ {key}: {config['train_ratio']:.1%} / {config['validation_ratio']:.1%} / {config['test_ratio']:.1%}[/green]")
                else:
                    self.console.print(f"  [dim]Using defaults[/dim]")
            else:
                self.console.print(f"  [dim]Using defaults[/dim]")

            self.console.print()

        return custom_by_key

    def _configure_custom_by_value(self, keys_to_train: List[str], all_keys_values: Dict[str, set],
                                    use_test_set: bool, default_train: float,
                                    default_validation: float, default_test: float) -> Dict[str, Dict[str, float]]:
        """Configure custom splits per value.

        Args:
            use_test_set: If True, configure train/val/test. If False, configure train/val only.
        """
        from rich.prompt import Confirm

        custom_by_value = {}

        self.console.print("\n[bold yellow]⚙️  Custom Configuration (per value)[/bold yellow]\n")

        for key in keys_to_train:
            values = sorted(all_keys_values[key])
            self.console.print(f"[bold cyan]{key}[/bold cyan] ({len(values)} values)")

            customize_key = Confirm.ask(f"  Customize splits for values in '{key}'?", default=False)

            if customize_key:
                for value in values:
                    full_name = f"{key}_{value}"

                    customize_value = Confirm.ask(f"    Customize '{value}'?", default=False)

                    if customize_value:
                        config = self._ask_split_ratios(use_test_set, default_train, default_validation, default_test)
                        if config:
                            custom_by_value[full_name] = config
                            self.console.print(f"    [green]✓ {value}: {config['train_ratio']:.1%} / {config['validation_ratio']:.1%} / {config['test_ratio']:.1%}[/green]")
                        else:
                            self.console.print(f"    [dim]Using defaults[/dim]")
                    else:
                        self.console.print(f"    [dim]Using defaults[/dim]")

            self.console.print()

        return custom_by_value

    def _ask_split_ratios(self, use_test_set: bool, default_train: float,
                          default_validation: float, default_test: float) -> Optional[Dict[str, float]]:
        """Ask for split ratios and validate them.

        Args:
            use_test_set: If True, ask for train/val/test. If False, ask for train/val only.
        """
        from rich.prompt import FloatPrompt

        try:
            train = FloatPrompt.ask("      Train ratio", default=default_train)

            # Calculate dynamic default for validation based on entered train ratio
            remaining = round(1.0 - train, 10)
            if use_test_set:
                # Split remaining between val and test
                dynamic_val_default = round(min(default_validation, remaining * 0.67), 10)
            else:
                # All remaining goes to validation
                dynamic_val_default = remaining

            validation = FloatPrompt.ask("      Validation ratio", default=dynamic_val_default)

            if use_test_set:
                # Calculate remaining for test
                remaining_for_test = round(1.0 - train - validation, 10)
                test = FloatPrompt.ask("      Test ratio", default=max(0.0, remaining_for_test))
            else:
                test = 0.0

            # Validate
            train, validation, test = self._validate_split_ratios(train, validation, test)

            return {
                'train_ratio': train,
                'validation_ratio': validation,
                'test_ratio': test
            }

        except ValueError as e:
            self.console.print(f"      [red]Error: {e}[/red]")
            return None

    def _validate_and_filter_insufficient_labels(
        self,
        input_file: str,
        strategy: str,
        min_samples: int = 2,
        auto_remove: bool = False,
        train_by_language: bool = False
    ) -> Tuple[str, bool]:
        """
        Validate that all labels have at least min_samples.
        If not, prompt user to remove insufficient labels.

        CRITICAL: This validation must be LANGUAGE-AWARE when train_by_language=True
        to match the actual splitting logic in DataUtil.prepare_splits().

        Args:
            input_file: Path to JSONL training file
            strategy: 'multi-label' or 'single-label' (multi-class)
            min_samples: Minimum samples required per label (default: 2 for train+val split)
            auto_remove: If True, automatically remove insufficient labels without prompting
            train_by_language: If True, validate per-language label counts (CRITICAL for multilingual)

        Returns:
            Tuple of (filtered_file_path, was_modified)
        """
        import json
        from collections import Counter
        from pathlib import Path
        from rich.table import Table
        from rich import box
        from rich.prompt import Confirm

        input_path = Path(input_file)
        if not input_path.exists():
            return str(input_file), False

        # Read dataset and count labels
        # CRITICAL: When train_by_language=True, count per language-label combination
        label_counter = Counter()
        records = []

        try:
            with open(input_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if not line.strip():
                        continue
                    record = json.loads(line)
                    records.append(record)

                    # Extract labels based on strategy
                    labels_data = record.get('labels', record.get('label'))
                    lang = record.get('lang', 'unknown') if train_by_language else None

                    if strategy == 'multi-label':
                        # Labels is a list of strings
                        if isinstance(labels_data, list):
                            for label in labels_data:
                                if train_by_language:
                                    # CRITICAL: Count per language (matches DataUtil.prepare_splits logic)
                                    key = f"{label}_{lang}"
                                else:
                                    key = str(label)
                                label_counter[key] += 1
                        elif isinstance(labels_data, str):
                            if train_by_language:
                                key = f"{labels_data}_{lang}"
                            else:
                                key = labels_data
                            label_counter[key] += 1
                    else:
                        # Single-label: labels is a string
                        if labels_data:
                            if train_by_language:
                                key = f"{labels_data}_{lang}"
                            else:
                                key = str(labels_data)
                            label_counter[key] += 1

        except Exception as e:
            self.logger.warning(f"Could not validate labels: {e}")
            return str(input_file), False

        # Find insufficient labels
        insufficient_labels = {
            label: count for label, count in label_counter.items()
            if count < min_samples
        }

        if not insufficient_labels:
            # All labels have sufficient samples
            return str(input_file), False

        # Display warning
        self.console.print(f"\n[bold red]⚠️  INSUFFICIENT SAMPLES DETECTED[/bold red]\n")
        if train_by_language:
            self.console.print(f"[yellow]The following language-specific labels have fewer than {min_samples} samples (minimum for train+validation split):[/yellow]")
            self.console.print(f"[dim]Note: Validation is language-aware because train_by_language=True[/dim]\n")
        else:
            self.console.print(f"[yellow]The following labels have fewer than {min_samples} samples (minimum for train+validation split):[/yellow]\n")

        table = Table(border_style="red", show_header=True, header_style="bold red", box=box.ROUNDED)
        table.add_column("Label", style="yellow bold", width=40)
        table.add_column("Samples", style="red", justify="right", width=15)
        table.add_column("Status", style="red", width=20)

        for label, count in sorted(insufficient_labels.items(), key=lambda x: x[1]):
            table.add_row(
                label,
                str(count),
                "❌ BLOCKED"
            )

        self.console.print(table)
        self.console.print()

        # Ask user what to do
        if not auto_remove:
            self.console.print("[bold]Options:[/bold]")
            if strategy == 'multi-label':
                self.console.print("  • [green]Remove[/green]: Automatically remove insufficient labels from samples (samples will be kept)")
                self.console.print("  • [red]Cancel[/red]: Stop training and fix dataset manually\n")
            else:
                self.console.print("  • [green]Remove[/green]: Automatically remove samples with insufficient labels")
                self.console.print("  • [red]Cancel[/red]: Stop training and fix dataset manually\n")

            should_remove = Confirm.ask(
                "Remove insufficient labels automatically?",
                default=False
            )

            if not should_remove:
                self.console.print("[yellow]❌ Training cancelled. Please fix dataset manually.[/yellow]")
                raise ValueError(f"Dataset contains {len(insufficient_labels)} label(s) with insufficient samples (< {min_samples})")

        # Filter dataset
        self.console.print(f"\n[yellow]🔄 Filtering dataset to remove insufficient labels...[/yellow]")

        filtered_records = []
        removed_count = 0
        labels_removed_count = 0  # Track number of label instances removed
        samples_with_removed_labels = 0  # Track samples that had labels removed but were kept

        for record in records:
            labels_data = record.get('labels', record.get('label'))
            lang = record.get('lang', 'unknown') if train_by_language else None

            if strategy == 'multi-label':
                # Filter list of labels - KEEP SAMPLE even if all labels are removed
                if isinstance(labels_data, list):
                    original_labels = labels_data
                    if train_by_language:
                        # Check language-specific keys
                        filtered_labels = [
                            label for label in labels_data
                            if f"{label}_{lang}" not in insufficient_labels
                        ]
                    else:
                        filtered_labels = [
                            label for label in labels_data
                            if str(label) not in insufficient_labels
                        ]

                    # Count removed labels
                    removed_labels_in_sample = len(original_labels) - len(filtered_labels)
                    if removed_labels_in_sample > 0:
                        labels_removed_count += removed_labels_in_sample
                        samples_with_removed_labels += 1

                    # CRITICAL FIX: Keep record even if all labels were removed
                    # The sample itself is still valid, just has no sufficient labels
                    record_copy = record.copy()
                    record_copy['labels'] = filtered_labels  # May be empty list
                    filtered_records.append(record_copy)
                else:
                    # Single label in multi-label format - convert to list and check
                    if labels_data:
                        if train_by_language:
                            check_key = f"{labels_data}_{lang}"
                        else:
                            check_key = str(labels_data)

                        if check_key not in insufficient_labels:
                            # Keep as-is (string format)
                            filtered_records.append(record)
                        else:
                            # Label is insufficient - keep sample but remove label
                            labels_removed_count += 1
                            samples_with_removed_labels += 1
                            record_copy = record.copy()
                            record_copy['labels'] = []  # Empty labels list
                            filtered_records.append(record_copy)
                    else:
                        # No labels at all - keep sample
                        filtered_records.append(record)
            else:
                # Single-label: MUST remove sample if label is insufficient
                # (cannot have a single-label sample with no label)
                if labels_data:
                    if train_by_language:
                        check_key = f"{labels_data}_{lang}"
                    else:
                        check_key = str(labels_data)

                    if check_key not in insufficient_labels:
                        filtered_records.append(record)
                    else:
                        # For single-label, we must remove the sample
                        removed_count += 1
                        labels_removed_count += 1
                else:
                    # No label - remove sample
                    removed_count += 1

        # Save filtered dataset
        filtered_path = input_path.parent / f"{input_path.stem}_filtered{input_path.suffix}"

        with open(filtered_path, 'w', encoding='utf-8') as f:
            for record in filtered_records:
                f.write(json.dumps(record, ensure_ascii=False) + '\n')

        self.console.print(f"[green]✓ Filtered dataset saved:[/green] {filtered_path.name}")
        self.console.print(f"  • [cyan]Original samples:[/cyan] {len(records)}")
        self.console.print(f"  • [cyan]Filtered samples:[/cyan] {len(filtered_records)}")

        if strategy == 'multi-label':
            # For multi-label, show label removal stats (samples are kept)
            self.console.print(f"  • [green]Samples kept:[/green] {len(filtered_records)} (all samples preserved)")
            if removed_count > 0:
                self.console.print(f"  • [yellow]Samples removed:[/yellow] {removed_count} (only if needed)")
            self.console.print(f"  • [yellow]Samples with labels removed:[/yellow] {samples_with_removed_labels}")
            self.console.print(f"  • [red]Label instances removed:[/red] {labels_removed_count}")
            self.console.print(f"  • [red]Insufficient label types:[/red] {len(insufficient_labels)}")
        else:
            # For single-label, samples must be removed if label is insufficient
            self.console.print(f"  • [yellow]Removed samples:[/yellow] {removed_count}")
            self.console.print(f"  • [red]Removed label types:[/red] {len(insufficient_labels)}")

        self.console.print()

        return str(filtered_path), True

    def _validate_split_ratios(self, train: float, validation: float, test: float) -> Tuple[float, float, float]:
        """Validate and normalize split ratios."""
        # Check total
        total = train + validation + test

        if abs(total - 1.0) > 0.001:
            # Auto-adjust
            factor = 1.0 / total
            train *= factor
            validation *= factor
            test *= factor
            self.console.print(f"  [yellow]⚠️  Ratios adjusted to sum to 1.0[/yellow]")

        # Minimum values
        if train < 0.5:
            raise ValueError("Training ratio must be at least 50%")

        if validation > 0 and validation < 0.05:
            raise ValueError("Validation ratio must be at least 5% if used")

        if test > 0 and test < 0.05:
            raise ValueError("Test ratio must be at least 5% if used")

        return train, validation, test

    def _display_split_summary(self, split_config: Dict[str, Any], keys_to_train: List[str],
                               all_keys_values: Dict[str, set], key_strategies: Dict[str, str]) -> None:
        """Display summary of split configuration."""
        from rich.table import Table
        from rich import box

        self.console.print("\n[bold green]✓ Split Configuration Complete[/bold green]\n")

        mode = split_config['mode']
        use_test_set = split_config['use_test_set']

        if mode == 'uniform':
            ratios = split_config['uniform']
            self.console.print("[bold]Uniform Split (all keys/values):[/bold]")
            self.console.print(f"  • Train:      {ratios['train_ratio']:.1%}")
            self.console.print(f"  • Validation: {ratios['validation_ratio']:.1%}")
            if use_test_set:
                self.console.print(f"  • Test:       {ratios['test_ratio']:.1%}")

        else:
            self.console.print("[bold]Custom Split:[/bold]")

            custom_by_key = split_config.get('custom_by_key', {})
            custom_by_value = split_config.get('custom_by_value', {})
            defaults = split_config.get('defaults', {})

            if custom_by_key:
                self.console.print(f"\n  [green]Configured keys: {len(custom_by_key)}[/green]")
                for key, ratios in list(custom_by_key.items())[:5]:
                    self.console.print(f"    • {key}: {ratios['train_ratio']:.1%} / {ratios['validation_ratio']:.1%} / {ratios['test_ratio']:.1%}")
                if len(custom_by_key) > 5:
                    self.console.print(f"    ... and {len(custom_by_key) - 5} more")

            if custom_by_value:
                self.console.print(f"\n  [yellow]Configured values: {len(custom_by_value)}[/yellow]")
                for value, ratios in list(custom_by_value.items())[:5]:
                    self.console.print(f"    • {value}: {ratios['train_ratio']:.1%} / {ratios['validation_ratio']:.1%} / {ratios['test_ratio']:.1%}")
                if len(custom_by_value) > 5:
                    self.console.print(f"    ... and {len(custom_by_value) - 5} more")

            if defaults:
                self.console.print(f"\n  [dim]Defaults (for others): {defaults['train_ratio']:.1%} / {defaults['validation_ratio']:.1%} / {defaults['test_ratio']:.1%}[/dim]")

        self.console.print()

    def _collect_quick_mode_parameters(self, bundle: TrainingDataBundle, preloaded_params: Optional[Dict[str, Any]] = None) -> Optional[Dict[str, Any]]:
        """
        Collect parameters for quick mode training (STEP 1 of new flow).

        Returns dict with keys: model_name, reinforced_learning, epochs
        Returns None if user cancels
        """
        from rich.prompt import Prompt, IntPrompt, Confirm
        from llm_tool.utils.model_display import get_recommended_models, display_all_models
        from rich.table import Table
        from rich import box

        # STEP 1A: Token Length Strategy Selection
        self.console.print("\n[bold cyan]═══════════════════════════════════════════════════════════════[/bold cyan]")
        self.console.print("[bold cyan]           📏 STEP 1: Token Length Strategy                    [/bold cyan]")
        self.console.print("[bold cyan]═══════════════════════════════════════════════════════════════[/bold cyan]\n")

        # Get languages from metadata
        languages = set()
        if hasattr(bundle, 'metadata') and bundle.metadata:
            languages = bundle.metadata.get('confirmed_languages', bundle.metadata.get('languages', set()))
        if not languages and hasattr(bundle, 'languages') and bundle.languages:
            languages = set([lang.upper() for lang in bundle.languages])
        if languages:
            languages = set([str(lang).upper() for lang in languages])

        # Get text length stats
        text_length_stats = bundle.metadata.get('text_length_stats', {}) if hasattr(bundle, 'metadata') else {}
        if text_length_stats.get('token_mean'):
            text_length_avg = text_length_stats['token_mean']
        elif text_length_stats.get('char_mean'):
            text_length_avg = text_length_stats['char_mean']
        else:
            text_length_avg = getattr(bundle, 'text_length_avg', 158)

        requires_long_model = text_length_stats.get('requires_long_model', False)

        # Get distribution data to calculate percentage exceeding 512 tokens
        distribution = text_length_stats.get('distribution', {})

        # Calculate percentage exceeding 512 tokens
        # Handle different possible structures of distribution
        total_docs = 0
        docs_exceeding_512 = 0
        pct_exceeding_512 = 0

        if distribution and isinstance(distribution, dict):
            # Try to extract counts - distribution might be nested
            try:
                # Check if values are integers (direct counts)
                if all(isinstance(v, (int, float)) for v in distribution.values()):
                    total_docs = sum(distribution.values())
                    docs_exceeding_512 = distribution.get('long', 0) + distribution.get('very_long', 0)
                    pct_exceeding_512 = (docs_exceeding_512 / total_docs * 100) if total_docs > 0 else 0
                else:
                    # Distribution might have nested structure - try to extract counts
                    for key, value in distribution.items():
                        if isinstance(value, dict) and 'count' in value:
                            total_docs += value['count']
                            if key in ['long', 'very_long']:
                                docs_exceeding_512 += value['count']
                    pct_exceeding_512 = (docs_exceeding_512 / total_docs * 100) if total_docs > 0 else 0
            except (TypeError, KeyError, AttributeError):
                # Fallback to percentage-based calculation
                pass

        # If we couldn't calculate from distribution, try direct percentage fields
        if pct_exceeding_512 == 0 and total_docs == 0:
            if 'pct_long' in text_length_stats and 'pct_very_long' in text_length_stats:
                pct_exceeding_512 = text_length_stats.get('pct_long', 0) + text_length_stats.get('pct_very_long', 0)
                # Estimate docs count if we have the total
                if 'total_docs' in text_length_stats:
                    total_docs = text_length_stats['total_docs']
                    docs_exceeding_512 = int(total_docs * pct_exceeding_512 / 100)

        # Show token length summary
        self.console.print("[bold]📊 Your Dataset Token Analysis:[/bold]\n")

        stats_table = Table(show_header=True, header_style="bold magenta", border_style="cyan", box=box.SIMPLE)
        stats_table.add_column("Metric", style="cyan", width=30)
        stats_table.add_column("Value", style="white", width=25)

        # Use actual values from text_length_stats
        token_mean = text_length_stats.get('token_mean', text_length_stats.get('avg_tokens', 0))
        token_median = text_length_stats.get('token_median', text_length_stats.get('median_tokens', 0))
        token_p95 = text_length_stats.get('token_p95', text_length_stats.get('p95_tokens', 0))
        token_max = text_length_stats.get('token_max', text_length_stats.get('max_tokens', 0))

        stats_table.add_row("Mean tokens per document", f"{token_mean:.0f}")
        stats_table.add_row("Median tokens", f"{token_median:.0f}")
        stats_table.add_row("95th percentile", f"{token_p95:.0f}")
        stats_table.add_row("Maximum tokens", f"{token_max:.0f}")
        stats_table.add_row("[bold]% exceeding 512 tokens[/bold]", f"[bold yellow]{pct_exceeding_512:.1f}%[/bold yellow]")

        # Show distribution if available
        if distribution and total_docs > 0:
            self.console.print(stats_table)
            self.console.print()

            self.console.print("[bold]📈 Token Length Distribution:[/bold]\n")
            dist_table = Table(show_header=True, header_style="bold magenta", border_style="blue", box=box.SIMPLE)
            dist_table.add_column("Category", style="cyan", width=20)
            dist_table.add_column("Token Range", style="white", width=20)
            dist_table.add_column("Count", style="green", width=12, justify="right")
            dist_table.add_column("Percentage", style="yellow", width=12, justify="right")

            # Extract counts - handle both dict and int values
            def get_count(category_data):
                if isinstance(category_data, dict):
                    return category_data.get('count', 0)
                elif isinstance(category_data, (int, float)):
                    return int(category_data)
                return 0

            short_count = get_count(distribution.get('short', 0))
            medium_count = get_count(distribution.get('medium', 0))
            long_count = get_count(distribution.get('long', 0))
            very_long_count = get_count(distribution.get('very_long', 0))

            dist_table.add_row("Short", "< 128 tokens", f"{short_count:,}", f"{short_count/total_docs*100:.1f}%")
            dist_table.add_row("Medium", "128-511 tokens", f"{medium_count:,}", f"{medium_count/total_docs*100:.1f}%")
            dist_table.add_row("[yellow]Long[/yellow]", "[yellow]512-1023 tokens[/yellow]", f"[yellow]{long_count:,}[/yellow]", f"[yellow]{long_count/total_docs*100:.1f}%[/yellow]")
            dist_table.add_row("[red]Very Long[/red]", "[red]≥ 1024 tokens[/red]", f"[red]{very_long_count:,}[/red]", f"[red]{very_long_count/total_docs*100:.1f}%[/red]")

            self.console.print(dist_table)
        else:
            self.console.print(stats_table)

        self.console.print()

        # Check if there are ANY documents exceeding 512 tokens
        if pct_exceeding_512 == 0.0:
            # No documents exceed 512 tokens - no strategy needed!
            self.console.print("[bold green]✓ Perfect! All documents fit within 512 tokens[/bold green]")
            self.console.print("[dim]No special handling needed - you can use any standard BERT model.[/dim]\n")

            self.console.print("[bold cyan]📊 Why this matters:[/bold cyan]")
            self.console.print(f"  • [green]Maximum document length:[/green] {token_max:.0f} tokens (well below 512 limit)")
            self.console.print(f"  • [green]Mean document length:[/green] {token_mean:.0f} tokens")
            self.console.print(f"  • [green]95th percentile:[/green] {token_p95:.0f} tokens")
            self.console.print("  • [green]All data will be used[/green] without chunking or truncation")
            self.console.print("  • [green]Fastest training[/green] with standard models (BERT, RoBERTa, CamemBERT, etc.)\n")

            # Set default flags - no special handling needed
            prefers_long_models = False
            exclude_long_texts = False
            split_long_texts = False
        else:
            # Determine recommended strategy based on percentage (intelligent)
            if pct_exceeding_512 < 10:
                recommended_strategy = "truncate"
                rec_reason = f"Only {pct_exceeding_512:.1f}% exceed 512 tokens - splitting long documents will preserve all information"
            elif pct_exceeding_512 < 25:
                recommended_strategy = "truncate"
                rec_reason = f"{pct_exceeding_512:.1f}% exceed 512 tokens - splitting is recommended, or consider long models for better context"
            elif pct_exceeding_512 < 40:
                recommended_strategy = "long_models"
                rec_reason = f"{pct_exceeding_512:.1f}% exceed 512 tokens - long models recommended to preserve document context"
            else:
                recommended_strategy = "long_models"
                rec_reason = f"{pct_exceeding_512:.1f}% exceed 512 tokens - long models strongly recommended"

            # Present 3 strategies
            self.console.print("[bold yellow]⚠️  Standard BERT models have a 512 token limit[/bold yellow]")
            self.console.print("[dim]You need to choose how to handle longer documents:[/dim]\n")

            strategy_table = Table(show_header=True, header_style="bold magenta", border_style="green", box=box.ROUNDED)
            strategy_table.add_column("Strategy", style="cyan bold", width=18)
            strategy_table.add_column("Description", style="white", width=70)

            truncate_mark = " ✓ [green]RECOMMENDED[/green]" if recommended_strategy == "truncate" else ""
            exclude_mark = " ✓ [green]RECOMMENDED[/green]" if recommended_strategy == "exclude" else ""
            long_mark = " ✓ [green]RECOMMENDED[/green]" if recommended_strategy == "long_models" else ""

            # Calculate how many extra samples we'd get from splitting
            estimated_extra_samples = 0
            if docs_exceeding_512 > 0:
                # Estimate based on average tokens for long docs
                estimated_extra_samples = int(docs_exceeding_512 * 1.5)  # Conservative estimate
                extra_info = f"Creates ~{estimated_extra_samples:,} additional training samples from long documents"
            else:
                extra_info = "No documents exceed 512 tokens"

            strategy_table.add_row(
                "1. Split/Chunk" + truncate_mark,
                "✂️  Split long documents into 512-token chunks (with overlap)\n"
                f"• [green]Each chunk keeps the same label[/green] → More training data!\n"
                f"• Example: 1024-token doc → 2 samples (tokens 0-512, tokens 256-768)\n"
                f"• {extra_info}\n"
                f"• Fastest training (~5-10 min)\n"
                f"• Works with all standard models (BERT, RoBERTa, CamemBERT, etc.)\n"
                f"• [bold]No information loss[/bold] - all text is used"
            )
            strategy_table.add_row(
                "2. Exclude" + exclude_mark,
                f"🗑️  Remove documents exceeding 512 tokens entirely\n"
                f"• Would exclude {docs_exceeding_512:,} documents ({pct_exceeding_512:.1f}% of dataset)\n"
                f"• [red]Reduces training data significantly[/red]\n"
                f"• Model won't learn from long documents\n"
                f"• Only use if long documents are outliers/noise"
            )
            strategy_table.add_row(
                "3. Long Models" + long_mark,
                "🔬 Use long-document models (up to 4096 tokens)\n"
                "• Preserves full document context in single sample\n"
                "• Better for tasks requiring full document understanding\n"
                "• Slower training (~15-30 min) and inference\n"
                "• Models: Longformer, BigBird, Long-T5, XLM-RoBERTa-Longformer"
            )

            self.console.print(strategy_table)
            self.console.print()

            self.console.print(f"[bold yellow]💡 Smart Recommendation:[/bold yellow] [cyan]{rec_reason}[/cyan]\n")

            # Ask user to choose
            strategy_choice = Prompt.ask(
                "[bold yellow]Choose strategy[/bold yellow]",
                choices=["1", "2", "3", "split", "chunk", "exclude", "long", "long_models"],
                default="1" if recommended_strategy == "truncate" else ("2" if recommended_strategy == "exclude" else "3")
            )

            # Map choice to boolean flags
            # Initialize all flags
            prefers_long_models = False
            exclude_long_texts = False
            split_long_texts = False

            if strategy_choice in ["1", "split", "chunk", "truncate"]:
                split_long_texts = True
                if docs_exceeding_512 > 0:
                    self.console.print(f"[green]✓ Strategy: Split long documents into chunks (creates ~{estimated_extra_samples:,} extra samples)[/green]\n")
                else:
                    self.console.print("[green]✓ Strategy: Split long documents (if any) into chunks[/green]\n")
            elif strategy_choice in ["2", "exclude"]:
                exclude_long_texts = True
                self.console.print(f"[yellow]✓ Strategy: Exclude {docs_exceeding_512:,} documents >512 tokens ({pct_exceeding_512:.1f}% of dataset)[/yellow]\n")
            else:  # "3", "long", or "long_models"
                prefers_long_models = True
                self.console.print("[green]✓ Strategy: Use long-document models (up to 4096 tokens)[/green]\n")

        # Store choice in text_length_stats for later use
        text_length_stats['user_prefers_long_models'] = prefers_long_models
        text_length_stats['exclude_long_texts'] = exclude_long_texts
        text_length_stats['split_long_texts'] = split_long_texts

        # STEP 2A: Multilingual Strategy (if multiple languages detected)
        train_by_language = False
        if len(languages) > 1:
            self.console.print("\n[bold cyan]═══════════════════════════════════════════════════════════════[/bold cyan]")
            self.console.print("[bold cyan]           🌍 STEP 2A: Multilingual Strategy                   [/bold cyan]")
            self.console.print("[bold cyan]═══════════════════════════════════════════════════════════════[/bold cyan]\n")

            self.console.print(f"[bold]Your dataset contains multiple languages:[/bold] {', '.join(sorted(languages))}\n")

            strategy_table = Table(show_header=True, header_style="bold magenta", border_style="green", box=box.ROUNDED)
            strategy_table.add_column("Approach", style="cyan bold", width=25)
            strategy_table.add_column("Description", style="white", width=70)

            strategy_table.add_row(
                "1. Multilingual Model",
                "🌐 Train ONE model that handles all languages\n"
                f"• Works across {', '.join(sorted(languages))} without distinction\n"
                "• Faster: Single training run\n"
                "• Good for: Cross-lingual tasks, similar performance needed across languages\n"
                "• Models: XLM-RoBERTa, mBERT, mT5, etc.\n"
                "• [green]Recommended if[/green]: Languages are balanced in dataset"
            )
            strategy_table.add_row(
                "2. One Model per Language",
                "🎯 Train SEPARATE specialized models for each language\n"
                f"• {len(languages)} models total: one for each language\n"
                f"• Each model specialized for its language (e.g., CamemBERT for FR, BERT for EN)\n"
                "• Better performance: Language-specific models often outperform multilingual\n"
                "• Longer training: Multiple training runs\n"
                f"• You'll select a model for each language: {', '.join(sorted(languages))}\n"
                "• [green]Recommended if[/green]: Best possible performance is priority"
            )

            self.console.print(strategy_table)
            self.console.print()

            multilingual_choice = Prompt.ask(
                "[bold yellow]Choose approach[/bold yellow]",
                choices=["1", "2", "multilingual", "per-language", "per_language"],
                default="2"  # Recommend per-language for better performance
            )

            if multilingual_choice in ["2", "per-language", "per_language"]:
                train_by_language = True
                self.console.print(f"\n[green]✓ Will train {len(languages)} specialized models (one per language)[/green]\n")
            else:
                train_by_language = False
                self.console.print("\n[green]✓ Will train 1 multilingual model[/green]\n")

        # STEP 2B: Model Selection
        self.console.print("\n[bold cyan]═══════════════════════════════════════════════════════════════[/bold cyan]")
        self.console.print("[bold cyan]           🤖 STEP 2B: Model Selection                         [/bold cyan]")
        self.console.print("[bold cyan]═══════════════════════════════════════════════════════════════[/bold cyan]\n")

        # Import model utilities
        from llm_tool.utils.model_display import get_recommended_models, MODEL_METADATA

        # ASK ABOUT BENCHMARK MODE
        self.console.print("[bold]🎯 Benchmark Mode[/bold]")
        self.console.print("  • [cyan]Compare multiple models[/cyan] before full training")
        self.console.print("  • [cyan]Test on selected categories[/cyan] with class imbalance analysis")
        self.console.print("  • [cyan]See which models perform best[/cyan] on your specific data")
        self.console.print("  • [cyan]Make informed model selection[/cyan] based on real performance\n")

        self.console.print("[yellow]Requirements:[/yellow]")
        self.console.print("  • Must select at least [bold]2 models[/bold] per language (or 2+ multilingual models)")
        self.console.print("  • Benchmark runs quick training (3-5 epochs) on subset of data")
        self.console.print("  • Takes ~5-15 min depending on models selected\n")

        enable_benchmark = Confirm.ask(
            "[bold yellow]Enable benchmark mode to compare models?[/bold yellow]",
            default=False
        )

        # ============ BENCHMARK MODE INTEGRATION ============
        if enable_benchmark:
            # Run benchmark mode workflow
            benchmark_result = self._run_benchmark_mode(
                bundle=bundle,
                languages=languages,
                train_by_language=train_by_language,
                text_length_avg=text_length_avg,
                prefers_long_models=prefers_long_models
            )

            if benchmark_result is None:
                # User chose to stop
                return None

            # Extract selected models from benchmark result
            model_name = benchmark_result.get('model_name')
            models_by_language = benchmark_result.get('models_by_language', {})

            # Ensure model_name is set for compatibility
            if not model_name and models_by_language:
                # Per-language mode: use first model as primary for compatibility
                model_name = list(models_by_language.values())[0]

            # Show summary of benchmark-selected models
            if train_by_language and models_by_language:
                self.console.print(f"\n[bold green]✓ Models Selected from Benchmark:[/bold green]")
                for lang, model in sorted(models_by_language.items()):
                    self.console.print(f"  • {lang}: [cyan]{model}[/cyan]")
            elif model_name:
                self.console.print(f"\n[bold green]✓ Model Selected from Benchmark:[/bold green]")
                self.console.print(f"  • [cyan]{model_name}[/cyan]")

            # Continue to rest of flow (epochs, reinforced learning, etc.)
            # with the models selected from benchmark
        else:
            # Normal flow: manual model selection

            # Get model strategy
            if train_by_language:
                model_strategy = "per-language"
            elif len(languages) > 1:
                model_strategy = "multilingual"
            elif 'FR' in languages:
                model_strategy = "fr"
            elif 'EN' in languages:
                model_strategy = "en"
            else:
                model_strategy = "multilingual"

            # Initialize models_by_language dict
            models_by_language = {}

        # ============ MODEL SELECTION (normal flow when benchmark disabled) ============
        # Handle per-language model selection
        if train_by_language and not enable_benchmark:
            # Select one model for each language
            for lang in sorted(languages):
                self.console.print(f"\n[bold yellow]{'─'*60}[/bold yellow]")
                self.console.print(f"[bold yellow]🎯 Selecting model for {lang} texts[/bold yellow]")
                self.console.print(f"[bold yellow]{'─'*60}[/bold yellow]\n")

                # Get recommendations for this specific language
                lang_recommended = get_recommended_models(
                    languages={lang},  # Use set, not list
                    avg_text_length=text_length_avg,
                    requires_long_model=prefers_long_models,
                    top_n=10
                )

                if lang_recommended:
                    # Show top 10 models for this language
                    self.console.print(f"[bold cyan]🎯 Top 10 Recommended Models for {lang}:[/bold cyan]\n")

                    models_table = Table(show_header=True, header_style="bold magenta", border_style="cyan", box=box.ROUNDED)
                    models_table.add_column("#", style="yellow", width=3)
                    models_table.add_column("Model ID", style="cyan", width=45)
                    models_table.add_column("Languages", style="green", width=15)
                    models_table.add_column("Max Tokens", style="blue", width=11)
                    models_table.add_column("Size", style="magenta", width=10)
                    models_table.add_column("Description", style="white", width=45)

                    for idx, model_id in enumerate(lang_recommended[:10], 1):
                        meta = MODEL_METADATA.get(model_id, {})
                        langs = ', '.join(meta.get('languages', ['?']))
                        max_len = str(meta.get('max_length', '?'))
                        size = meta.get('size', '?')
                        desc = meta.get('description', '')[:43]

                        models_table.add_row(str(idx), model_id, langs, max_len, size, desc)

                    self.console.print(models_table)
                    default_model = lang_recommended[0]
                else:
                    # Fallback defaults by language
                    if lang == 'FR':
                        default_model = 'camembert-base'
                    elif lang == 'EN':
                        default_model = 'bert-base-uncased'
                    else:
                        default_model = 'xlm-roberta-base'

                # Offer to display all models
                self.console.print(f"\n[dim]💡 Selection Options:[/dim]")
                self.console.print(f"[dim]  • Enter [cyan]1-10[/cyan] to select from Top 10 recommendations[/dim]")
                self.console.print(f"[dim]  • Enter [cyan]'info X'[/cyan] (e.g., 'info 1') to see full details of a model[/dim]")
                self.console.print(f"[dim]  • Enter [cyan]'all'[/cyan] to see ALL {len(MODEL_METADATA)} available models[/dim]")
                self.console.print(f"[dim]  • Enter any [cyan]HuggingFace model ID[/cyan] directly[/dim]")

                model_input = Prompt.ask(f"\n[bold yellow]Model for {lang}[/bold yellow]", default=default_model)

                # Check if user wants info on a model
                if model_input.lower().startswith('info '):
                    info_target = model_input[5:].strip()
                    if info_target.isdigit():
                        info_idx = int(info_target) - 1
                        if lang_recommended and 0 <= info_idx < len(lang_recommended):
                            self._display_model_details(lang_recommended[info_idx], MODEL_METADATA)
                        else:
                            self.console.print(f"[red]Invalid model number: {info_target}[/red]")
                    else:
                        self._display_model_details(info_target, MODEL_METADATA)
                    # After showing info, ask again for selection
                    model_input = Prompt.ask(f"\n[bold yellow]Model for {lang}[/bold yellow]", default=default_model)

                # Check if user wants to see all models
                if model_input.lower() == 'all':
                    # Show ALL models with complete characteristics
                    self.console.print(f"\n[bold cyan]📚 ALL {len(MODEL_METADATA)} Available Models:[/bold cyan]\n")

                    all_models_table = Table(show_header=True, header_style="bold magenta", border_style="green", box=box.ROUNDED)
                    all_models_table.add_column("#", style="yellow", width=4)
                    all_models_table.add_column("Model ID", style="cyan", width=40)
                    all_models_table.add_column("Languages", style="green", width=15)
                    all_models_table.add_column("Max Tokens", style="blue", width=11)
                    all_models_table.add_column("Size", style="magenta", width=10)
                    all_models_table.add_column("Description", style="white", width=50)

                    # Sort models: recommended first, then by relevance
                    all_model_ids = list(MODEL_METADATA.keys())
                    sorted_model_ids = []
                    for model_id in lang_recommended:
                        if model_id in all_model_ids:
                            sorted_model_ids.append(model_id)
                    for model_id in all_model_ids:
                        if model_id not in sorted_model_ids:
                            sorted_model_ids.append(model_id)

                    for idx, model_id in enumerate(sorted_model_ids, 1):
                        meta = MODEL_METADATA.get(model_id, {})
                        langs = ', '.join(meta.get('languages', ['?']))
                        max_len = str(meta.get('max_length', '?'))
                        size = meta.get('size', '?')
                        desc = meta.get('description', '')[:48]

                        # Highlight recommended models
                        if lang_recommended and model_id in lang_recommended[:10]:
                            all_models_table.add_row(
                                f"[bold green]{idx}[/bold green]",
                                f"[bold green]{model_id}[/bold green]",
                                langs,
                                max_len,
                                size,
                                desc
                            )
                        else:
                            all_models_table.add_row(str(idx), model_id, langs, max_len, size, desc)

                    self.console.print(all_models_table)

                    self.console.print(f"\n[dim]💡 [bold green]Green models[/bold green] are in your Top 10 recommendations for {lang}[/dim]")
                    self.console.print(f"\n[bold yellow]Select a model for {lang}:[/bold yellow]")
                    self.console.print(f"[dim]  • Enter the # number from the table[/dim]")
                    self.console.print(f"[dim]  • Or enter the model ID directly[/dim]")

                    model_input_after_all = Prompt.ask(f"\nModel for {lang}", default=default_model)

                    if model_input_after_all.isdigit():
                        idx = int(model_input_after_all) - 1
                        if 0 <= idx < len(sorted_model_ids):
                            lang_model = sorted_model_ids[idx]
                            self.console.print(f"[green]✓ Selected for {lang}: {lang_model}[/green]")
                        else:
                            self.console.print(f"[yellow]⚠️  Invalid selection. Using default: {default_model}[/yellow]")
                            lang_model = default_model
                    else:
                        lang_model = model_input_after_all
                elif model_input.isdigit():
                    idx = int(model_input) - 1
                    if lang_recommended and 0 <= idx < len(lang_recommended):
                        lang_model = lang_recommended[idx]
                        self.console.print(f"[green]✓ Selected for {lang}: {lang_model}[/green]")
                    else:
                        self.console.print(f"[yellow]⚠️  Invalid selection. Using default: {default_model}[/yellow]")
                        lang_model = default_model
                else:
                    lang_model = model_input

                # Display full model details after selection
                self._display_model_details(lang_model, MODEL_METADATA)

                models_by_language[lang] = lang_model

            # Show summary of selected models
            self.console.print(f"\n[bold green]✓ Model Selection Complete:[/bold green]")
            for lang, model in sorted(models_by_language.items()):
                self.console.print(f"  • {lang}: [cyan]{model}[/cyan]")

            # For compatibility with rest of code, use first model as primary
            model_name = list(models_by_language.values())[0]

        elif not enable_benchmark:
            # Single model selection (multilingual or single language)
            # Display context
            strategy_desc = "Long-document models" if prefers_long_models else "Standard models (512 tokens max)"

            # Determine which languages to use for recommendations
            # If multilingual strategy was chosen, only show multilingual models
            if model_strategy == "multilingual" and len(languages) > 1:
                # User chose multilingual model - only show multilingual models
                languages_for_recommendation = {'MULTI'}
            else:
                # Per-language or single language - show language-specific models
                languages_for_recommendation = languages

            # Get intelligent recommendations using utility function
            recommended_models_list = get_recommended_models(
                languages=languages_for_recommendation,
                avg_text_length=text_length_avg,
                requires_long_model=prefers_long_models,
                top_n=10
            )

            if recommended_models_list:
                # Show top 10 with detailed characteristics
                self.console.print("[bold cyan]🎯 Top 10 Recommended Models:[/bold cyan]\n")

                models_table = Table(show_header=True, header_style="bold magenta", border_style="cyan", box=box.ROUNDED)
                models_table.add_column("#", style="yellow", width=3)
                models_table.add_column("Model ID", style="cyan", width=45)
                models_table.add_column("Languages", style="green", width=15)
                models_table.add_column("Max Tokens", style="blue", width=11)
                models_table.add_column("Size", style="magenta", width=10)
                models_table.add_column("Description", style="white", width=45)

                for idx, model_id in enumerate(recommended_models_list[:10], 1):
                    meta = MODEL_METADATA.get(model_id, {})
                    langs = ', '.join(meta.get('languages', ['?']))
                    max_len = str(meta.get('max_length', '?'))
                    size = meta.get('size', '?')
                    desc = meta.get('description', '')[:43]

                    models_table.add_row(str(idx), model_id, langs, max_len, size, desc)

                self.console.print(models_table)
                default_model = recommended_models_list[0]
            else:
                if 'FR' in languages:
                    default_model = 'camembert-base'
                elif 'EN' in languages:
                    default_model = 'bert-base-uncased'
                else:
                    default_model = 'xlm-roberta-base'

            # Use preloaded model if available
            if preloaded_params and preloaded_params.get('model_name'):
                default_model = preloaded_params['model_name']

            # Offer to display all models
            self.console.print(f"\n[dim]💡 Selection Options:[/dim]")
            self.console.print(f"[dim]  • Enter [cyan]1-10[/cyan] to select from Top 10 recommendations[/dim]")
            self.console.print(f"[dim]  • Enter [cyan]'info X'[/cyan] (e.g., 'info 1') to see full details of a model[/dim]")
            self.console.print(f"[dim]  • Enter [cyan]'all'[/cyan] to see ALL {len(MODEL_METADATA)} available models with complete characteristics[/dim]")
            self.console.print(f"[dim]  • Enter any [cyan]HuggingFace model ID[/cyan] directly (e.g., 'bert-base-multilingual-cased')[/dim]")

            model_input = Prompt.ask("\n[bold yellow]Model to train[/bold yellow]", default=default_model)

            # Check if user wants info on a model
            if model_input.lower().startswith('info '):
                info_target = model_input[5:].strip()
                if info_target.isdigit():
                    info_idx = int(info_target) - 1
                    if recommended_models_list and 0 <= info_idx < len(recommended_models_list):
                        self._display_model_details(recommended_models_list[info_idx], MODEL_METADATA)
                    else:
                        self.console.print(f"[red]Invalid model number: {info_target}[/red]")
                else:
                    self._display_model_details(info_target, MODEL_METADATA)
                # After showing info, ask again for selection
                model_input = Prompt.ask("\n[bold yellow]Model to train[/bold yellow]", default=default_model)

            # Check if user wants to see all models
            if model_input.lower() == 'all':
                # Show ALL models with complete characteristics
                self.console.print(f"\n[bold cyan]📚 ALL {len(MODEL_METADATA)} Available Models:[/bold cyan]\n")

                all_models_table = Table(show_header=True, header_style="bold magenta", border_style="green", box=box.ROUNDED)
                all_models_table.add_column("#", style="yellow", width=4)
                all_models_table.add_column("Model ID", style="cyan", width=40)
                all_models_table.add_column("Languages", style="green", width=15)
                all_models_table.add_column("Max Tokens", style="blue", width=11)
                all_models_table.add_column("Size", style="magenta", width=10)
                all_models_table.add_column("Description", style="white", width=50)

                # Sort models: recommended first, then by relevance
                all_model_ids = list(MODEL_METADATA.keys())
                # Put recommended models at the top
                sorted_model_ids = []
                for model_id in recommended_models_list:
                    if model_id in all_model_ids:
                        sorted_model_ids.append(model_id)
                # Add remaining models
                for model_id in all_model_ids:
                    if model_id not in sorted_model_ids:
                        sorted_model_ids.append(model_id)

                for idx, model_id in enumerate(sorted_model_ids, 1):
                    meta = MODEL_METADATA.get(model_id, {})
                    langs = ', '.join(meta.get('languages', ['?']))
                    max_len = str(meta.get('max_length', '?'))
                    size = meta.get('size', '?')
                    desc = meta.get('description', '')[:48]

                    # Highlight recommended models
                    if model_id in recommended_models_list[:10]:
                        all_models_table.add_row(
                            f"[bold green]{idx}[/bold green]",
                            f"[bold green]{model_id}[/bold green]",
                            langs,
                            max_len,
                            size,
                            desc
                        )
                    else:
                        all_models_table.add_row(str(idx), model_id, langs, max_len, size, desc)

                self.console.print(all_models_table)

                self.console.print(f"\n[dim]💡 [bold green]Green models[/bold green] are in your Top 10 recommendations[/dim]")
                self.console.print(f"\n[bold yellow]Select a model:[/bold yellow]")
                self.console.print(f"[dim]  • Enter the # number from the table[/dim]")
                self.console.print(f"[dim]  • Or enter the model ID directly[/dim]")

                model_input_after_all = Prompt.ask("\nModel to train", default=default_model)

                if model_input_after_all.isdigit():
                    idx = int(model_input_after_all) - 1
                    if 0 <= idx < len(sorted_model_ids):
                        model_name = sorted_model_ids[idx]
                        self.console.print(f"[green]✓ Selected: {model_name}[/green]")
                    else:
                        self.console.print(f"[yellow]⚠️  Invalid selection. Using default: {default_model}[/yellow]")
                        model_name = default_model
                else:
                    model_name = model_input_after_all
            elif model_input.isdigit():
                idx = int(model_input) - 1
                if 0 <= idx < len(recommended_models_list):
                    model_name = recommended_models_list[idx]
                    self.console.print(f"[green]✓ Selected: {model_name}[/green]")
                else:
                    self.console.print(f"[yellow]⚠️  Invalid selection. Using default: {default_model}[/yellow]")
                    model_name = default_model
            else:
                model_name = model_input

            # Display full model details after selection
            self._display_model_details(model_name, MODEL_METADATA)

        # STEP 3: Reinforced Learning
        self.console.print("\n[bold cyan]═══════════════════════════════════════════════════════════════[/bold cyan]")
        self.console.print("[bold cyan]           🎓 STEP 3: Reinforced Learning                      [/bold cyan]")
        self.console.print("[bold cyan]═══════════════════════════════════════════════════════════════[/bold cyan]\n")

        self.console.print("[bold]What is Reinforced Learning?[/bold]")
        self.console.print("  • [cyan]Adaptive retraining[/cyan]: If model underperforms (F1 < threshold), additional training cycles activate")
        self.console.print("  • [cyan]Minority class oversampling[/cyan]: Duplicates minority class samples during training")
        self.console.print("  • [cyan]Adaptive parameters[/cyan]: Automatically adjusts learning rate, batch size, and epochs")
        self.console.print("  • [cyan]Loss correction[/cyan]: Applies class weights to the cross-entropy loss function\n")

        self.console.print("[bold yellow]⚠️  Default Settings (Configurable):[/bold yellow]")
        self.console.print("  • [yellow]F1 Threshold[/yellow]: 0.70 - Triggers reinforced learning when F1 < threshold")
        self.console.print("  • [yellow]Oversampling Factor[/yellow]: 2.0 - Minority class appears 2× more in training")
        self.console.print("  • [yellow]Loss Weight Factor[/yellow]: 2.0 - Minority class errors weighted 2× higher\n")

        self.console.print("[bold]📊 What These Parameters Do:[/bold]")
        self.console.print("  • [green]F1 Threshold[/green]: Lower = More aggressive (activates earlier)")
        self.console.print("    Example: 0.50 → Triggers when model performs poorly")
        self.console.print("    Example: 0.80 → Triggers only for high-performing models")
        self.console.print("  • [green]Oversampling Factor[/green]: How many times to duplicate minority samples")
        self.console.print("    Example: 3.0 → Minority class appears 3× in each epoch")
        self.console.print("  • [green]Loss Weight Factor[/green]: Penalty multiplier for minority class errors")
        self.console.print("    Example: 3.0 → Model penalized 3× more for missing minority samples\n")

        self.console.print("[bold red]Risks & Considerations:[/bold red]")
        self.console.print("  • [yellow]Longer training time[/yellow] (can add 50-100% more time)")
        self.console.print("  • [yellow]Potential overfitting[/yellow] if dataset is very small (<500 samples)")
        self.console.print("  • [yellow]May not help[/yellow] if data quality or quantity is insufficient")
        self.console.print("  • [yellow]High oversampling[/yellow] (>5.0) can cause memorization of minority class\n")

        self.console.print("[yellow]Note:[/yellow] [dim]Compatible with ALL models (BERT, RoBERTa, DeBERTa, etc.)[/dim]\n")

        # Use preloaded value if available
        default_reinforced = preloaded_params.get('reinforced_learning', False) if preloaded_params else False

        enable_reinforced_learning = Confirm.ask(
            "[bold yellow]Enable reinforced learning?[/bold yellow]",
            default=default_reinforced
        )

        # Default reinforced learning parameters
        rl_f1_threshold = 0.70
        rl_oversample_factor = 2.0
        rl_class_weight_factor = 2.0
        manual_rl_epochs = None  # Initialize here to avoid UnboundLocalError

        if enable_reinforced_learning:
            # Ask if user wants to configure parameters
            configure_rl = Confirm.ask(
                "\n[bold cyan]Configure reinforced learning parameters manually?[/bold cyan]\n"
                "[dim](Choose 'n' to use recommended defaults)[/dim]",
                default=False
            )

            if configure_rl:
                self.console.print("\n[bold green]⚙️  Manual Configuration[/bold green]\n")

                # F1 Threshold
                self.console.print("[bold]1️⃣  F1 Activation Threshold[/bold]")
                self.console.print("   [dim]When F1-score drops below this value, reinforced learning activates[/dim]")
                self.console.print("   • Recommended: [green]0.70[/green] (moderate)")
                self.console.print("   • Conservative: [yellow]0.50[/yellow] (only very poor models)")
                self.console.print("   • Aggressive: [yellow]0.85[/yellow] (triggers early)\n")

                f1_input = Prompt.ask(
                    "F1 threshold",
                    default="0.70"
                )
                try:
                    rl_f1_threshold = float(f1_input)
                    if rl_f1_threshold < 0 or rl_f1_threshold > 1:
                        self.console.print("[yellow]⚠️  F1 must be between 0 and 1. Using default 0.70[/yellow]")
                        rl_f1_threshold = 0.70
                except ValueError:
                    self.console.print("[yellow]⚠️  Invalid input. Using default 0.70[/yellow]")
                    rl_f1_threshold = 0.70

                # Oversampling Factor
                self.console.print("\n[bold]2️⃣  Minority Class Oversampling Factor[/bold]")
                self.console.print("   [dim]How many times to duplicate minority class samples during training[/dim]")
                self.console.print("   • Recommended: [green]2.0[/green] (doubles minority samples)")
                self.console.print("   • Light: [yellow]1.5[/yellow] (50% increase)")
                self.console.print("   • Heavy: [yellow]4.0[/yellow] (4× minority samples)")
                self.console.print("   • [red]⚠️  Values > 5.0 risk overfitting[/red]\n")

                oversample_input = Prompt.ask(
                    "Oversampling factor",
                    default="2.0"
                )
                try:
                    rl_oversample_factor = float(oversample_input)
                    if rl_oversample_factor < 1.0:
                        self.console.print("[yellow]⚠️  Factor must be ≥ 1.0. Using default 2.0[/yellow]")
                        rl_oversample_factor = 2.0
                    elif rl_oversample_factor > 5.0:
                        self.console.print("[yellow]⚠️  Warning: High values (>5.0) may cause overfitting[/yellow]")
                except ValueError:
                    self.console.print("[yellow]⚠️  Invalid input. Using default 2.0[/yellow]")
                    rl_oversample_factor = 2.0

                # Class Weight Factor
                self.console.print("\n[bold]3️⃣  Cross-Entropy Loss Weight Factor[/bold]")
                self.console.print("   [dim]Penalty multiplier for misclassifying minority class samples[/dim]")
                self.console.print("   • Recommended: [green]2.0[/green] (2× penalty for minority errors)")
                self.console.print("   • Light: [yellow]1.5[/yellow] (50% higher penalty)")
                self.console.print("   • Heavy: [yellow]4.0[/yellow] (4× penalty)")
                self.console.print("   • [red]⚠️  Values > 5.0 may destabilize training[/red]\n")

                weight_input = Prompt.ask(
                    "Loss weight factor",
                    default="2.0"
                )
                try:
                    rl_class_weight_factor = float(weight_input)
                    if rl_class_weight_factor < 1.0:
                        self.console.print("[yellow]⚠️  Factor must be ≥ 1.0. Using default 2.0[/yellow]")
                        rl_class_weight_factor = 2.0
                    elif rl_class_weight_factor > 5.0:
                        self.console.print("[yellow]⚠️  Warning: High values (>5.0) may destabilize training[/yellow]")
                except ValueError:
                    self.console.print("[yellow]⚠️  Invalid input. Using default 2.0[/yellow]")
                    rl_class_weight_factor = 2.0

                # Reinforced Epochs
                self.console.print("\n[bold]4️⃣  Reinforced Learning Epochs[/bold]")
                self.console.print("   [dim]Number of additional epochs to run when F1 < threshold[/dim]")
                self.console.print("   • Default: [green]Auto-calculated[/green] (8-20 epochs based on model type)")
                self.console.print("   • Manual: [yellow]Choose fixed number[/yellow] (applies to all models)\n")

                use_auto_epochs = Confirm.ask(
                    "Use auto-calculated epochs?",
                    default=True
                )

                manual_rl_epochs = None
                if not use_auto_epochs:
                    manual_rl_epochs = IntPrompt.ask(
                        "[bold yellow]Reinforced epochs[/bold yellow]",
                        default=10
                    )

                # Summary
                self.console.print("\n[bold green]✓ Reinforced Learning Configuration:[/bold green]")
                self.console.print(f"  • F1 Threshold: [cyan]{rl_f1_threshold:.2f}[/cyan]")
                self.console.print(f"  • Oversampling Factor: [cyan]{rl_oversample_factor:.1f}×[/cyan]")
                self.console.print(f"  • Loss Weight Factor: [cyan]{rl_class_weight_factor:.1f}×[/cyan]")
                if manual_rl_epochs:
                    self.console.print(f"  • Reinforced Epochs: [cyan]{manual_rl_epochs}[/cyan] (manual)")
                else:
                    self.console.print(f"  • Reinforced Epochs: [cyan]Auto-calculated[/cyan]")
                self.console.print()
            else:
                self.console.print("\n[green]✓ Using recommended defaults (F1=0.70, Oversample=2.0×, Weight=2.0×)[/green]\n")

                # Ask if user wants to configure RL epochs manually (like in benchmark mode)
                configure_rl_epochs = Confirm.ask(
                    "[bold yellow]Configure reinforced learning epochs manually?[/bold yellow]\n"
                    "[dim](Default: auto-calculated based on model performance)[/dim]",
                    default=False
                )

                if configure_rl_epochs:
                    self.console.print("\n[bold cyan]ℹ️  Reinforced Learning Epochs:[/bold cyan]")
                    self.console.print("[dim]These epochs will be used when F1 < {:.2f}[/dim]".format(rl_f1_threshold))
                    self.console.print("[dim]Auto-calculation typically uses 8-20 epochs based on model type[/dim]\n")

                    manual_rl_epochs = IntPrompt.ask(
                        "[bold yellow]Reinforced epochs[/bold yellow]",
                        default=10
                    )

                    self.console.print(f"[green]✓ Manual reinforced epochs set to: {manual_rl_epochs}[/green]\n")
                else:
                    self.console.print("[green]✓ Reinforced learning epochs will be auto-calculated[/green]\n")
                    manual_rl_epochs = None

        # STEP 4: Epochs
        self.console.print("\n[bold cyan]═══════════════════════════════════════════════════════════════[/bold cyan]")
        self.console.print("[bold cyan]           ⏱️  STEP 4: Training Epochs                           [/bold cyan]")
        self.console.print("[bold cyan]═══════════════════════════════════════════════════════════════[/bold cyan]\n")

        self.console.print("[bold]What are Epochs?[/bold]")
        self.console.print("  • [cyan]One epoch[/cyan] = One complete pass through your entire training dataset")
        self.console.print("  • [cyan]More epochs[/cyan] = Model sees and learns from data more times")
        self.console.print("  • [cyan]Typical range[/cyan]: 3-15 epochs for BERT-like models\n")

        self.console.print("[bold]Guidelines:[/bold]")
        self.console.print("  • [green]Small dataset (<1000 samples)[/green]: 10-15 epochs recommended")
        self.console.print("  • [green]Medium dataset (1000-10000)[/green]: 5-10 epochs recommended")
        self.console.print("  • [green]Large dataset (>10000)[/green]: 3-5 epochs recommended\n")

        self.console.print("[bold green]💾 Automatic Best Model Checkpointing:[/bold green]")
        self.console.print("  • [cyan]Don't worry about setting too many epochs![/cyan]")
        self.console.print("  • The [bold]BEST model[/bold] is automatically saved during training")
        self.console.print("  • System monitors [yellow]validation F1 score[/yellow] after each epoch")
        self.console.print("  • Only the checkpoint with [bold green]highest F1[/bold green] is kept")
        self.console.print("  • Early stopping prevents overfitting automatically\n")

        self.console.print("[dim]💡 Example: You set 15 epochs, but best F1 was at epoch 8 → Model from epoch 8 is used[/dim]\n")

        # Use preloaded value if available
        default_epochs = preloaded_params.get('epochs', 10) if preloaded_params else 10

        epochs = IntPrompt.ask("[bold yellow]Number of epochs[/bold yellow]", default=default_epochs)

        # Prepare return dict
        result = {
            'model_name': model_name,
            'reinforced_learning': enable_reinforced_learning,
            'epochs': epochs,
            # Reinforced learning parameters
            'rl_f1_threshold': rl_f1_threshold,
            'rl_oversample_factor': rl_oversample_factor,
            'rl_class_weight_factor': rl_class_weight_factor,
            'manual_rl_epochs': manual_rl_epochs if manual_rl_epochs else None
        }

        # Include models_by_language if training per-language
        if train_by_language and models_by_language:
            result['models_by_language'] = models_by_language
            result['train_by_language'] = True

        return result

    def _training_studio_run_quick(self, bundle: TrainingDataBundle, model_config: Dict[str, Any], quick_params: Optional[Dict[str, Any]] = None, session_id: Optional[str] = None) -> Dict[str, Any]:
        """
        Quick training mode - simple and fast with sensible defaults.

        Args:
            bundle: Training data bundle
            model_config: Model configuration dict (will be updated with runtime params)
            quick_params: Pre-collected parameters (model_name, reinforced_learning, epochs)
            session_id: Session timestamp for organizing logs by session

        Returns:
            dict with keys: 'runtime_params', 'models_trained', 'best_model', 'best_f1'
        """
        self.console.print("\n[bold]Quick training[/bold] - using configured parameters.")

        # CRITICAL: Log session management for debugging
        self.logger.info("="*80)
        self.logger.info("SESSION MANAGEMENT - FULL TRAINING")
        self.logger.info(f"  session_id (passed to function): {session_id}")
        self.logger.info(f"  self.current_session_id: {getattr(self, 'current_session_id', 'NOT SET')}")
        if session_id and hasattr(self, 'current_session_id') and session_id == self.current_session_id:
            self.logger.info("  ✓ Full training REUSING same session_id as benchmark")
            self.logger.info(f"  Models will be saved to: models/{session_id}/normal_training/")
            self.logger.info(f"  Logs will be saved to: logs/training_arena/{session_id}/training_metrics/normal_training/")
        elif session_id:
            self.logger.warning("  ⚠️  session_id provided but differs from self.current_session_id")
            self.logger.info(f"  Models will be saved to: models/{session_id}/normal_training/")
        else:
            self.logger.warning("  ⚠️  No session_id provided - will create new one (BAD!)")
        self.logger.info("="*80)
        if session_id:
            self.console.print(f"\n[cyan]📂 Session ID:[/cyan] [bold]{session_id}[/bold]")
            self.console.print(f"[dim]All trained models will be saved to: models/{session_id}/normal_training/[/dim]\n")

        # Use parameters from quick_params (already collected before config summary)
        if quick_params:
            # CRITICAL: Debug log to capture exact type of models_by_language from quick_params
            self.logger.debug(f"quick_params keys: {quick_params.keys()}")
            if 'models_by_language' in quick_params:
                self.logger.debug(f"models_by_language type in quick_params: {type(quick_params['models_by_language'])}")
                self.logger.debug(f"models_by_language value in quick_params: {quick_params['models_by_language']}")

            model_name = quick_params['model_name']
            epochs = quick_params['epochs']
            enable_reinforced_learning = quick_params['reinforced_learning']
            models_by_language = quick_params.get('models_by_language', None)
            train_by_language_flag = quick_params.get('train_by_language', False)
            manual_rl_epochs = quick_params.get('manual_rl_epochs', None)
            rl_f1_threshold = quick_params.get('rl_f1_threshold', 0.70)
        else:
            # Fallback for legacy resume mode
            model_name = model_config.get('quick_model_name', 'bert-base-uncased')
            epochs = model_config.get('quick_epochs', 10)
            enable_reinforced_learning = model_config.get('use_reinforcement', False)
            models_by_language = None
            train_by_language_flag = False
            manual_rl_epochs = None
            rl_f1_threshold = 0.70

        # ============================================================
        # CRITICAL: Validate and filter insufficient labels BEFORE training
        # MUST happen AFTER extracting train_by_language_flag
        # ============================================================
        if bundle.primary_file:
            try:
                filtered_file, was_filtered = self._validate_and_filter_insufficient_labels(
                    input_file=str(bundle.primary_file),
                    strategy=bundle.strategy,
                    min_samples=2,
                    auto_remove=False,  # Ask user for confirmation
                    train_by_language=train_by_language_flag  # CRITICAL: Language-aware validation
                )
                if was_filtered:
                    # Update bundle to use filtered file
                    bundle.primary_file = Path(filtered_file)
                    self.console.print(f"[green]✓ Using filtered training dataset[/green]\n")
            except ValueError as e:
                # User cancelled or validation failed
                self.console.print(f"[red]{e}[/red]")
                return {
                    'runtime_params': {},
                    'models_trained': [],
                    'best_model': None,
                    'best_f1': None,
                    'error': str(e)
                }
            except Exception as e:
                self.logger.warning(f"Label validation failed: {e}")
                # Continue with original file if validation fails
                pass

        # Display training configuration summary
        self.console.print()

        # CRITICAL: Validate models_by_language type before using len()
        if models_by_language and not isinstance(models_by_language, dict):
            self.console.print(f"[red]⚠️  ERROR: models_by_language has invalid type: {type(models_by_language)}[/red]")
            self.logger.error(f"models_by_language type error: {type(models_by_language)}, value: {models_by_language}")
            models_by_language = None  # Reset to None to prevent crash

        if models_by_language:
            self.console.print(f"  • Models: [cyan]{len(models_by_language)}[/cyan] (language-specific)")
        else:
            self.console.print(f"  • Model: [cyan]{model_name}[/cyan]")

        # Display epochs with reinforced learning info if enabled
        if enable_reinforced_learning:
            if manual_rl_epochs is not None:
                # Manual reinforced epochs configured
                max_epochs = epochs + manual_rl_epochs
                self.console.print(f"  • Epochs: [cyan]{epochs}[/cyan] (up to [yellow]{max_epochs}[/yellow] with reinforced learning)")
            else:
                # Auto-calculated reinforced epochs (typically 8-20)
                self.console.print(f"  • Epochs: [cyan]{epochs}[/cyan] (up to [yellow]{epochs}+auto[/yellow] with reinforced learning)")
            self.console.print(f"  • Reinforced learning: [cyan]Enabled[/cyan] (F1 < {rl_f1_threshold:.2f})")
        else:
            self.console.print(f"  • Epochs: [cyan]{epochs}[/cyan]")
        self.console.print()

        # Get languages from metadata (needed for training)
        languages = set()
        if hasattr(bundle, 'metadata') and bundle.metadata:
            languages = bundle.metadata.get('confirmed_languages', bundle.metadata.get('languages', set()))
        if not languages and hasattr(bundle, 'languages') and bundle.languages:
            languages = set([lang.upper() for lang in bundle.languages])
        if languages:
            languages = set([str(lang).upper() for lang in languages])

        # Capture runtime parameters for full reproducibility
        if models_by_language:
            # Per-language models selected
            runtime_params = {
                'quick_models_by_language': models_by_language,
                'quick_epochs': epochs,
                'reinforced_learning': enable_reinforced_learning,
                'actual_models_trained': list(models_by_language.values())
            }
        else:
            # Single model for all languages
            runtime_params = {
                'quick_model_name': model_name,
                'quick_epochs': epochs,
                'reinforced_learning': enable_reinforced_learning,
                'actual_models_trained': [model_name]
            }

        # CRITICAL: DO NOT create a new timestamped directory for Training Arena.
        # Models are saved using session_id which is passed in the config to bert_base.py.
        # The output_dir is only used as a fallback placeholder for save_model_as.
        # Real path: models/{session_id}/normal_training/{category}/{language}/{model}/
        # This ensures benchmark and full training use THE SAME session folder.
        output_dir = Path("models") / "placeholder_not_used"

        # Initialize multiclass_groups (will be set if detected)
        multiclass_groups = None

        # CRITICAL: Extract training_approach BEFORE the multi-label block so it's accessible later
        training_approach_from_metadata = bundle.metadata.get('training_approach') if hasattr(bundle, 'metadata') else None

        # For multi-label, check if it's actually multi-class
        if bundle.strategy == "multi-label":
            # Load data to check structure
            from llm_tool.trainers.multi_label_trainer import MultiLabelTrainer, TrainingConfig as MultiLabelTrainingConfig
            ml_trainer = MultiLabelTrainer(config=MultiLabelTrainingConfig(), verbose=False)

            # Use primary_file for one-vs-all, dataset_path otherwise
            data_path = str(bundle.primary_file) if hasattr(bundle, 'primary_file') else str(bundle.dataset_path)

            samples = ml_trainer.load_multi_label_data(
                data_path,
                text_field=bundle.text_column,
                label_fields=None,  # Will auto-detect
                id_field=bundle.id_column if hasattr(bundle, 'id_column') else None,
                lang_field=bundle.lang_column if hasattr(bundle, 'lang_column') else None,
                labels_dict_field=bundle.label_column if hasattr(bundle, 'label_column') else 'labels'
            )

            # Detect multi-class groups
            multiclass_groups = ml_trainer.detect_multiclass_groups(samples)

            # Check if user already answered this question during dataset building
            use_multiclass_training = False

            if multiclass_groups:
                if training_approach_from_metadata == 'multi-class':
                    # User already chose multi-class during dataset building
                    use_multiclass_training = True
                    self.console.print("\n[green]✓ Using multi-class training (from dataset configuration)[/green]\n")
                elif training_approach_from_metadata == 'one-vs-all':
                    # User already chose one-vs-all during dataset building
                    use_multiclass_training = False
                    multiclass_groups = None
                    self.console.print("\n[yellow]✓ Using one-vs-all training (from dataset configuration)[/yellow]\n")
                elif training_approach_from_metadata in ['hybrid', 'custom']:
                    # User already chose hybrid/custom - will be handled later in dedicated section
                    use_multiclass_training = False
                    multiclass_groups = None
                    self.console.print(f"\n[cyan]✓ Using {training_approach_from_metadata} training (from dataset configuration)[/cyan]\n")
                else:
                    # No previous choice - ask user
                    self.console.print("\n[yellow]ℹ️  Detected multi-class classification:[/yellow]")
                    for group_name, labels in multiclass_groups.items():
                        value_names = [lbl[len(group_name)+1:] if lbl.startswith(group_name+'_') else lbl for lbl in labels]
                        self.console.print(f"  • {group_name}: {', '.join(value_names)}")

                    # Ask user if they want true multi-class (1 model) or one-vs-all (N models)
                    self.console.print("\n[bold]Training approach:[/bold]")
                    self.console.print("  • [green]Multi-class[/green]: Train 1 model per group to predict among all classes")
                    self.console.print("  • [yellow]One-vs-all[/yellow]: Train N separate binary models (1 per class)")

                    use_multiclass_training = Confirm.ask(
                        "\n[bold]Use multi-class training? (recommended)[/bold]",
                        default=True
                    )

                    if use_multiclass_training:
                        self.console.print("[green]✓ Will use multi-class training[/green]\n")
                    else:
                        self.console.print("[yellow]✓ Will train separate binary classifiers[/yellow]\n")
                        multiclass_groups = None  # Don't pass to trainer

        # Create TrainingConfig with user's chosen model
        from llm_tool.trainers.model_trainer import TrainingConfig
        training_config = TrainingConfig()
        training_config.model_name = model_name
        training_config.num_epochs = epochs

        # Determine if we need to train by language
        needs_language_training = False

        if models_by_language:
            # User selected different models for each language
            needs_language_training = True
            self.console.print(f"\n[yellow]🌍 Multi-language training enabled:[/yellow]")
            self.console.print(f"[dim]Training with specialized models for each language:[/dim]")
            for lang in sorted(models_by_language.keys()):
                self.console.print(f"  • {lang.upper()}: {models_by_language[lang]}")
        else:
            # Single model - check if it's monolingual and we have multiple languages
            is_multilingual = self._is_model_multilingual(model_name)
            needs_language_training = not is_multilingual and len(languages) > 1

            if needs_language_training:
                self.console.print(f"\n[yellow]🌍 Multi-language training enabled:[/yellow]")
                self.console.print(f"[dim]The model '{model_name}' is language-specific, so separate models will be trained for each language:[/dim]")
                for lang in sorted(languages):
                    self.console.print(f"  • {lang.upper()}")

        trainer = ModelTrainer(config=training_config)

        # Build trainer config with multiclass_groups if detected
        extra_config = {
            "model_name": model_name,
            "num_epochs": epochs,
            "reinforced_learning": enable_reinforced_learning,  # CRITICAL: Pass reinforced learning setting
            "train_by_language": needs_language_training,
            "confirmed_languages": list(languages) if languages else None,  # Pass all detected languages
            "training_approach": training_approach_from_metadata  # CRITICAL: Pass training approach to prevent multiclass auto-detection for one-vs-all
        }

        # Add reinforced learning parameters if enabled
        if enable_reinforced_learning and quick_params:
            extra_config["rl_f1_threshold"] = quick_params.get('rl_f1_threshold', 0.70)
            extra_config["rl_oversample_factor"] = quick_params.get('rl_oversample_factor', 2.0)
            extra_config["rl_class_weight_factor"] = quick_params.get('rl_class_weight_factor', 2.0)
            # Pass manual reinforced epochs if configured
            if quick_params.get('manual_rl_epochs') is not None:
                extra_config["reinforced_epochs"] = quick_params['manual_rl_epochs']

        # Add models_by_language if user selected per-language models
        if models_by_language:
            extra_config["models_by_language"] = models_by_language

        # Add multiclass_groups if user opted for multi-class training
        # CRITICAL: Do NOT add multiclass_groups if user chose one-vs-all (which uses multi-label infrastructure but creates binary models)
        if bundle.strategy == "multi-label" and multiclass_groups and training_approach_from_metadata != 'one-vs-all':
            extra_config["multiclass_groups"] = multiclass_groups

        # CRITICAL FIX: Handle one-vs-all training properly
        # For one-vs-all, we need to train separate binary models for each label

        # DEBUG logging
        self.logger.debug(f"[ONE-VS-ALL DEBUG] training_approach_from_metadata = {training_approach_from_metadata}")
        self.logger.debug(f"[ONE-VS-ALL DEBUG] hasattr(bundle, 'training_files') = {hasattr(bundle, 'training_files')}")
        if hasattr(bundle, 'training_files'):
            self.logger.debug(f"[ONE-VS-ALL DEBUG] bundle.training_files.keys() = {list(bundle.training_files.keys()) if bundle.training_files else None}")

        if training_approach_from_metadata == 'one-vs-all':
            # One-vs-all training: create separate binary models for each label

            # First, try to use pre-generated category CSV files if they exist
            category_files = {}
            if hasattr(bundle, 'training_files') and bundle.training_files:
                # Extract the category files (exclude 'multilabel' key)
                category_files = {k: v for k, v in bundle.training_files.items() if k != 'multilabel'}

            # If no category files exist, create them from the JSONL file
            if not category_files:
                self.console.print("\n[yellow]⚡ Creating binary datasets for one-vs-all training...[/yellow]")

                # Load the JSONL file to extract labels
                import json
                data_path = str(bundle.primary_file) if hasattr(bundle, 'primary_file') else str(bundle.dataset_path)

                # Read the JSONL and collect unique labels
                all_labels_set = set()
                records = []
                with open(data_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        record = json.loads(line)
                        records.append(record)
                        if 'labels' in record:
                            # Handle both list and dict formats
                            if isinstance(record['labels'], dict):
                                all_labels_set.update(record['labels'].keys())
                            elif isinstance(record['labels'], list):
                                all_labels_set.update(record['labels'])

                self.logger.debug(f"[ONE-VS-ALL] Found {len(records)} records")
                self.logger.debug(f"[ONE-VS-ALL] Found {len(all_labels_set)} unique labels: {sorted(all_labels_set)}")

                if not all_labels_set:
                    # Debug: print first record to see the structure
                    if records:
                        self.logger.error(f"[ONE-VS-ALL] No labels found! First record structure: {records[0]}")
                        self.console.print(f"\n[red]✗ Could not find labels in JSONL file[/red]")
                        self.console.print(f"[dim]First record structure: {json.dumps(records[0], indent=2)}[/dim]")
                    return {
                        'runtime_params': runtime_params,
                        'models_trained': [],
                        'best_model': None,
                        'best_f1': None,
                        'error': 'No labels found in JSONL'
                    }

                # Create temporary CSV files for each label
                import tempfile
                import csv
                temp_dir = Path(tempfile.mkdtemp(prefix="onevsall_"))

                # Get filter logger for tracking (with session context if available)
                filter_logger = get_filter_logger(session_id=getattr(self, 'current_session_id', None))
                location = "advanced_cli.one_vs_all_binary_dataset_creation"

                for label_name in sorted(all_labels_set):
                    # Create binary CSV: text + label (0 or 1)
                    csv_path = temp_dir / f"binary_{label_name}.csv"

                    # Track filtered items for this label
                    filtered_empty_texts = []
                    filtered_invalid_texts = []
                    written_count = 0

                    with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:
                        writer = csv.DictWriter(csvfile, fieldnames=['text', 'label', 'language'])
                        writer.writeheader()

                        for idx, record in enumerate(records):
                            # Binary label: 1 if this label is present/True, 0 otherwise
                            labels_data = record.get('labels', {})

                            # Handle both dict and list formats
                            if isinstance(labels_data, dict):
                                label_raw = labels_data.get(label_name, 0)
                                # Handle bool, int, or string values
                                if isinstance(label_raw, bool):
                                    label_value = 1 if label_raw else 0
                                elif isinstance(label_raw, (int, float)):
                                    label_value = 1 if label_raw > 0 else 0
                                else:
                                    label_value = 1 if str(label_raw).lower() in ['1', 'true', 'yes'] else 0
                            elif isinstance(labels_data, list):
                                # For list format, check if label is in the list
                                label_value = 1 if label_name in labels_data else 0
                            else:
                                label_value = 0

                            # CRITICAL: Validate text is a valid non-empty string
                            text_raw = record.get('text', '')
                            if not isinstance(text_raw, str):
                                # Log invalid type
                                filtered_invalid_texts.append({
                                    'index': idx,
                                    'type': type(text_raw).__name__,
                                    'value': str(text_raw)[:100] if text_raw else 'None'
                                })
                                text_raw = str(text_raw) if text_raw else ''

                            # Skip empty texts
                            if not text_raw.strip():
                                filtered_empty_texts.append({
                                    'index': idx,
                                    'id': record.get('id', 'unknown'),
                                    'text_length': len(text_raw)
                                })
                                continue

                            # Ensure language is a string
                            lang_raw = record.get('lang', record.get('language', ''))
                            if not isinstance(lang_raw, str):
                                lang_raw = str(lang_raw) if lang_raw else ''

                            row = {
                                'text': text_raw.strip(),
                                'label': label_value,
                                'language': lang_raw
                            }
                            writer.writerow(row)
                            written_count += 1

                    # Log filtered items
                    if filtered_empty_texts:
                        filter_logger.log_filtered_batch(
                            items=[f"Record {f['index']} (id: {f['id']})" for f in filtered_empty_texts],
                            reason="empty_text",
                            location=f"{location}.{label_name}",
                            indices=[f['index'] for f in filtered_empty_texts]
                        )

                    if filtered_invalid_texts:
                        filter_logger.log_filtered_batch(
                            items=[f"Record {f['index']}: {f['type']}" for f in filtered_invalid_texts],
                            reason="invalid_text_type",
                            location=f"{location}.{label_name}",
                            indices=[f['index'] for f in filtered_invalid_texts]
                        )

                    category_files[label_name] = csv_path
                    self.console.print(f"[dim]  Created binary dataset for: {label_name} ({written_count} samples)[/dim]")

                    # Warn if too many filtered
                    total_filtered = len(filtered_empty_texts) + len(filtered_invalid_texts)
                    if total_filtered > 0:
                        self.console.print(f"[yellow]    ⚠️  Filtered {total_filtered} invalid/empty texts[/yellow]")

                self.console.print(f"[green]✓ Created {len(category_files)} binary datasets[/green]\n")

            if category_files:
                self.console.print(f"\n[yellow]⚠️  One-vs-all requires training {len(category_files)} separate binary models.[/yellow]")
                self.console.print("[dim]   Note: 'distributed' training mode exists but is NOT RECOMMENDED (untested).[/dim]")
                self.console.print("[yellow]   Quick mode will train them sequentially...[/yellow]\n")

                # Initialize global progress tracking for one-vs-all training
                import time
                global_start_time = time.time()

                # CRITICAL: Calculate total models based on training approach
                # One-vs-all creates one binary model per category
                # If needs_language_training=True, we train one model PER (category, language)
                num_categories = int(len(category_files))
                num_languages = int(len(languages)) if languages else 1

                if needs_language_training and num_languages > 1:
                    # Per-language training: one model per (category, language) combination
                    global_total_models = int(num_categories * num_languages)
                    self.logger.info(f"[EPOCH CALC] One-vs-all + per-language: {num_categories} categories × {num_languages} languages = {global_total_models} total models")
                else:
                    # Multilingual model: one model per category (handles all languages)
                    global_total_models = int(num_categories)
                    self.logger.info(f"[EPOCH CALC] One-vs-all + multilingual: {num_categories} categories = {global_total_models} total models")

                epochs = int(epochs) if epochs is not None else 10
                manual_rl_epochs = int(manual_rl_epochs) if manual_rl_epochs is not None else None
                global_total_epochs = int(global_total_models * epochs)

                # Calculate maximum possible epochs (if all models trigger reinforced learning)
                if enable_reinforced_learning and manual_rl_epochs is not None:
                    global_max_epochs = int(global_total_models * (epochs + manual_rl_epochs))
                else:
                    global_max_epochs = int(global_total_epochs)

                # DEBUGGING: Log the epoch calculation
                self.logger.info("="*80)
                self.logger.info("GLOBAL EPOCHS CALCULATION DEBUG")
                self.logger.info(f"  Training mode: one-vs-all")
                self.logger.info(f"  Language training: {'per-language' if needs_language_training else 'multilingual'}")
                self.logger.info(f"  Number of categories: {num_categories}")
                self.logger.info(f"  Number of languages: {num_languages}")
                self.logger.info(f"  Languages: {sorted(languages) if languages else 'N/A'}")
                self.logger.info(f"  Base epochs per model: {epochs}")
                self.logger.info(f"  RL epochs per model: {manual_rl_epochs if manual_rl_epochs else 'None'}")
                self.logger.info(f"  CALCULATED global_total_models: {global_total_models}")
                self.logger.info(f"  CALCULATED global_total_epochs: {global_total_epochs}")
                self.logger.info(f"  CALCULATED global_max_epochs: {global_max_epochs}")
                self.logger.info("="*80)

                global_completed_epochs = int(0)

                # Train each binary model sequentially
                results_per_category = {}
                for idx, (category_name, category_file) in enumerate(category_files.items(), 1):
                    self.console.print(f"\n[cyan]Training binary model for: {category_name}[/cyan]")

                    # Create config for this specific category
                    # CRITICAL: Convert all numeric values to Python int to avoid numpy.int64 issues
                    category_config = {
                        'input_file': str(category_file),
                        'text_column': 'text',
                        'label_column': 'label',
                        'model_name': model_name,
                        'num_epochs': int(epochs),
                        'reinforced_learning': enable_reinforced_learning,  # CRITICAL: Pass reinforced learning setting
                        'output_dir': str(Path(output_dir) / f'model_{category_name}'),
                        'training_strategy': 'single-label',  # Binary classification
                        'category_name': category_name,  # For display in metrics
                        'confirmed_languages': list(languages) if languages else None,
                        'train_by_language': needs_language_training,
                        'session_id': session_id,
                        'split_config': bundle.metadata.get('split_config') if hasattr(bundle, 'metadata') else None,
                        # Global progress tracking - ALL converted to Python int
                        'global_total_models': int(global_total_models),
                        'global_current_model': int(idx),
                        'global_total_epochs': int(global_total_epochs),
                        'global_max_epochs': int(global_max_epochs),
                        'global_completed_epochs': int(global_completed_epochs),
                        'global_start_time': global_start_time
                    }

                    # Add reinforced learning parameters if enabled
                    if enable_reinforced_learning and manual_rl_epochs is not None:
                        category_config["reinforced_epochs"] = int(manual_rl_epochs)

                    # Add models_by_language if user selected per-language models
                    # CRITICAL: Validate type before passing to avoid numpy type errors
                    if models_by_language:
                        if not isinstance(models_by_language, dict):
                            self.console.print(f"[red]⚠️  ERROR: models_by_language has invalid type: {type(models_by_language)}[/red]")
                            self.logger.error(f"one-vs-all: models_by_language type error: {type(models_by_language)}, value: {models_by_language}")
                        else:
                            category_config["models_by_language"] = models_by_language

                    # CRITICAL DEBUG: Log category_config to detect numpy types
                    self.logger.debug("=" * 80)
                    self.logger.debug(f"category_config for {category_name}:")
                    for key, value in category_config.items():
                        self.logger.debug(f"  {key}: type={type(value)}, value={value}")
                    self.logger.debug("=" * 80)

                    try:
                        category_result = trainer.train(category_config)
                        results_per_category[category_name] = category_result
                        self.console.print(f"[green]✓ Completed {category_name}: Accuracy={category_result.get('accuracy', 0):.4f}, F1={category_result.get('best_f1_macro', 0):.4f}[/green]")
                    except Exception as exc:
                        self.console.print(f"[red]✗ Failed to train {category_name}: {exc}[/red]")
                        self.logger.exception(f"Training failed for {category_name}", exc_info=exc)
                        # CRITICAL: Log full traceback
                        import traceback
                        self.logger.error(f"Full traceback:\n{traceback.format_exc()}")
                        results_per_category[category_name] = {'error': str(exc)}
                        # CRITICAL: Re-raise to see actual error
                        raise

                # Aggregate results
                successful_results = [r for r in results_per_category.values() if 'error' not in r]
                if successful_results:
                    avg_accuracy = sum(r.get('accuracy', 0) for r in successful_results) / len(successful_results)
                    avg_f1 = sum(r.get('best_f1_macro', 0) for r in successful_results) / len(successful_results)

                    result = {
                        'best_model': model_name,
                        'accuracy': avg_accuracy,
                        'best_f1_macro': avg_f1,
                        'model_path': str(output_dir),
                        'training_time': sum(r.get('training_time', 0) for r in successful_results),
                        'models_trained': len(successful_results),
                        'total_models': len(category_files),
                        'per_category_results': results_per_category
                    }
                else:
                    self.console.print("[red]All category trainings failed[/red]")
                    return {
                        'runtime_params': runtime_params,
                        'models_trained': [],
                        'best_model': None,
                        'best_f1': None,
                        'error': 'All category trainings failed'
                    }
            else:
                self.console.print("[red]No category files found for one-vs-all training[/red]")
                return {
                    'runtime_params': runtime_params,
                    'models_trained': [],
                    'best_model': None,
                    'best_f1': None,
                    'error': 'No category files'
                }
        elif training_approach_from_metadata in ['hybrid', 'custom'] and hasattr(bundle, 'training_files') and bundle.training_files:
            # Hybrid/Custom training: mix of multi-class and one-vs-all per key
            multiclass_keys = bundle.metadata.get('multiclass_keys', [])
            onevsall_keys = bundle.metadata.get('onevsall_keys', [])

            self.console.print(f"\n[cyan]🔀 Hybrid/Custom training:[/cyan]")
            self.console.print(f"  • {len(multiclass_keys)} keys with multi-class strategy")
            self.console.print(f"  • {len(onevsall_keys)} keys with one-vs-all strategy\n")

            # Initialize global progress tracking for hybrid/custom training
            import time
            global_start_time = time.time()

            # CRITICAL: Calculate total models for hybrid/custom training
            # Multi-class keys: 1 model per key (handles all classes in that key)
            # One-vs-all keys: N models per key (1 per class/category in that key)
            num_multiclass_models = len(multiclass_keys)
            num_onevsall_models = 0
            if onevsall_keys:
                # Count total categories across all one-vs-all keys
                for key in onevsall_keys:
                    if key in bundle.training_files:
                        # Each one-vs-all key creates multiple binary models
                        # TODO: This is approximate - actual count depends on number of categories per key
                        num_onevsall_models += 1  # Placeholder - needs refinement

            # If per-language training, multiply by number of languages
            num_languages = int(len(languages)) if languages else 1
            if needs_language_training and num_languages > 1:
                global_total_models = (num_multiclass_models + num_onevsall_models) * num_languages
                self.logger.info(f"[EPOCH CALC] Hybrid/custom + per-language: ({num_multiclass_models} + {num_onevsall_models}) × {num_languages} languages = {global_total_models} total models")
            else:
                global_total_models = num_multiclass_models + num_onevsall_models
                self.logger.info(f"[EPOCH CALC] Hybrid/custom + multilingual: {num_multiclass_models} + {num_onevsall_models} = {global_total_models} total models")

            global_total_epochs = global_total_models * epochs

            # Calculate maximum possible epochs (if all models trigger reinforced learning)
            if enable_reinforced_learning and manual_rl_epochs is not None:
                global_max_epochs = global_total_models * (epochs + manual_rl_epochs)
            else:
                global_max_epochs = global_total_epochs

            # DEBUGGING: Log the epoch calculation
            self.logger.info("="*80)
            self.logger.info("GLOBAL EPOCHS CALCULATION DEBUG")
            self.logger.info(f"  Training mode: hybrid/custom")
            self.logger.info(f"  Language training: {'per-language' if needs_language_training else 'multilingual'}")
            self.logger.info(f"  Multiclass keys: {num_multiclass_models}")
            self.logger.info(f"  One-vs-all keys: {num_onevsall_models}")
            self.logger.info(f"  Number of languages: {num_languages}")
            self.logger.info(f"  Languages: {sorted(languages) if languages else 'N/A'}")
            self.logger.info(f"  Base epochs per model: {epochs}")
            self.logger.info(f"  RL epochs per model: {manual_rl_epochs if manual_rl_epochs else 'None'}")
            self.logger.info(f"  CALCULATED global_total_models: {global_total_models}")
            self.logger.info(f"  CALCULATED global_total_epochs: {global_total_epochs}")
            self.logger.info(f"  CALCULATED global_max_epochs: {global_max_epochs}")
            self.logger.info("="*80)

            global_completed_epochs = 0

            results_per_key = {}

            # Train multi-class keys (one model per key)
            key_files = {k: v for k, v in bundle.training_files.items() if k in multiclass_keys}
            for idx, (key_name, key_file_path) in enumerate(key_files.items(), 1):
                self.console.print(f"\n[bold]Training multi-class model for '{key_name}'[/bold] ({key_file_path.name})")

                key_config = {
                    'input_file': str(key_file_path),
                    'model_name': model_name,
                    'num_epochs': epochs,
                    'output_dir': str(output_dir / f"key_{key_name}"),
                    'text_column': bundle.text_column,
                    'label_column': bundle.label_column,
                    'training_strategy': 'single-label',
                    'category_name': key_name,
                    'reinforced_learning': enable_reinforced_learning,
                    'session_id': session_id,
                    'split_config': bundle.metadata.get('split_config') if hasattr(bundle, 'metadata') else None,
                    # Global progress tracking
                    'global_total_models': global_total_models,
                    'global_current_model': idx,
                    'global_total_epochs': global_total_epochs,
                    'global_max_epochs': global_max_epochs,
                    'global_completed_epochs': global_completed_epochs,
                    'global_start_time': global_start_time
                }

                if models_by_language:
                    key_config["models_by_language"] = models_by_language

                try:
                    key_result = trainer.train(key_config)
                    # Update global completed epochs
                    global_completed_epochs = key_result.get('global_completed_epochs', global_completed_epochs)
                    results_per_key[key_name] = key_result
                    self.console.print(f"[green]✓ Completed {key_name}: Accuracy={key_result.get('accuracy', 0):.4f}, F1={key_result.get('best_f1_macro', 0):.4f}[/green]")
                except Exception as exc:
                    self.console.print(f"[red]✗ Failed to train {key_name}: {exc}[/red]")
                    self.logger.exception(f"Training failed for {key_name}", exc_info=exc)
                    results_per_key[key_name] = {'error': str(exc)}

            # Train one-vs-all keys (using MultiLabelTrainer with multiclass_groups detection)
            if onevsall_keys and 'onevsall_multilabel' in bundle.training_files:
                onevsall_file = bundle.training_files['onevsall_multilabel']
                self.console.print(f"\n[bold yellow]Training one-vs-all models for {len(onevsall_keys)} keys[/bold yellow]")

                # Use multi-label trainer for one-vs-all
                onevsall_config = {
                    'input_file': str(onevsall_file),
                    'model_name': model_name,
                    'num_epochs': epochs,
                    'output_dir': str(output_dir / "onevsall"),
                    'text_column': bundle.text_column,
                    'label_column': bundle.label_column,
                    'training_strategy': 'multi-label',  # CRITICAL: Use multi-label trainer
                    'training_approach': 'one-vs-all',  # CRITICAL: Explicitly mark as one-vs-all to prevent multiclass detection
                    'multiclass_groups': None,  # Force one-vs-all
                    'reinforced_learning': enable_reinforced_learning,
                    'confirmed_languages': list(languages) if languages else None,
                    'session_id': session_id,
                    'split_config': bundle.metadata.get('split_config') if hasattr(bundle, 'metadata') else None,
                    # Global progress tracking
                    'global_total_models': global_total_models,
                    'global_current_model': len(multiclass_keys) + 1,
                    'global_total_epochs': global_total_epochs,
                    'global_max_epochs': global_max_epochs,
                    'global_completed_epochs': global_completed_epochs,
                    'global_start_time': global_start_time
                }

                if models_by_language:
                    onevsall_config["models_by_language"] = models_by_language

                try:
                    onevsall_result = trainer.train(onevsall_config)
                    # Update global completed epochs
                    global_completed_epochs = onevsall_result.get('global_completed_epochs', global_completed_epochs)
                    results_per_key['onevsall_combined'] = onevsall_result
                    self.console.print(f"[green]✓ Completed one-vs-all models[/green]")
                except Exception as exc:
                    self.console.print(f"[red]✗ Failed to train one-vs-all models: {exc}[/red]")
                    self.logger.exception(f"One-vs-all training failed", exc_info=exc)
                    results_per_key['onevsall_combined'] = {'error': str(exc)}

            # Aggregate results
            successful_results = [r for r in results_per_key.values() if 'error' not in r]
            if successful_results:
                avg_accuracy = sum(r.get('accuracy', 0) for r in successful_results) / len(successful_results)
                avg_f1 = sum(r.get('best_f1_macro', 0) for r in successful_results) / len(successful_results)

                result = {
                    'best_model': model_name,
                    'accuracy': avg_accuracy,
                    'best_f1_macro': avg_f1,
                    'model_path': str(output_dir),
                    'training_time': sum(r.get('training_time', 0) for r in successful_results),
                    'models_trained': len(successful_results),
                    'training_approach': training_approach_from_metadata,
                    'per_key_results': results_per_key
                }
            else:
                self.console.print("[red]All trainings failed[/red]")
                return {
                    'runtime_params': runtime_params,
                    'models_trained': [],
                    'best_model': None,
                    'best_f1': None,
                    'error': 'All trainings failed'
                }
        elif training_approach_from_metadata == 'multi-class' and hasattr(bundle, 'training_files') and bundle.training_files:
            # Multi-class training with multiple keys: train ONE model PER KEY
            # Extract the key files (exclude 'multilabel' key)
            key_files = {k: v for k, v in bundle.training_files.items() if k != 'multilabel'}

            if key_files:
                self.console.print(f"\n[cyan]🎯 Multi-class training: {len(key_files)} models (one per key)[/cyan]\n")

                # Initialize global progress tracking for multi-class training
                import time
                global_start_time = time.time()

                # CRITICAL: Calculate total models for multi-class training
                # Multi-class: 1 model per key (each model handles all classes in that key)
                num_keys = len(key_files)
                num_languages = int(len(languages)) if languages else 1

                if needs_language_training and num_languages > 1:
                    # Per-language training: one model per (key, language) combination
                    global_total_models = num_keys * num_languages
                    self.logger.info(f"[EPOCH CALC] Multi-class + per-language: {num_keys} keys × {num_languages} languages = {global_total_models} total models")
                else:
                    # Multilingual model: one model per key (handles all languages)
                    global_total_models = num_keys
                    self.logger.info(f"[EPOCH CALC] Multi-class + multilingual: {num_keys} keys = {global_total_models} total models")

                global_total_epochs = global_total_models * epochs

                # Calculate maximum possible epochs (if all models trigger reinforced learning)
                if enable_reinforced_learning and manual_rl_epochs is not None:
                    global_max_epochs = global_total_models * (epochs + manual_rl_epochs)
                else:
                    global_max_epochs = global_total_epochs

                # DEBUGGING: Log the epoch calculation
                self.logger.info("="*80)
                self.logger.info("GLOBAL EPOCHS CALCULATION DEBUG")
                self.logger.info(f"  Training mode: multi-class")
                self.logger.info(f"  Language training: {'per-language' if needs_language_training else 'multilingual'}")
                self.logger.info(f"  Number of keys: {num_keys}")
                self.logger.info(f"  Number of languages: {num_languages}")
                self.logger.info(f"  Languages: {sorted(languages) if languages else 'N/A'}")
                self.logger.info(f"  Base epochs per model: {epochs}")
                self.logger.info(f"  RL epochs per model: {manual_rl_epochs if manual_rl_epochs else 'None'}")
                self.logger.info(f"  CALCULATED global_total_models: {global_total_models}")
                self.logger.info(f"  CALCULATED global_total_epochs: {global_total_epochs}")
                self.logger.info(f"  CALCULATED global_max_epochs: {global_max_epochs}")
                self.logger.info("="*80)

                global_completed_epochs = 0

                results_per_key = {}

                for idx, (key_name, key_file_path) in enumerate(key_files.items(), 1):
                    self.console.print(f"\n[bold]Training model for key '{key_name}'[/bold] ({key_file_path.name})")

                    # Create config for this key
                    key_config = {
                        'input_file': str(key_file_path),
                        'model_name': model_name,
                        'num_epochs': epochs,
                        'output_dir': str(output_dir / f"key_{key_name}"),
                        'text_column': bundle.text_column,
                        'label_column': bundle.label_column,
                        'training_strategy': 'single-label',  # Each key file is single-label
                        'category_name': key_name,
                        'reinforced_learning': enable_reinforced_learning,
                        'session_id': session_id,
                        'split_config': bundle.metadata.get('split_config') if hasattr(bundle, 'metadata') else None,
                        # Global progress tracking
                        'global_total_models': global_total_models,
                        'global_current_model': idx,
                        'global_total_epochs': global_total_epochs,
                        'global_max_epochs': global_max_epochs,
                        'global_completed_epochs': global_completed_epochs,
                        'global_start_time': global_start_time
                    }

                    # Add models_by_language if user selected per-language models
                    if models_by_language:
                        key_config["models_by_language"] = models_by_language

                    try:
                        key_result = trainer.train(key_config)
                        # Update global completed epochs
                        global_completed_epochs = key_result.get('global_completed_epochs', global_completed_epochs)
                        results_per_key[key_name] = key_result
                        self.console.print(f"[green]✓ Completed {key_name}: Accuracy={key_result.get('accuracy', 0):.4f}, F1={key_result.get('best_f1_macro', 0):.4f}[/green]")
                    except Exception as exc:
                        self.console.print(f"[red]✗ Failed to train {key_name}: {exc}[/red]")
                        self.logger.exception(f"Training failed for {key_name}", exc_info=exc)
                        results_per_key[key_name] = {'error': str(exc)}

                # Aggregate results
                successful_results = [r for r in results_per_key.values() if 'error' not in r]
                if successful_results:
                    avg_accuracy = sum(r.get('accuracy', 0) for r in successful_results) / len(successful_results)
                    avg_f1 = sum(r.get('best_f1_macro', 0) for r in successful_results) / len(successful_results)

                    result = {
                        'best_model': model_name,
                        'accuracy': avg_accuracy,
                        'best_f1_macro': avg_f1,
                        'model_path': str(output_dir),
                        'training_time': sum(r.get('training_time', 0) for r in successful_results),
                        'models_trained': len(successful_results),
                        'total_keys': len(key_files),
                        'per_key_results': results_per_key,
                        'training_approach': 'multi-class'
                    }
                else:
                    self.console.print("[red]All key trainings failed[/red]")
                    return {
                        'runtime_params': runtime_params,
                        'models_trained': [],
                        'best_model': None,
                        'best_f1': None,
                        'error': 'All key trainings failed'
                    }
            else:
                self.console.print("[yellow]⚠️  No key files found, falling back to standard multi-label training[/yellow]")

                # ============================================================
                # CRITICAL: Validate and filter insufficient labels BEFORE training
                # ============================================================
                input_file_to_use = str(bundle.primary_file)
                if bundle.primary_file:
                    try:
                        filtered_file, was_filtered = self._validate_and_filter_insufficient_labels(
                            input_file=str(bundle.primary_file),
                            strategy=bundle.strategy,
                            min_samples=2,
                            auto_remove=False,  # Ask user for confirmation
                            train_by_language=needs_language_training  # CRITICAL: Language-aware validation
                        )
                        if was_filtered:
                            input_file_to_use = filtered_file
                            self.console.print(f"[green]✓ Using filtered training dataset[/green]\n")
                    except ValueError as e:
                        # User cancelled or validation failed
                        self.console.print(f"[red]{e}[/red]")
                        return {
                            'runtime_params': runtime_params,
                            'models_trained': [],
                            'best_model': None,
                            'best_f1': None,
                            'error': str(e)
                        }
                    except Exception as e:
                        self.logger.warning(f"Label validation failed: {e}")
                        # Continue with original file if validation fails
                        pass

                # Fall through to standard training
                # Initialize global progress tracking
                import time
                global_start_time = time.time()
                global_total_models = 1
                global_total_epochs = epochs

                # Calculate maximum possible epochs (if model triggers reinforced learning)
                if enable_reinforced_learning and manual_rl_epochs is not None:
                    global_max_epochs = epochs + manual_rl_epochs
                else:
                    global_max_epochs = global_total_epochs

                # DEBUGGING: Log the epoch calculation
                num_languages = int(len(languages)) if languages else 1
                self.logger.info("="*80)
                self.logger.info("GLOBAL EPOCHS CALCULATION DEBUG")
                self.logger.info(f"  Training mode: multi-label (single model)")
                self.logger.info(f"  Number of models: {global_total_models}")
                self.logger.info(f"  Number of languages: {num_languages}")
                self.logger.info(f"  Languages: {sorted(languages) if languages else 'N/A'}")
                self.logger.info(f"  Base epochs: {epochs}")
                self.logger.info(f"  RL epochs: {manual_rl_epochs if manual_rl_epochs else 'None'}")
                self.logger.info(f"  CALCULATED global_total_models: {global_total_models}")
                self.logger.info(f"  CALCULATED global_total_epochs: {global_total_epochs}")
                self.logger.info(f"  CALCULATED global_max_epochs: {global_max_epochs}")
                self.logger.info("="*80)

                global_completed_epochs = 0

                result = trainer.train({
                    'input_file': input_file_to_use,
                    'model_name': model_name,
                    'num_epochs': epochs,
                    'output_dir': str(output_dir),
                    'text_column': bundle.text_column,
                    'label_column': bundle.label_column,
                    'multiclass_groups': multiclass_groups,
                    'reinforced_learning': enable_reinforced_learning,
                    'session_id': session_id,
                    'split_config': bundle.metadata.get('split_config') if hasattr(bundle, 'metadata') else None,
                    # Global progress tracking
                    'global_total_models': global_total_models,
                    'global_current_model': 1,
                    'global_total_epochs': global_total_epochs,
                    'global_max_epochs': global_max_epochs,
                    'global_completed_epochs': global_completed_epochs,
                    'global_start_time': global_start_time,
                    **extra_config
                })
        else:
            # Standard training (multi-class or multi-label)

            # ============================================================
            # CRITICAL: Validate and filter insufficient labels BEFORE training
            # ============================================================
            if bundle.primary_file:
                try:
                    filtered_file, was_filtered = self._validate_and_filter_insufficient_labels(
                        input_file=str(bundle.primary_file),
                        strategy=bundle.strategy,
                        min_samples=2,
                        auto_remove=False,  # Ask user for confirmation
                        train_by_language=needs_language_training  # CRITICAL: Language-aware validation
                    )
                    if was_filtered:
                        # Update bundle to use filtered file
                        bundle.primary_file = Path(filtered_file)
                        self.console.print(f"[green]✓ Using filtered training dataset[/green]\n")
                except ValueError as e:
                    # User cancelled or validation failed
                    self.console.print(f"[red]{e}[/red]")
                    return {
                        'runtime_params': runtime_params,
                        'models_trained': [],
                        'best_model': None,
                        'best_f1': None,
                        'error': str(e)
                    }
                except Exception as e:
                    self.logger.warning(f"Label validation failed: {e}")
                    # Continue with original file if validation fails
                    pass

            # Initialize global progress tracking
            import time
            global_start_time = time.time()
            global_total_models = 1
            global_total_epochs = epochs

            # Calculate maximum possible epochs (if model triggers reinforced learning)
            if enable_reinforced_learning and manual_rl_epochs is not None:
                global_max_epochs = epochs + manual_rl_epochs
            else:
                global_max_epochs = global_total_epochs

            # DEBUGGING: Log the epoch calculation
            num_languages = int(len(languages)) if languages else 1
            self.logger.info("="*80)
            self.logger.info("GLOBAL EPOCHS CALCULATION DEBUG")
            self.logger.info(f"  Training mode: standard (multi-label or multi-class single model)")
            self.logger.info(f"  Number of models: {global_total_models}")
            self.logger.info(f"  Number of languages: {num_languages}")
            self.logger.info(f"  Languages: {sorted(languages) if languages else 'N/A'}")
            self.logger.info(f"  Base epochs: {epochs}")
            self.logger.info(f"  RL epochs: {manual_rl_epochs if manual_rl_epochs else 'None'}")
            self.logger.info(f"  CALCULATED global_total_models: {global_total_models}")
            self.logger.info(f"  CALCULATED global_total_epochs: {global_total_epochs}")
            self.logger.info(f"  CALCULATED global_max_epochs: {global_max_epochs}")
            self.logger.info("="*80)

            global_completed_epochs = 0

            config = bundle.to_trainer_config(output_dir, extra_config)
            config['session_id'] = session_id
            config['split_config'] = bundle.metadata.get('split_config') if hasattr(bundle, 'metadata') else None
            # Add global progress tracking
            config['global_total_models'] = global_total_models
            config['global_current_model'] = 1
            config['global_total_epochs'] = global_total_epochs
            config['global_max_epochs'] = global_max_epochs
            config['global_completed_epochs'] = global_completed_epochs
            config['global_start_time'] = global_start_time

            try:
                result = trainer.train(config)
            except Exception as exc:  # pylint: disable=broad-except
                self.console.print(f"[red]Training failed:[/red] {exc}")
                self.logger.exception("Quick training failed", exc_info=exc)
                return {
                    'runtime_params': runtime_params,
                    'models_trained': [],
                    'best_model': None,
                    'best_f1': None,
                    'error': str(exc)
                }

        self._training_studio_show_training_result(result, bundle, title="Quick training results")

        # Return complete training info for metadata save
        return {
            'runtime_params': runtime_params,
            'models_trained': [model_name],
            'best_model': result.get('best_model'),
            'best_f1': result.get('best_f1') or result.get('f1_macro')
        }

    def _training_studio_resolve_multilabel_dataset(self, bundle: TrainingDataBundle) -> Optional[Tuple[Path, Optional[List[str]]]]:
        """Return the consolidated multi-label dataset path if available."""
        multilabel_path = bundle.training_files.get("multilabel")
        if multilabel_path:
            path_obj = Path(multilabel_path)
            if path_obj.exists():
                label_fields = bundle.metadata.get("labels_detected")
                return path_obj, label_fields if isinstance(label_fields, list) else None

        if bundle.strategy == "multi-label" and bundle.primary_file:
            path_obj = Path(bundle.primary_file)
            if path_obj.exists() and path_obj.suffix.lower() in {".json", ".jsonl"}:
                label_fields = bundle.metadata.get("labels_detected")
                return path_obj, label_fields if isinstance(label_fields, list) else None

        return None

    def _training_studio_show_distributed_results(
        self,
        trainer: MultiLabelTrainer,
        models: Dict[str, MultiLabelModelInfo],
        output_dir: Path,
    ) -> None:
        """Render a summary table of distributed training results."""
        if not models:
            message = "No models were produced during distributed training."
            if HAS_RICH and self.console:
                self.console.print(f"[yellow]{message}[/yellow]")
            else:
                print(message)
            return

        if HAS_RICH and self.console:
            table = Table(title="Distributed training results", border_style="green")
            table.add_column("Model", style="cyan", width=30)
            table.add_column("Label", style="white", width=25)
            table.add_column("Language", style="white", width=12)
            table.add_column("Macro F1", justify="right", width=12)

            for model_name, info in sorted(models.items()):
                metrics = info.performance_metrics or {}
                macro_f1 = metrics.get("macro_f1", 0.0)
                table.add_row(
                    model_name,
                    info.label_name,
                    info.language or "—",
                    f"{macro_f1:.3f}"
                )

            self.console.print(table)
            self.console.print(f"[dim]Models saved to[/dim] {output_dir}")
        else:
            print("\nDistributed training results:")
            for model_name, info in sorted(models.items()):
                metrics = info.performance_metrics or {}
                macro_f1 = metrics.get('macro_f1', 0.0)
                print(f"  - {model_name}: label={info.label_name}, lang={info.language or '-'}, macro_f1={macro_f1:.3f}")
            print(f"Models saved to {output_dir}")


    def _training_studio_show_training_result(self, result: Dict[str, Any], bundle: TrainingDataBundle, title: str) -> None:
        table = Table(title=title, border_style="green")
        table.add_column("Metric", style="cyan", width=15)
        table.add_column("Value", style="white", width=60)

        table.add_row("Model", str(result.get("best_model", "n/a")))
        table.add_row("Accuracy", f"{result.get('accuracy', 0.0):.4f}")
        table.add_row("F1 macro", f"{result.get('best_f1_macro', 0.0):.4f}")
        table.add_row("Model path", result.get("model_path", "—"))

        self.console.print(table)

        if bundle.strategy == "multi-label":
            metrics = result.get("metrics", {})
            per_label = metrics.get("per_label_results")
            if per_label:
                detail_table = Table(title="Per-label performance", border_style="blue")
                detail_table.add_column("Label", width=30)
                detail_table.add_column("Accuracy", width=12)
                detail_table.add_column("F1 macro", width=12)

                for label, stats in per_label.items():
                    if isinstance(stats, dict) and "error" not in stats:
                        detail_table.add_row(
                            label,
                            f"{stats.get('accuracy', 0.0):.4f}",
                            f"{stats.get('f1_macro', 0.0):.4f}",
                        )
                    elif isinstance(stats, dict):
                        detail_table.add_row(label, stats.get("error", "error"), "—")

                self.console.print(detail_table)

    def _training_studio_show_benchmark_results(self, report: Dict[str, Any]) -> None:
        results = report.get("results", [])
        if not results:
            self.console.print("[yellow]No benchmark results available.[/yellow]")
            return

        table = Table(title="Benchmark results", border_style="green")
        table.add_column("#", style="cyan", width=5)
        table.add_column("Model", style="white", width=35)
        table.add_column("Accuracy", justify="right", width=12)
        table.add_column("F1 macro", justify="right", width=12)

        for idx, entry in enumerate(results, start=1):
            table.add_row(
                str(idx),
                entry.get("model", "?"),
                f"{entry.get('accuracy', 0.0):.4f}",
                f"{entry.get('f1_macro', 0.0):.4f}",
            )

        self.console.print(table)

        best_model = report.get("best_model")
        if best_model:
            best_f1 = report.get("best_f1_macro", 0.0)
            self.console.print(f"[green]Best model:[/green] {best_model} (F1 {best_f1:.4f})")

    def _training_studio_resolve_benchmark_dataset(self, bundle: TrainingDataBundle) -> Tuple[Path, str, str]:
        # Support both single-label and multi-label datasets for benchmarking
        if bundle.primary_file:
            return bundle.primary_file, bundle.text_column, bundle.label_column

        # For multi-label distributed training, we have individual label files
        candidates = [(label, path) for label, path in bundle.training_files.items() if label != "multilabel"]

        if not candidates:
            raise ValueError("No dataset available for benchmarking.")

        if len(candidates) == 1:
            label, path = candidates[0]
            self.console.print(f"Using dataset for label [cyan]{label}[/cyan].")
            return path, "text", "label"

        self.console.print("\nSelect the label you want to benchmark:")
        for idx, (label, _) in enumerate(candidates, start=1):
            self.console.print(f"  {idx}. {label}")

        choice = self._int_prompt_with_validation("Label", default=1, min_value=1, max_value=len(candidates))
        label, path = candidates[choice - 1]
        self.console.print(f"Benchmarking label [cyan]{label}[/cyan].")
        return path, "text", "label"

    def _training_studio_make_output_dir(self, prefix: str) -> Path:
        """
        Create output directory for models.

        CRITICAL: This function should NOT be used in Training Arena mode.
        Instead, models are saved directly to models/{session_id}/...
        This function is kept for backward compatibility with legacy modes.

        Args:
            prefix: Prefix for directory name (e.g., 'training_studio_quick')

        Returns:
            Path to created directory
        """
        directory = self.settings.paths.models_dir / f"{prefix}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        directory.mkdir(parents=True, exist_ok=True)
        return directory

    def _flatten_trainer_models(self) -> List[str]:
        if not self.available_trainer_models:
            return []

        names: List[str] = []
        for models in self.available_trainer_models.values():
            names.extend(model["name"] for model in models)

        seen = set()
        unique: List[str] = []
        for name in names:
            if name not in seen:
                unique.append(name)
                seen.add(name)
        return unique

    def _get_majority_language(self, languages: set, language_distribution: dict = None) -> str:
        """
        Determine the majority/dominant language from a set of languages.

        Args:
            languages: Set of language codes
            language_distribution: Optional dict mapping language codes to counts

        Returns:
            Dominant language code (lowercase) or None
        """
        if not languages:
            return None

        # If we have distribution data, use it to find the majority
        if language_distribution:
            total = sum(language_distribution.values())
            if total > 0:
                # Find language with highest percentage
                majority_lang = max(language_distribution.items(), key=lambda x: x[1])
                percentage = (majority_lang[1] / total) * 100

                # If a language represents >50%, it's the majority
                if percentage > 50:
                    return majority_lang[0].lower()

        # Fallback: if only one language, return it
        if len(languages) == 1:
            return list(languages)[0].lower()

        # If multiple languages without clear majority, check for common cases
        lang_list = [l.lower() for l in languages]
        if 'fr' in lang_list:
            return 'fr'  # Often FR is dominant after correction
        elif 'en' in lang_list:
            return 'en'

        return None

    def _is_model_multilingual(self, model_name: str) -> bool:
        """
        Determine if a model is multilingual or language-specific.

        Args:
            model_name: HuggingFace model ID

        Returns:
            True if multilingual, False if language-specific
        """
        # Model-to-language mapping (same as in _get_intelligent_benchmark_models)
        MULTILINGUAL_KEYWORDS = ['xlm', 'multilingual', 'mdeberta', 'long-t5']
        MONOLINGUAL_PATTERNS = {
            'camembert': 'fr',
            'flaubert': 'fr',
            'bert-base-german-cased': 'de',
            'distilbert-base-german-cased': 'de',
            'roberta': 'en',  # RoBERTa is English-only unless specified
            'bert-base-uncased': 'en',
            'bert-base-cased': 'en',
            'distilbert-base-uncased': 'en',
        }

        model_lower = model_name.lower()

        # Check for multilingual keywords
        if any(keyword in model_lower for keyword in MULTILINGUAL_KEYWORDS):
            return True

        # Check for monolingual patterns
        if any(pattern in model_lower for pattern in MONOLINGUAL_PATTERNS.keys()):
            return False

        # Default: assume multilingual for safety (won't create extra models)
        return True

    def _get_intelligent_benchmark_models(self, languages: set, text_length_avg: float, model_strategy: str, recommended_model: str = None, language_distribution: dict = None, user_prefers_long_models: bool = False) -> Tuple[List[str], Dict[str, Optional[str]]]:
        """
        HIGHLY INTELLIGENT model selection using ALL available models in the package.

        Selection criteria (scored):
        1. Language match (primary, 100 points)
        2. Text length compatibility (50 points) - BOOSTED if user_prefers_long_models=True
        3. Model size/efficiency (30 points)
        4. Model popularity/reliability (20 points)
        5. Multilingual capability (bonus for mixed languages)

        Returns:
            Tuple of (models_list, model_to_language_map)
            model_to_language_map: {model_name: language_code or None for multilingual}
        """
        lang_list = list(languages) if languages else []

        # COMPREHENSIVE language-to-model mapping using ALL models from sota_models
        MODEL_LANGUAGE_MAP = {
            # ============ MULTILINGUAL MODELS ============
            'xlm-roberta-base': None,
            'xlm-roberta-large': None,
            'bert-base-multilingual-cased': None,
            'bert-base-multilingual-uncased': None,
            'distilbert-base-multilingual-cased': None,
            'microsoft/mdeberta-v3-base': None,

            # ============ MULTILINGUAL LONG-DOCUMENT MODELS ============
            'markussagen/xlm-roberta-longformer-base-4096': None,  # Multilingual Longformer, 4096 tokens, 100+ languages
            'google/long-t5-local-base': None,  # Multilingual T5 with local attention, 4096+ tokens
            'google/long-t5-tglobal-base': None,  # Multilingual T5 with transient global attention, 4096+ tokens

            # ============ ENGLISH MODELS ============
            'bert-base-uncased': 'en',
            'bert-base-cased': 'en',
            'bert-large-uncased': 'en',
            'bert-large-cased': 'en',
            'roberta-base': 'en',
            'roberta-large': 'en',
            'distilbert-base-uncased': 'en',
            'distilbert-base-cased': 'en',
            'distilroberta-base': 'en',
            'albert-base-v2': 'en',
            'albert-large-v2': 'en',
            'albert-xlarge-v2': 'en',
            'google/electra-base-discriminator': 'en',
            'google/electra-large-discriminator': 'en',
            'microsoft/deberta-base': 'en',
            'microsoft/deberta-large': 'en',
            'microsoft/deberta-v3-base': 'en',
            'microsoft/deberta-v3-large': 'en',
            'microsoft/deberta-v3-small': 'en',
            'allenai/longformer-base-4096': 'en',
            'google/bigbird-roberta-base': 'en',
            'google/bigbird-roberta-large': 'en',
            'squeezebert/squeezebert-uncased': 'en',
            'sentence-transformers/all-MiniLM-L6-v2': 'en',

            # ============ FRENCH MODELS ============
            'camembert-base': 'fr',
            'camembert/camembert-base': 'fr',
            'camembert/camembert-large': 'fr',
            'flaubert/flaubert_base_cased': 'fr',
            'flaubert/flaubert_base_uncased': 'fr',
            'flaubert/flaubert_large_cased': 'fr',
            'cmarkea/distilcamembert-base': 'fr',
            'almanach/camembert-base': 'fr',
            'dbmdz/bert-base-french-europeana-cased': 'fr',
            'dangvantuan/sentence-camembert-base': 'fr',
            'qwant/fralbert-base': 'fr',

            # ============ GERMAN MODELS ============
            'bert-base-german-cased': 'de',
            'bert-base-german-dbmdz-cased': 'de',
            'bert-base-german-dbmdz-uncased': 'de',
            'deepset/gbert-base': 'de',
            'deepset/gbert-large': 'de',
            'distilbert-base-german-cased': 'de',
            'uklfr/gottbert-base': 'de',
            'dbmdz/bert-base-german-europeana-cased': 'de',

            # ============ SPANISH MODELS ============
            'dccuchile/bert-base-spanish-wwm-cased': 'es',
            'dccuchile/bert-base-spanish-wwm-uncased': 'es',
            'PlanTL-GOB-ES/roberta-base-bne': 'es',
            'mrm8488/electricidad-base-discriminator': 'es',
            'bertin-project/bertin-roberta-base-spanish': 'es',

            # ============ ITALIAN MODELS ============
            'dbmdz/bert-base-italian-cased': 'it',
            'dbmdz/bert-base-italian-uncased': 'it',
            'dbmdz/bert-base-italian-xxl-cased': 'it',
            'dbmdz/bert-base-italian-xxl-uncased': 'it',
            'Musixmatch/umberto-commoncrawl-cased-v1': 'it',

            # ============ PORTUGUESE MODELS ============
            'neuralmind/bert-base-portuguese-cased': 'pt',
            'neuralmind/bert-large-portuguese-cased': 'pt',
            'adalbertojunior/distilbert-portuguese-cased': 'pt',
            'pierreguillou/bert-base-cased-pt-lenerbr': 'pt',

            # ============ DUTCH MODELS ============
            'GroNLP/bert-base-dutch-cased': 'nl',
            'wietsedv/bert-base-dutch-cased': 'nl',
            'pdelobelle/robbert-v2-dutch-base': 'nl',
            'DTAI-KULeuven/robbert-2023-dutch-large': 'nl',

            # ============ POLISH MODELS ============
            'dkleczek/bert-base-polish-uncased-v1': 'pl',
            'dkleczek/bert-base-polish-cased-v1': 'pl',
            'allegro/herbert-base-cased': 'pl',
            'allegro/herbert-large-cased': 'pl',

            # ============ ARABIC MODELS ============
            'aubmindlab/bert-base-arabertv2': 'ar',
            'aubmindlab/bert-large-arabertv2': 'ar',
            'asafaya/bert-base-arabic': 'ar',
            'CAMeL-Lab/bert-base-arabic-camelbert-msa': 'ar',
            'UBC-NLP/MARBERT': 'ar',

            # ============ CHINESE MODELS ============
            'bert-base-chinese': 'zh',
            'hfl/chinese-bert-wwm': 'zh',
            'hfl/chinese-bert-wwm-ext': 'zh',
            'hfl/chinese-roberta-wwm-ext': 'zh',
            'hfl/chinese-roberta-wwm-ext-large': 'zh',
            'hfl/chinese-electra-base-discriminator': 'zh',

            # ============ RUSSIAN MODELS ============
            'DeepPavlov/rubert-base-cased': 'ru',
            'DeepPavlov/rubert-base-cased-conversational': 'ru',
            'ai-forever/ruBert-base': 'ru',
            'ai-forever/ruBert-large': 'ru',
            'cointegrated/rubert-tiny': 'ru',

            # ============ JAPANESE MODELS ============
            'cl-tohoku/bert-base-japanese': 'ja',
            'cl-tohoku/bert-base-japanese-whole-word-masking': 'ja',
            'cl-tohoku/bert-large-japanese': 'ja',
            'nlp-waseda/roberta-base-japanese': 'ja',
            'nlp-waseda/roberta-large-japanese': 'ja',

            # ============ KOREAN MODELS ============
            'klue/bert-base': 'ko',
            'kykim/bert-kor-base': 'ko',
            'beomi/kcbert-base': 'ko',
            'beomi/kcbert-large': 'ko',

            # ============ TURKISH MODELS ============
            'dbmdz/bert-base-turkish-cased': 'tr',
            'dbmdz/bert-base-turkish-uncased': 'tr',
            'dbmdz/electra-base-turkish-cased-discriminator': 'tr',

            # ============ SWEDISH MODELS ============
            'KB/bert-base-swedish-cased': 'sv',
            'af-ai-center/bert-base-swedish-uncased': 'sv',

            # ============ DANISH MODELS ============
            'Maltehb/danish-bert-botxo': 'da',
            'sarnikowski/convbert-small-da-cased': 'da',

            # ============ NORWEGIAN MODELS ============
            'ltg/norbert': 'no',
            'NbAiLab/nb-bert-base': 'no',

            # ============ FINNISH MODELS ============
            'TurkuNLP/bert-base-finnish-cased-v1': 'fi',
            'TurkuNLP/bert-base-finnish-uncased-v1': 'fi',

            # ============ HINDI MODELS ============
            'ai4bharat/indic-bert': 'hi',

            # ============ VIETNAMESE MODELS ============
            'vinai/phobert-base': 'vi',
            'vinai/phobert-large': 'vi',

            # ============ THAI MODELS ============
            'airesearch/wangchanberta-base-att-spm-uncased': 'th',

            # ============ INDONESIAN MODELS ============
            'indobenchmark/indobert-base-p1': 'id',
            'indobenchmark/indobert-large-p1': 'id',

            # ============ CZECH MODELS ============
            'Seznam/retromae-small-cs': 'cs',
            'ufal/robeczech-base': 'cs',

            # ============ GREEK MODELS ============
            'nlpaueb/bert-base-greek-uncased-v1': 'el',

            # ============ HEBREW MODELS ============
            'onlplab/alephbert-base': 'he',

            # ============ ROMANIAN MODELS ============
            'dumitrescustefan/bert-base-romanian-cased-v1': 'ro',

            # ============ BULGARIAN MODELS ============
            'iarfmoose/roberta-base-bulgarian': 'bg',

            # ============ CROATIAN MODELS ============
            'classla/bcms-bertic': 'hr',

            # ============ SERBIAN MODELS ============
            'classla/bcms-bertic': 'sr',

            # ============ UKRAINIAN MODELS ============
            'youscan/ukr-roberta-base': 'uk',
        }

        # ============================================================
        # STEP 1: Analyze language distribution
        # ============================================================
        total_samples = sum(language_distribution.values()) if language_distribution else 0
        lang_percentages = {}
        if total_samples > 0:
            lang_percentages = {lang.lower(): (count / total_samples * 100)
                                for lang, count in language_distribution.items()}

        # Find dominant language (>70%)
        dominant_lang = None
        for lang, pct in lang_percentages.items():
            if pct > 70:
                dominant_lang = lang
                break

        # Check if single language
        if len(lang_list) == 1:
            dominant_lang = lang_list[0].lower()

        # Check if balanced multilingual
        is_balanced_multilingual = len(lang_list) > 1 and not dominant_lang

        # ============================================================
        # STEP 2: Score ALL available models
        # ============================================================
        model_scores = {}

        for model_name, model_lang in MODEL_LANGUAGE_MAP.items():
            score = 0.0

            # CRITERION 1: Language Match (100 points max)
            if model_lang is None:  # Multilingual model
                if is_balanced_multilingual:
                    score += 90  # Excellent for balanced multilingual
                elif len(lang_list) > 1:
                    score += 70  # Good for any multilingual
                else:
                    score += 40  # Okay for single language
            else:  # Language-specific model
                if dominant_lang and model_lang == dominant_lang:
                    score += 100  # Perfect match!
                elif model_lang in [l.lower() for l in lang_list]:
                    pct = lang_percentages.get(model_lang, 0)
                    score += pct  # Score based on language percentage
                else:
                    score += 0  # No language match

            # CRITERION 2: Text Length Compatibility (50 points max, BOOSTED to 150 if user wants long models)
            is_long_model = ('longformer' in model_name.lower() or
                           'bigbird' in model_name.lower() or
                           'long-t5' in model_name.lower() or
                           '4096' in model_name or
                           '16384' in model_name)

            if user_prefers_long_models:
                # User explicitly wants long-document models - BOOST them heavily
                if is_long_model:
                    score += 150  # MASSIVE boost for long models when user wants them
                else:
                    # Standard models get penalty when long models are preferred
                    if text_length_avg > 500:
                        score += 10  # Not ideal - will truncate
                    else:
                        score += 20  # Acceptable but not preferred
            else:
                # Normal scoring when user hasn't expressed preference
                if is_long_model:
                    if text_length_avg > 400:
                        score += 50  # Perfect for long texts
                    elif text_length_avg > 200:
                        score += 30  # Okay for medium texts
                    else:
                        score += 10  # Overkill for short texts
                else:
                    if text_length_avg <= 300:
                        score += 40  # Good for short/medium texts
                    elif text_length_avg <= 500:
                        score += 30  # Okay for longer texts
                    else:
                        score += 15  # Not ideal but works

            # CRITERION 3: Model Size/Efficiency (30 points max)
            if 'distil' in model_name.lower() or 'tiny' in model_name.lower() or 'small' in model_name.lower():
                score += 30  # Efficient models
            elif 'base' in model_name.lower():
                score += 25  # Standard models
            elif 'large' in model_name.lower() or 'xlarge' in model_name.lower():
                score += 15  # Large models (slower)
            else:
                score += 20  # Unknown size

            # CRITERION 4: Model Popularity/Reliability (20 points max)
            # Based on known high-quality models
            high_quality_models = {
                'xlm-roberta-base': 20,
                'xlm-roberta-large': 18,
                'bert-base-multilingual-cased': 18,
                'camembert-base': 19,
                'roberta-base': 20,
                'bert-base-uncased': 19,
                'flaubert-base': 17,
                'bert-base-german-cased': 18,
                'microsoft/deberta-v3-base': 20,
                'microsoft/mdeberta-v3-base': 19,
                'markussagen/xlm-roberta-longformer-base-4096': 20,  # Excellent multilingual long-document (100+ languages)
                'google/long-t5-local-base': 18,  # High-quality multilingual T5 long-document
                'google/long-t5-tglobal-base': 18,  # High-quality multilingual T5 long-document
                'allenai/longformer-base-4096': 17,  # Popular long-document (EN only)
                'google/bigbird-roberta-base': 17,  # Popular long-document (EN only)
            }
            score += high_quality_models.get(model_name, 15)  # Default 15 for others

            # BONUS: Recommended model gets extra points
            if recommended_model and model_name == recommended_model:
                score += 50

            model_scores[model_name] = score

        # ============================================================
        # STEP 3: Select top models by score
        # ============================================================
        sorted_models = sorted(model_scores.items(), key=lambda x: x[1], reverse=True)

        # Select top 10 models, then filter to diverse set
        top_candidates = [model for model, score in sorted_models[:20]]

        final_models = []
        selected_langs = set()

        # Strategy: Pick diverse models
        # 1. Add multilingual models first (max 2)
        multilingual_added = 0
        for model in top_candidates:
            if MODEL_LANGUAGE_MAP[model] is None and multilingual_added < 2:
                final_models.append(model)
                multilingual_added += 1

        # 2. Add language-specific models for each detected language (1-2 per language)
        for lang in lang_list[:3]:  # Limit to 3 languages
            lang_lower = lang.lower()
            lang_models_added = 0
            for model in top_candidates:
                if MODEL_LANGUAGE_MAP[model] == lang_lower and lang_models_added < 2:
                    if model not in final_models:
                        final_models.append(model)
                        lang_models_added += 1
                        selected_langs.add(lang_lower)

        # 3. Fill remaining slots with highest scored models
        for model in top_candidates:
            if model not in final_models:
                final_models.append(model)
            if len(final_models) >= 7:
                break

        # Ensure we have at least some models
        if not final_models:
            final_models = ['xlm-roberta-base', 'bert-base-multilingual-cased', 'bert-base-uncased']

        # ============================================================
        # STEP 4: Display selection rationale
        # ============================================================
        if len(lang_list) > 1:
            lang_info = f"multilingual ({', '.join(lang_list[:3])})"
            if len(lang_list) > 3:
                lang_info += f" +{len(lang_list) - 3} more"
        elif len(lang_list) == 1:
            lang_info = f"{lang_list[0].upper()}"
        else:
            lang_info = "unknown language"

        text_len_info = "short" if text_length_avg < 150 else "medium" if text_length_avg < 350 else "long"

        self.console.print(f"[dim]🤖 AI Selection: {lang_info} dataset, {text_len_info} texts (avg {text_length_avg:.0f} chars)[/dim]")
        self.console.print(f"[dim]   Scored {len(MODEL_LANGUAGE_MAP)} models → Selected top {len(final_models)} by intelligent criteria[/dim]")

        # Build model-to-language mapping for selected models
        model_lang_map = {model: MODEL_LANGUAGE_MAP.get(model, None) for model in final_models}

        return final_models, model_lang_map

    def _get_preselected_benchmark_models(self, languages: set, text_length_avg: float) -> List[str]:
        """
        Let user choose from pre-selected model categories.
        NOW INCLUDES ALL LANGUAGES SUPPORTED IN THE PACKAGE!
        """
        self.console.print("\n[bold]📋 Pre-Selected Model Categories[/bold]\n")
        self.console.print("[dim]Choose from curated model lists organized by language and characteristics[/dim]\n")

        categories = {}
        lang_list = [l.lower() for l in languages] if languages else ['en']

        # ============ MULTILINGUAL MODELS ============
        if len(lang_list) > 1:
            categories['Multilingual'] = [
                'xlm-roberta-base',
                'xlm-roberta-large',
                'bert-base-multilingual-cased',
                'microsoft/mdeberta-v3-base'
            ]

        # ============ MAJOR LANGUAGES (Always show) ============
        categories['English'] = [
            'bert-base-uncased',
            'roberta-base',
            'distilbert-base-uncased',
            'microsoft/deberta-v3-base'
        ]

        if 'fr' in lang_list or True:  # Always show major languages
            categories['French'] = [
                'camembert-base',
                'flaubert/flaubert_base_cased',
                'cmarkea/distilcamembert-base',
                'almanach/camembert-base'
            ]

        if 'de' in lang_list or True:
            categories['German'] = [
                'bert-base-german-cased',
                'deepset/gbert-base',
                'distilbert-base-german-cased'
            ]

        if 'es' in lang_list or True:
            categories['Spanish'] = [
                'dccuchile/bert-base-spanish-wwm-cased',
                'PlanTL-GOB-ES/roberta-base-bne',
                'bertin-project/bertin-roberta-base-spanish'
            ]

        # ============ EUROPEAN LANGUAGES ============
        if 'it' in lang_list:
            categories['Italian'] = [
                'dbmdz/bert-base-italian-cased',
                'dbmdz/bert-base-italian-xxl-cased'
            ]

        if 'pt' in lang_list:
            categories['Portuguese'] = [
                'neuralmind/bert-base-portuguese-cased',
                'neuralmind/bert-large-portuguese-cased'
            ]

        if 'nl' in lang_list:
            categories['Dutch'] = [
                'GroNLP/bert-base-dutch-cased',
                'pdelobelle/robbert-v2-dutch-base'
            ]

        if 'pl' in lang_list:
            categories['Polish'] = [
                'dkleczek/bert-base-polish-uncased-v1',
                'allegro/herbert-base-cased'
            ]

        if 'sv' in lang_list:
            categories['Swedish'] = ['KB/bert-base-swedish-cased']

        if 'da' in lang_list:
            categories['Danish'] = ['Maltehb/danish-bert-botxo']

        if 'no' in lang_list:
            categories['Norwegian'] = ['ltg/norbert', 'NbAiLab/nb-bert-base']

        if 'fi' in lang_list:
            categories['Finnish'] = ['TurkuNLP/bert-base-finnish-cased-v1']

        if 'el' in lang_list:
            categories['Greek'] = ['nlpaueb/bert-base-greek-uncased-v1']

        if 'tr' in lang_list:
            categories['Turkish'] = ['dbmdz/bert-base-turkish-cased']

        if 'ro' in lang_list:
            categories['Romanian'] = ['dumitrescustefan/bert-base-romanian-cased-v1']

        if 'bg' in lang_list:
            categories['Bulgarian'] = ['iarfmoose/roberta-base-bulgarian']

        if 'hr' in lang_list or 'sr' in lang_list:
            categories['Croatian/Serbian'] = ['classla/bcms-bertic']

        if 'uk' in lang_list:
            categories['Ukrainian'] = ['youscan/ukr-roberta-base']

        if 'cs' in lang_list:
            categories['Czech'] = ['ufal/robeczech-base']

        # ============ ASIAN LANGUAGES ============
        if 'zh' in lang_list:
            categories['Chinese'] = [
                'bert-base-chinese',
                'hfl/chinese-roberta-wwm-ext',
                'hfl/chinese-roberta-wwm-ext-large'
            ]

        if 'ja' in lang_list:
            categories['Japanese'] = [
                'cl-tohoku/bert-base-japanese',
                'nlp-waseda/roberta-base-japanese'
            ]

        if 'ko' in lang_list:
            categories['Korean'] = [
                'klue/bert-base',
                'beomi/kcbert-base'
            ]

        if 'ar' in lang_list:
            categories['Arabic'] = [
                'aubmindlab/bert-base-arabertv2',
                'CAMeL-Lab/bert-base-arabic-camelbert-msa',
                'UBC-NLP/MARBERT'
            ]

        if 'ru' in lang_list:
            categories['Russian'] = [
                'DeepPavlov/rubert-base-cased',
                'ai-forever/ruBert-base'
            ]

        if 'hi' in lang_list:
            categories['Hindi'] = ['ai4bharat/indic-bert']

        if 'vi' in lang_list:
            categories['Vietnamese'] = ['vinai/phobert-base']

        if 'th' in lang_list:
            categories['Thai'] = ['airesearch/wangchanberta-base-att-spm-uncased']

        if 'id' in lang_list:
            categories['Indonesian'] = ['indobenchmark/indobert-base-p1']

        if 'he' in lang_list:
            categories['Hebrew'] = ['onlplab/alephbert-base']

        # ============ SPECIAL CATEGORIES ============
        if text_length_avg > 400:
            categories['Long Documents (>400 chars, 4096 tokens)'] = [
                'markussagen/xlm-roberta-longformer-base-4096',  # Multilingual FIRST
                'google/long-t5-local-base',  # Multilingual
                'allenai/longformer-base-4096',  # English only
                'google/bigbird-roberta-base'  # English only
            ]

        categories['Efficient/Fast'] = [
            'distilbert-base-uncased',
            'distilroberta-base',
            'albert-base-v2',
            'squeezebert/squeezebert-uncased'
        ]

        categories['State-of-the-Art'] = [
            'microsoft/deberta-v3-base',
            'microsoft/mdeberta-v3-base',
            'google/electra-base-discriminator',
            'xlm-roberta-large'
        ]

        # Display categories in organized fashion
        self.console.print("[bold cyan]Available Categories:[/bold cyan]\n")
        for i, (cat_name, models) in enumerate(categories.items(), 1):
            model_list = ', '.join(models[:3])  # Show first 3
            if len(models) > 3:
                model_list += f" (+{len(models)-3} more)"
            self.console.print(f"  [green]{i}.[/green] [cyan]{cat_name}:[/cyan] {model_list}")

        self.console.print(f"\n[dim]Total: {len(categories)} categories available[/dim]")
        self.console.print("\n[yellow]📝 Enter category names separated by commas[/yellow]")
        self.console.print("[dim]   Example: 'English,Multilingual' or 'French,Efficient'[/dim]\n")

        # Smart default based on detected languages
        default_cats = []
        if len(lang_list) > 1:
            default_cats.append("Multilingual")
        if 'en' in lang_list:
            default_cats.append("English")
        if 'fr' in lang_list:
            default_cats.append("French")
        if 'de' in lang_list:
            default_cats.append("German")
        if 'es' in lang_list:
            default_cats.append("Spanish")

        default_str = ','.join(default_cats) if default_cats else "Multilingual,English"

        selected_cats = Prompt.ask("Select categories", default=default_str)

        # Parse selected categories
        selected_models = []
        for cat in selected_cats.split(','):
            cat = cat.strip()
            # Case-insensitive matching
            for cat_name, models in categories.items():
                if cat.lower() in cat_name.lower():
                    selected_models.extend(models)
                    break

        # Deduplicate
        selected_models = list(dict.fromkeys(selected_models))

        return selected_models if selected_models else ['xlm-roberta-base', 'bert-base-multilingual-cased']

    def _get_custom_benchmark_models(self) -> List[str]:
        """Let user manually select models"""
        self.console.print("\n[bold]✏️  Custom Model Selection[/bold]\n")

        all_models = self._flatten_trainer_models()
        self.console.print(f"[dim]Available models ({len(all_models)}):[/dim]")
        for i, model in enumerate(all_models, 1):
            if i % 3 == 0:
                self.console.print(f"  {model}")
            else:
                self.console.print(f"  {model}", end="  ")
        if len(all_models) % 3 != 0:
            self.console.print()

        self.console.print("\n[dim]Enter model names separated by commas, or HuggingFace model IDs[/dim]")
        models_input = Prompt.ask("Model names", default="bert-base-uncased,xlm-roberta-base")

        selected_models = [m.strip() for m in models_input.split(',')]
        return selected_models

    def _save_training_metadata(
        self,
        bundle: TrainingDataBundle,
        mode: str,
        model_config: Dict[str, Any],
        execution_status: Optional[Dict[str, Any]] = None,
        session_id: Optional[str] = None,
        quick_params: Optional[Dict[str, Any]] = None,
        runtime_params: Optional[Dict[str, Any]] = None,
        training_context: Optional[Dict[str, Any]] = None
    ) -> Path:
        """
        Save COMPREHENSIVE training session metadata for reproducibility and resume capability.

        Now uses the enhanced MetadataManager for complete parameter capture.

        Parameters
        ----------
        bundle : TrainingDataBundle
            The training data bundle with all dataset information
        mode : str
            Training mode: quick, benchmark, custom, etc.
        model_config : dict
            Model configuration including selected_model, epochs, batch_size, etc.
        execution_status : dict, optional
            Execution status information (status, started_at, completed_at, etc.)
        session_id : str, optional
            Session ID to use (defaults to timestamp)
        quick_params : dict, optional
            Quick mode parameters if applicable
        runtime_params : dict, optional
            Runtime parameters from actual training
        training_context : dict, optional
            Additional training context information

        Returns
        -------
        Path
            Path to the saved metadata JSON file
        """
        from datetime import datetime
        from llm_tool.utils.metadata_manager import MetadataManager

        # Use provided session_id or create new one
        timestamp = session_id or datetime.now().strftime('%Y%m%d_%H%M%S')

        # Initialize metadata manager
        metadata_manager = MetadataManager(session_id=timestamp)

        # Save comprehensive metadata using the new manager
        metadata_path = metadata_manager.save_comprehensive_metadata(
            bundle=bundle,
            mode=mode,
            model_config=model_config,
            quick_params=quick_params,
            execution_status=execution_status,
            runtime_params=runtime_params,
            training_context=training_context
        )

        # Store metadata manager for later updates
        self._current_metadata_manager = metadata_manager

        return metadata_path

    def _update_training_metadata(
        self,
        metadata_path: Path,
        **updates
    ) -> None:
        """
        Update existing training metadata file with new information (post-training).

        Now uses the enhanced MetadataManager for safe updates.

        Parameters
        ----------
        metadata_path : Path
            Path to the existing metadata JSON file
        **updates : dict
            Sections to update (e.g., execution_status={'status': 'completed'})
        """
        from llm_tool.utils.metadata_manager import MetadataManager

        try:
            # Use metadata manager for updates
            if hasattr(self, '_current_metadata_manager') and self._current_metadata_manager:
                # Use existing manager if available
                self._current_metadata_manager.update_metadata(**updates)
            else:
                # Create new manager from path
                session_id = metadata_path.parent.parent.name
                metadata_manager = MetadataManager(session_id=session_id)
                metadata_manager.update_metadata(**updates)

        except Exception as e:
            self.logger.error(f"Failed to update metadata: {e}")

            # Fallback to direct JSON update
            import json
            if metadata_path.exists():
                try:
                    with open(metadata_path, 'r', encoding='utf-8') as f:
                        metadata = json.load(f)

                    for section, data in updates.items():
                        if section in metadata:
                            if isinstance(metadata[section], dict) and isinstance(data, dict):
                                metadata[section].update(data)
                            else:
                                metadata[section] = data
                        else:
                            metadata[section] = data

                    with open(metadata_path, 'w', encoding='utf-8') as f:
                        json.dump(metadata, f, indent=2, ensure_ascii=False)

                except Exception as fallback_error:
                    self.logger.error(f"Fallback update also failed: {fallback_error}")

    def _reconstruct_bundle_from_metadata(self, metadata: Dict[str, Any]) -> Optional[TrainingDataBundle]:
        """
        Reconstruct a TrainingDataBundle from saved metadata for resume/relaunch.

        Now handles the comprehensive metadata format from MetadataManager.

        Parameters
        ----------
        metadata : dict
            Loaded metadata dictionary from JSON file

        Returns
        -------
        TrainingDataBundle or None
            Reconstructed bundle, or None if reconstruction fails
        """
        try:
            # Handle both old and new metadata formats
            dataset_config = metadata.get('dataset_config', {})
            language_config = metadata.get('language_config', {})
            text_analysis = metadata.get('text_analysis', {})
            split_config = metadata.get('split_config', {})
            label_config = metadata.get('label_config', {})
            preprocessing_config = metadata.get('preprocessing', {})

            # Load primary file
            primary_file_str = dataset_config.get('primary_file')
            if not primary_file_str:
                self.console.print("[red]Error: No primary file found in metadata[/red]")
                return None

            primary_file = Path(primary_file_str)
            if not primary_file.exists():
                self.console.print(f"[red]Error: Dataset file not found: {primary_file}[/red]")
                # Check for training files as fallback
                if dataset_config.get('training_files'):
                    self.console.print("[yellow]Primary file missing, but training files may be available[/yellow]")
                else:
                    return None

            # Create bundle with comprehensive info
            # Note: TrainingDataBundle doesn't accept format_type or samples parameters
            bundle = TrainingDataBundle(
                primary_file=primary_file if primary_file.exists() else None,
                strategy=dataset_config.get('strategy', 'single-label'),
                text_column=dataset_config.get('text_column', 'text'),
                label_column=dataset_config.get('label_column', 'label'),
                metadata={}
            )

            # Store format information in metadata instead
            bundle.metadata['format_type'] = dataset_config.get('format_type', dataset_config.get('format', 'unknown'))
            bundle.metadata['format'] = dataset_config.get('format', 'unknown')

            # Restore training_files if present
            if dataset_config.get('training_files'):
                bundle.training_files = {
                    label: Path(path)
                    for label, path in dataset_config['training_files'].items()
                }

            # Restore ALL metadata fields comprehensively
            # Language configuration
            bundle.metadata['confirmed_languages'] = set(language_config.get('confirmed_languages', []))
            bundle.metadata['language_distribution'] = language_config.get('language_distribution', {})
            bundle.metadata['model_strategy'] = language_config.get('model_strategy', 'multilingual')
            bundle.metadata['language_model_mapping'] = language_config.get('language_model_mapping', {})
            bundle.metadata['per_language_training'] = language_config.get('per_language_training', False)
            bundle.metadata['models_by_language'] = language_config.get('models_by_language', {})

            # Text analysis
            bundle.metadata['text_length_stats'] = text_analysis.get('text_length_stats', {})
            bundle.metadata['requires_long_document_model'] = text_analysis.get('requires_long_document_model', False)
            bundle.metadata['user_prefers_long_models'] = text_analysis.get('user_prefers_long_models', False)
            bundle.metadata['exclude_long_texts'] = text_analysis.get('exclude_long_texts', False)
            bundle.metadata['split_long_texts'] = text_analysis.get('split_long_texts', False)

            # Label configuration
            bundle.metadata['categories'] = dataset_config.get('categories', list(dataset_config.get('category_distribution', {}).keys()))
            bundle.metadata['category_distribution'] = dataset_config.get('category_distribution', {})
            bundle.metadata['num_categories'] = dataset_config.get('num_categories', len(bundle.metadata['categories']))
            bundle.metadata['label_type'] = label_config.get('label_type', 'single')
            bundle.metadata['label_mapping'] = label_config.get('label_mapping', {})
            bundle.metadata['imbalanced_labels'] = label_config.get('imbalanced_labels', [])

            # Dataset configuration
            bundle.metadata['source_file'] = dataset_config.get('source_file')
            bundle.metadata['annotation_column'] = dataset_config.get('annotation_column')
            bundle.metadata['training_approach'] = dataset_config.get('training_approach')
            bundle.metadata['original_strategy'] = dataset_config.get('original_strategy')

            # CRITICAL FIX: Restore hybrid/custom training configuration
            # These fields are REQUIRED for session relaunch to work with hybrid training
            bundle.metadata['multiclass_keys'] = dataset_config.get('multiclass_keys', [])
            bundle.metadata['onevsall_keys'] = dataset_config.get('onevsall_keys', [])
            bundle.metadata['key_strategies'] = dataset_config.get('key_strategies', {})
            bundle.metadata['files_per_key'] = dataset_config.get('files_per_key', {})

            # Split configuration
            if split_config:
                bundle.metadata['split_config'] = split_config

            # Preprocessing
            if preprocessing_config:
                bundle.metadata['preprocessing'] = preprocessing_config

            # Restore training files paths if they exist
            training_files_dict = dataset_config.get('training_files', {})
            if training_files_dict:
                bundle.training_files = {k: Path(v) for k, v in training_files_dict.items()}

            # Restore model configuration
            model_config = metadata.get('model_config', {})
            if model_config:
                bundle.metadata['recommended_model'] = model_config.get('recommended_model')
                bundle.metadata['selected_model'] = model_config.get('selected_model')
                bundle.metadata['models_by_language'] = model_config.get('models_by_language', {})

            # Restore advanced settings
            advanced_settings = metadata.get('advanced_settings', {})
            if advanced_settings:
                bundle.metadata['benchmark_mode'] = advanced_settings.get('benchmark_mode', False)
                bundle.metadata['one_vs_all'] = advanced_settings.get('one_vs_all', False)
                bundle.metadata['multi_label'] = advanced_settings.get('multi_label', False)

            # Set recommended model if available
            if 'recommended_model' in model_config:
                bundle.recommended_model = model_config['recommended_model']

            return bundle

        except Exception as e:
            self.logger.error(f"Failed to reconstruct bundle from metadata: {e}")
            self.console.print(f"[red]Error reconstructing dataset: {e}[/red]")
            return None

    def _resume_training_studio(self):
        """Resume or relaunch training using saved parameters from previous sessions"""

        self.console.print("\n[bold cyan]🔄 Resume/Relaunch Training[/bold cyan]\n")
        self.console.print("[dim]Load saved parameters from previous training sessions[/dim]\n")

        # Detect metadata files - now using the new training_arena structure
        from pathlib import Path
        base_dir = Path("logs/training_arena")

        self.console.print(f"[dim]Searching in: {base_dir}[/dim]\n")

        if not base_dir.exists():
            self.console.print("[yellow]⚠️  Training arena logs directory not found.[/yellow]")
            self.console.print(f"[dim]Expected location: {base_dir}[/dim]")
            self.console.print("[dim]Complete a training first to create session history.[/dim]")
            self.console.print("\n[dim]Press Enter to continue...[/dim]")
            input()
            return

        # Find all training sessions - BOTH with and without metadata
        # This ensures ALL sessions are recallable, even if metadata was not saved
        metadata_files = []
        sessions_without_metadata = []

        for session_dir in base_dir.iterdir():
            if not session_dir.is_dir():
                continue

            metadata_path = session_dir / "training_session_metadata" / "training_metadata.json"
            if metadata_path.exists():
                # Session has metadata - add to primary list
                metadata_files.append(metadata_path)
            else:
                # Session exists but has NO metadata - create minimal metadata on-the-fly
                # This allows recovery of sessions where user declined metadata saving
                sessions_without_metadata.append(session_dir)

        # Generate minimal metadata for sessions without it
        for session_dir in sessions_without_metadata:
            try:
                session_id = session_dir.name

                # Create minimal metadata structure for display and potential recovery
                minimal_metadata = {
                    "metadata_version": "2.0",
                    "created_at": datetime.fromtimestamp(session_dir.stat().st_mtime).isoformat(),
                    "last_updated": datetime.fromtimestamp(session_dir.stat().st_mtime).isoformat(),
                    "training_session": {
                        "session_id": session_id,
                        "timestamp": session_id,
                        "tool_version": "LLMTool",
                        "workflow": "Training Arena - Unknown",
                        "mode": "unknown"
                    },
                    "dataset_config": {
                        "primary_file": None,
                        "format_type": "unknown",
                        "strategy": "single-label",
                        "text_column": "text",
                        "label_column": "label",
                        "total_samples": 0
                    },
                    "model_config": {
                        "training_mode": "unknown",
                        "selected_model": None,
                        "epochs": None,
                        "batch_size": 16
                    },
                    "execution_status": {
                        "status": "no_metadata",
                        "started_at": None,
                        "completed_at": None
                    },
                    "output_paths": {
                        "session_dir": str(session_dir),
                        "models_dir": str(Path("models") / session_id),
                        "logs_dir": str(session_dir)
                    },
                    "_recovered": True  # Flag indicating this was auto-recovered
                }

                # Try to extract information from training_data directory if it exists
                training_data_dir = session_dir / "training_data"
                if training_data_dir.exists():
                    # Look for training files to get dataset info
                    for train_file in training_data_dir.glob("train_*.csv"):
                        minimal_metadata["dataset_config"]["primary_file"] = str(train_file)
                        break

                # Create a "virtual" metadata file path for this session
                # We don't actually save it to disk unless user tries to relaunch
                virtual_metadata_path = session_dir / "training_session_metadata" / "training_metadata.json"

                # Store the metadata temporarily (we'll handle it specially in the loop)
                # Add to metadata_files list as a tuple to distinguish it
                metadata_files.append((virtual_metadata_path, minimal_metadata))

            except Exception as e:
                self.logger.debug(f"Could not generate metadata for {session_dir.name}: {e}")
                continue

        if not metadata_files:
            self.console.print("[yellow]⚠️  No training sessions found.[/yellow]")
            self.console.print(f"[dim]Searched in: {base_dir}[/dim]")
            self.console.print("[dim]Complete a training to create session history.[/dim]")
            self.console.print("\n[dim]Press Enter to continue...[/dim]")
            input()
            return

        # Sort by modification time (most recent first)
        # Handle both Path objects and tuples (virtual metadata)
        def get_mtime(item):
            if isinstance(item, tuple):
                # Virtual metadata - use parent directory mtime
                return item[0].parent.parent.stat().st_mtime
            else:
                # Real metadata file
                return item.stat().st_mtime

        metadata_files.sort(key=get_mtime, reverse=True)

        # Display sessions table
        sessions_table = Table(
            title="📚 Previous Training Sessions (20 most recent)",
            border_style="cyan",
            box=box.ROUNDED
        )
        sessions_table.add_column("#", style="cyan bold", width=4)
        sessions_table.add_column("Session Name", style="white", width=25)
        sessions_table.add_column("Date", style="yellow", width=12)
        sessions_table.add_column("Time", style="yellow", width=8)
        sessions_table.add_column("Mode", style="magenta", width=12)
        sessions_table.add_column("Dataset", style="green", width=25)
        sessions_table.add_column("Model", style="blue", width=20)
        sessions_table.add_column("Status", style="white", width=12)

        import json
        from datetime import datetime

        # Load and display sessions
        valid_sessions = []
        parsing_errors = []  # Track errors for debugging

        for i, mf_item in enumerate(metadata_files[:20], 1):  # Show max 20 most recent
            try:
                # Handle both real metadata files and virtual metadata (tuples)
                if isinstance(mf_item, tuple):
                    # Virtual metadata - already loaded
                    mf, metadata = mf_item
                    is_recovered = True
                else:
                    # Real metadata file - load it
                    mf = mf_item
                    with open(mf, 'r', encoding='utf-8') as f:
                        metadata = json.load(f)
                    is_recovered = False

                session_info = metadata.get('training_session', {})
                dataset_config = metadata.get('dataset_config', {})
                model_config = metadata.get('model_config', {})
                exec_status = metadata.get('execution_status', {})

                # Extract session name and timestamp
                session_id = session_info.get('session_id', '')

                # Parse session name and timestamp from session_id
                # Expected format: {name}_{YYYYMMDD_HHMMSS}
                session_name_parts = session_id.rsplit('_', 2)
                if len(session_name_parts) >= 3 and len(session_name_parts[-1]) == 6 and len(session_name_parts[-2]) == 8:
                    # Has proper timestamp format
                    session_name = '_'.join(session_name_parts[:-2]) or 'training_session'
                    timestamp_str = f"{session_name_parts[-2]}_{session_name_parts[-1]}"
                else:
                    # Fallback for legacy or malformed session IDs
                    session_name = session_id.split('_20')[0] if '_20' in session_id else session_id
                    timestamp_str = session_info.get('timestamp', '')

                # Format date and time separately
                try:
                    dt = datetime.strptime(timestamp_str, '%Y%m%d_%H%M%S')
                    date_str = dt.strftime('%Y-%m-%d')
                    time_str = dt.strftime('%H:%M')
                except:
                    # Fallback if timestamp parsing fails
                    date_str = timestamp_str[:10] if len(timestamp_str) >= 10 else 'N/A'
                    time_str = timestamp_str[11:16] if len(timestamp_str) >= 16 else 'N/A'

                # Truncate session name if too long
                if len(session_name) > 23:
                    session_name = session_name[:20] + "..."

                mode = model_config.get('training_mode', 'unknown')
                dataset_path = dataset_config.get('primary_file', '')
                dataset_name = Path(dataset_path).name if dataset_path else 'N/A'
                if len(dataset_name) > 23:
                    dataset_name = dataset_name[:20] + "..."

                model_name = model_config.get('selected_model') or 'N/A'
                if len(model_name) > 18:
                    model_name = model_name[:15] + "..."

                status = exec_status.get('status', 'unknown')

                # Color code status
                if is_recovered:
                    # Session recovered without metadata
                    status_display = f"[yellow]⚠ recovered[/yellow]"
                elif status == 'completed':
                    status_display = f"[green]✓ {status}[/green]"
                elif status == 'failed':
                    status_display = f"[red]✗ {status}[/red]"
                elif 'benchmark' in status.lower():
                    # Benchmark-specific statuses
                    if 'completed' in status.lower():
                        status_display = f"[green]🎯 benchmark[/green]"
                    else:
                        status_display = f"[cyan]🎯 {status}[/cyan]"
                else:
                    status_display = f"[yellow]⏸ {status}[/yellow]"

                sessions_table.add_row(
                    str(i),
                    session_name,
                    date_str,
                    time_str,
                    mode,
                    dataset_name,
                    model_name,
                    status_display
                )

                valid_sessions.append((mf, metadata))

            except Exception as e:
                self.logger.debug(f"Skipping invalid metadata file {mf}: {e}")
                parsing_errors.append((mf.name, str(e)))
                continue

        if not valid_sessions:
            self.console.print("[yellow]No valid training sessions found.[/yellow]")

            # Show parsing errors if any for debugging
            if parsing_errors:
                self.console.print(f"\n[dim]Parsing errors for {len(parsing_errors)} files:[/dim]")
                for fname, err in parsing_errors[:5]:  # Show first 5
                    self.console.print(f"[dim]  • {fname}: {err[:80]}[/dim]")

            self.console.print("\n[dim]Press Enter to continue...[/dim]")
            input()
            return

        self.console.print(sessions_table)

        # Check if any sessions were recovered
        recovered_count = sum(1 for _, metadata in valid_sessions if metadata.get('_recovered', False))
        if recovered_count > 0:
            self.console.print(f"\n[yellow]ℹ️  {recovered_count} session(s) marked as 'recovered' had no saved metadata.[/yellow]")
            self.console.print("[dim]   These sessions can still be viewed, but may have limited information.[/dim]")
            self.console.print("[dim]   Note: Starting now, all new training sessions will automatically save metadata.[/dim]\n")

        # Select session
        session_choices = [str(i) for i in range(1, len(valid_sessions) + 1)] + ["back"]
        session_choice = Prompt.ask(
            "\n[bold yellow]Select session to resume/relaunch[/bold yellow]",
            choices=session_choices,
            default="1"
        )

        if session_choice == "back":
            return

        metadata_file, metadata = valid_sessions[int(session_choice) - 1]

        # Check if this is a recovered session
        is_recovered_session = metadata.get('_recovered', False)

        # Display selected session details
        self.console.print("\n[bold cyan]📋 Selected Session Details[/bold cyan]")

        if is_recovered_session:
            self.console.print("[yellow]⚠️  This session was recovered without complete metadata.[/yellow]")
            self.console.print("[dim]Some training parameters may be missing or unknown.[/dim]\n")

        details_table = Table(border_style="green", box=box.SIMPLE)
        details_table.add_column("Parameter", style="cyan bold", width=25)
        details_table.add_column("Value", style="white", width=60)

        session_info = metadata.get('training_session', {})
        dataset_config = metadata.get('dataset_config', {})
        model_config = metadata.get('model_config', {})
        exec_status = metadata.get('execution_status', {})

        details_table.add_row("Timestamp", session_info.get('timestamp', 'N/A'))
        details_table.add_row("Workflow", session_info.get('workflow', 'N/A'))

        dataset_file = dataset_config.get('primary_file', '')
        if dataset_file:
            details_table.add_row("Dataset", Path(dataset_file).name)
        else:
            details_table.add_row("Dataset", "[yellow]Unknown[/yellow]")

        details_table.add_row("Strategy", dataset_config.get('strategy', 'N/A'))
        details_table.add_row("Total Samples", str(dataset_config.get('total_samples', 0)))
        details_table.add_row("Training Mode", model_config.get('training_mode', 'N/A'))

        selected_model = model_config.get('selected_model')
        if selected_model:
            details_table.add_row("Model", selected_model)

        epochs = model_config.get('epochs')
        if epochs:
            details_table.add_row("Epochs", str(epochs))

        batch_size = model_config.get('batch_size')
        if batch_size:
            details_table.add_row("Batch Size", str(batch_size))

        details_table.add_row("Status", exec_status.get('status', 'unknown'))

        self.console.print(details_table)

        # Special handling for recovered sessions
        if is_recovered_session:
            self.console.print("\n[yellow]⚠️  Recovered Session Limitations:[/yellow]")
            self.console.print("  • Training parameters are incomplete")
            self.console.print("  • Cannot guarantee exact reproduction of original training")
            self.console.print("  • Consider starting a new training session instead\n")

            if not dataset_config.get('primary_file'):
                self.console.print("[red]✗ Cannot relaunch: Dataset file is unknown[/red]")
                self.console.print("[dim]Press Enter to continue...[/dim]")
                input()
                return

        # Ask: resume or relaunch?
        self.console.print("\n[bold cyan]🎯 Action Mode[/bold cyan]")
        self.console.print("  • [cyan]resume[/cyan]   - Continue incomplete training (if interrupted)")
        self.console.print("  • [cyan]relaunch[/cyan] - Start fresh with same parameters\n")

        action_mode = Prompt.ask(
            "[bold yellow]Resume or relaunch?[/bold yellow]",
            choices=["resume", "relaunch"],
            default="relaunch"  # Default to relaunch since resume is complex for training
        )

        # Reconstruct bundle from metadata
        self.console.print(f"\n[cyan]Reconstructing dataset configuration...[/cyan]")

        bundle = self._reconstruct_bundle_from_metadata(metadata)

        if bundle is None:
            self.console.print("[red]Failed to reconstruct training configuration.[/red]")
            self.console.print("\n[dim]Press Enter to continue...[/dim]")
            input()
            return

        # Get training mode
        mode = model_config.get('training_mode', 'quick')

        # CRITICAL: Initialize session attributes for resume/relaunch
        # Extract session_id from metadata
        session_info = metadata.get('training_session', {})
        session_id = session_info.get('session_id')

        if not session_id:
            # Fallback: try to extract from metadata file path or generate new one
            self.logger.warning("No session_id found in metadata, generating new session_id for relaunch")
            from datetime import datetime
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            session_id = f"relaunch_{timestamp}"

        # Initialize session manager (required by training execution code)
        from llm_tool.utils.training_data_utils import TrainingDataSessionManager
        session_manager = TrainingDataSessionManager(session_id=session_id)

        # Store session attributes for use throughout training
        self.current_session_id = session_id
        self.current_session_manager = session_manager

        self.console.print(f"[dim]Session ID: {session_id}[/dim]\n")

        # Display confirmation message
        if action_mode == 'resume':
            self.console.print(f"\n[green]✓ Resuming training session...[/green]\n")
        else:
            self.console.print(f"\n[green]✓ Relaunching training with saved parameters...[/green]\n")

        # Execute with loaded parameters
        # We'll modify _training_studio_confirm_and_execute to accept optional pre-loaded config
        self._training_studio_confirm_and_execute(
            bundle,
            mode,
            preloaded_config=model_config,
            is_resume=action_mode == 'resume'
        )

    def _training_studio_default_model(self) -> str:
        models = self._flatten_trainer_models()
        return "bert-base-uncased" if "bert-base-uncased" in models else (models[0] if models else "bert-base-uncased")

    def _show_analysis_and_get_columns(self, analysis: Dict[str, Any], format_type: str = "general") -> Dict[str, Any]:
        """
        Show file analysis results and intelligently detect columns with user confirmation.
        Returns dictionary with detected column names and confirmed languages.
        """
        result = {
            'text': 'text',
            'label': 'label',
            'id': None,
            'lang': None,
            'confirmed_languages': set()
        }

        # Show analysis issues
        if analysis['issues']:
            self.console.print("\n[yellow]⚠️  Analysis Results:[/yellow]")
            for issue in analysis['issues']:
                self.console.print(f"  {issue}")

        all_columns = analysis.get('all_columns', [])

        # Auto-suggest text column
        text_column_default = "text"
        if analysis['text_column_candidates']:
            best_text = analysis['text_column_candidates'][0]['name']
            text_column_default = best_text
            self.console.print(f"\n[green]✓ Text column detected: '{best_text}'[/green]")

        if all_columns:
            self.console.print(f"[dim]  Available columns: {', '.join(all_columns)}[/dim]")

        result['text'] = Prompt.ask("Text column", default=text_column_default)

        # Auto-suggest label column
        label_column_default = "labels" if "multi" in format_type else "label"
        if analysis['annotation_column_candidates']:
            best_label = analysis['annotation_column_candidates'][0]['name']
            label_column_default = best_label
            self.console.print(f"\n[green]✓ Label column detected: '{best_label}'[/green]")
            stats = analysis['annotation_stats'].get(best_label, {})
            fill_rate = stats.get('fill_rate', 0)
            if fill_rate > 0:
                self.console.print(f"[dim]  ({fill_rate*100:.1f}% of rows have labels)[/dim]")

        if all_columns:
            self.console.print(f"[dim]  Available columns: {', '.join(all_columns)}[/dim]")

        result['label'] = Prompt.ask("Label/Category column", default=label_column_default)

        # Language detection
        languages_found = set(analysis['languages_detected'].keys())

        if languages_found:
            self.console.print(f"\n[bold]🌍 Languages Detected:[/bold]")
            for lang, count in analysis['languages_detected'].items():
                self.console.print(f"  • {lang.upper()}: {count} rows")

            lang_list = ', '.join([l.upper() for l in sorted(languages_found)])
            lang_confirmed = Confirm.ask(
                f"\n[bold]Detected languages: {lang_list}. Is this correct?[/bold]",
                default=True
            )

            if lang_confirmed:
                result['confirmed_languages'] = languages_found
                self.console.print("[green]✓ Languages confirmed[/green]")
            else:
                self.console.print("\n[yellow]Please specify languages manually[/yellow]")
                manual_langs = Prompt.ask("Enter language codes (comma-separated, e.g., en,fr,de)")
                result['confirmed_languages'] = set([l.strip().lower() for l in manual_langs.split(',') if l.strip()])

            # Auto-suggest language column if detected
            if analysis['language_column_candidates']:
                lang_column_default = analysis['language_column_candidates'][0]
                self.console.print(f"\n[green]✓ Language column detected: '{lang_column_default}'[/green]")
                if all_columns:
                    self.console.print(f"[dim]  Available columns: {', '.join(all_columns)}[/dim]")
                result['lang'] = Prompt.ask("Language column (optional)", default=lang_column_default)
        else:
            # No language column detected - ask if user wants to apply language detection
            self.console.print("\n[yellow]ℹ️  No language column detected in data[/yellow]")
            apply_lang_detection = Confirm.ask(
                "Would you like to apply automatic language detection on the text column?",
                default=True
            )

            if apply_lang_detection:
                self.console.print("[cyan]🔍 Detecting languages from text content...[/cyan]")
                self.console.print("[dim]  Language detection will be applied during training[/dim]")
                manual_langs = Prompt.ask(
                    "Expected language codes (optional, comma-separated, e.g., en,fr,de)",
                    default=""
                )
                if manual_langs.strip():
                    result['confirmed_languages'] = set([l.strip().lower() for l in manual_langs.split(',') if l.strip()])

        # Auto-suggest ID column
        if analysis['id_column_candidates']:
            id_column_default = analysis['id_column_candidates'][0]
            self.console.print(f"\n[green]✓ ID column detected: '{id_column_default}'[/green]")
            if all_columns:
                self.console.print(f"[dim]  Available columns: {', '.join(all_columns)}[/dim]")
            result['id'] = Prompt.ask("Identifier column (optional)", default=id_column_default)

        return result

    def _get_long_document_model_recommendation(self, confirmed_languages: set) -> Optional[str]:
        """
        Get long-document model recommendations based on languages.
        Prioritizes models that can handle >512 tokens.
        """
        # Available long-document models
        LONG_DOCUMENT_MODELS = [
            {
                'model': 'allenai/longformer-base-4096',
                'max_tokens': 4096,
                'languages': ['en'],
                'reason': 'English long-document model (4096 tokens)'
            },
            {
                'model': 'google/bigbird-roberta-base',
                'max_tokens': 4096,
                'languages': ['en'],
                'reason': 'English sparse-attention long-document model (4096 tokens)'
            },
            {
                'model': 'markussagen/xlm-roberta-longformer-base-4096',
                'max_tokens': 4096,
                'languages': ['multilingual'],
                'reason': 'Multilingual long-document model (4096 tokens)'
            },
            {
                'model': 'xlm-roberta-base',
                'max_tokens': 512,
                'languages': ['multilingual'],
                'reason': 'Multilingual baseline (512 tokens, fallback)'
            },
        ]

        # Filter models based on languages
        suitable_models = []
        for model in LONG_DOCUMENT_MODELS:
            if 'multilingual' in model['languages']:
                suitable_models.append(model)
            elif confirmed_languages:
                if any(lang in model['languages'] for lang in confirmed_languages):
                    suitable_models.append(model)

        if not suitable_models:
            suitable_models = LONG_DOCUMENT_MODELS  # Fallback to all

        self.console.print(f"\n[bold]🤖 Long-Document Model Recommendations:[/bold]")
        for i, model_info in enumerate(suitable_models[:5], 1):
            self.console.print(f"  {i}. [cyan]{model_info['model']}[/cyan] - {model_info['reason']}")

        choice = Prompt.ask(
            f"Select model (1-{min(5, len(suitable_models))}, or enter model name)",
            default="1"
        )

        if choice.isdigit() and 0 < int(choice) <= len(suitable_models):
            model_to_use = suitable_models[int(choice) - 1]['model']
            self.console.print(f"[green]✓ Selected: {model_to_use}[/green]")
            return model_to_use
        else:
            return choice

    def _get_long_document_models_for_language(self, lang: str) -> list:
        """
        Get long-document model recommendations for a specific language.
        Returns list in LanguageNormalizer.recommend_models format.
        Uses the model catalog (TrainerModelDetector) when available.
        """
        # Try to get models from catalog first
        if self.available_trainer_models:
            # Map language codes to catalog categories
            LANG_TO_CATEGORY = {
                'en': 'Long Document Models',
                'fr': 'Long Document Models - French',
                'es': 'Long Document Models - Spanish',
                'de': 'Long Document Models - German',
                'it': 'Long Document Models - Italian',
                'pt': 'Long Document Models - Portuguese',
                'nl': 'Long Document Models - Dutch',
                'pl': 'Long Document Models - Polish',
                'ru': 'Long Document Models - Russian',
                'zh': 'Long Document Models - Chinese',
                'ja': 'Long Document Models - Japanese',
                'ar': 'Long Document Models - Arabic',
            }

            category = LANG_TO_CATEGORY.get(lang, 'Long Document Models')

            # Get models from catalog
            if category in self.available_trainer_models:
                catalog_models = self.available_trainer_models[category]
                recommendations = []

                for model in catalog_models:
                    # Build reason from model metadata
                    reason_parts = [
                        model.get('type', 'Unknown type'),
                        f"({model.get('max_length', '512')} tokens)"
                    ]
                    if model.get('performance'):
                        reason_parts.append(model['performance'])

                    recommendations.append({
                        'model': model['name'],
                        'reason': ' - '.join(reason_parts)
                    })

                # Add multilingual fallback if not already included
                if lang != 'en' and 'Long Document Models' in self.available_trainer_models:
                    for model in self.available_trainer_models['Long Document Models'][:2]:
                        if 'xlm' in model['name'].lower() or 'multilingual' in model.get('type', '').lower():
                            recommendations.append({
                                'model': model['name'],
                                'reason': f"{model.get('type')} - Multilingual fallback ({model.get('max_length', '4096')} tokens)"
                            })

                if recommendations:
                    return recommendations

        # Fallback: hardcoded comprehensive list if catalog unavailable
        LANG_LONG_MODELS = {
            'en': [
                {'model': 'allenai/longformer-base-4096', 'reason': 'English Longformer (4096 tokens, optimized for English)'},
                {'model': 'google/bigbird-roberta-base', 'reason': 'English BigBird sparse-attention (4096 tokens)'},
                {'model': 'google/long-t5-local-base', 'reason': 'Multilingual T5 for long documents (4096+ tokens)'},
                {'model': 'roberta-base', 'reason': 'English RoBERTa baseline (512 tokens, fallback)'},
            ],
            'fr': [
                {'model': 'markussagen/xlm-roberta-longformer-base-4096', 'reason': 'Multilingual Longformer supporting French (4096 tokens)'},
                {'model': 'google/long-t5-local-base', 'reason': 'Multilingual T5 for long documents (4096+ tokens)'},
                {'model': 'cmarkea/distilcamembert-base-nli', 'reason': 'French DistilCamemBERT optimized (512 tokens)'},
                {'model': 'camembert-base', 'reason': 'French CamemBERT baseline (512 tokens)'},
            ],
            'es': [
                {'model': 'PlanTL-GOB-ES/roberta-base-bne', 'reason': 'Spanish RoBERTa optimized (512 tokens)'},
                {'model': 'markussagen/xlm-roberta-longformer-base-4096', 'reason': 'Multilingual Longformer supporting Spanish (4096 tokens)'},
                {'model': 'dccuchile/bert-base-spanish-wwm-cased', 'reason': 'Spanish BERT baseline (512 tokens)'},
                {'model': 'bertin-project/bertin-roberta-base-spanish', 'reason': 'Spanish BERTIN RoBERTa (512 tokens)'},
            ],
            'de': [
                {'model': 'deepset/gbert-base', 'reason': 'German GBERT optimized (512 tokens)'},
                {'model': 'markussagen/xlm-roberta-longformer-base-4096', 'reason': 'Multilingual Longformer supporting German (4096 tokens)'},
                {'model': 'bert-base-german-cased', 'reason': 'German BERT baseline (512 tokens)'},
                {'model': 'dbmdz/bert-base-german-uncased', 'reason': 'German BERT uncased (512 tokens)'},
            ],
            'it': [
                {'model': 'dbmdz/bert-base-italian-cased', 'reason': 'Italian BERT optimized (512 tokens)'},
                {'model': 'markussagen/xlm-roberta-longformer-base-4096', 'reason': 'Multilingual Longformer supporting Italian (4096 tokens)'},
                {'model': 'dbmdz/bert-base-italian-xxl-cased', 'reason': 'Italian BERT XXL (512 tokens, high performance)'},
            ],
            'pt': [
                {'model': 'neuralmind/bert-base-portuguese-cased', 'reason': 'Portuguese BERT optimized (512 tokens)'},
                {'model': 'markussagen/xlm-roberta-longformer-base-4096', 'reason': 'Multilingual Longformer supporting Portuguese (4096 tokens)'},
                {'model': 'adalbertojunior/distilbert-portuguese-cased', 'reason': 'Portuguese DistilBERT (512 tokens, efficient)'},
            ],
            'nl': [
                {'model': 'GroNLP/bert-base-dutch-cased', 'reason': 'Dutch BERT optimized (512 tokens)'},
                {'model': 'markussagen/xlm-roberta-longformer-base-4096', 'reason': 'Multilingual Longformer supporting Dutch (4096 tokens)'},
                {'model': 'wietsedv/bert-base-dutch-cased', 'reason': 'Dutch BERT baseline (512 tokens)'},
            ],
            'pl': [
                {'model': 'allegro/herbert-base-cased', 'reason': 'Polish HerBERT optimized (514 tokens)'},
                {'model': 'markussagen/xlm-roberta-longformer-base-4096', 'reason': 'Multilingual Longformer supporting Polish (4096 tokens)'},
                {'model': 'dkleczek/bert-base-polish-cased-v1', 'reason': 'Polish BERT baseline (512 tokens)'},
            ],
            'ru': [
                {'model': 'DeepPavlov/rubert-base-cased', 'reason': 'Russian RuBERT optimized (512 tokens)'},
                {'model': 'markussagen/xlm-roberta-longformer-base-4096', 'reason': 'Multilingual Longformer supporting Russian (4096 tokens)'},
                {'model': 'sberbank-ai/ruBert-base', 'reason': 'Russian BERT baseline (512 tokens)'},
            ],
            'zh': [
                {'model': 'hfl/chinese-roberta-wwm-ext', 'reason': 'Chinese RoBERTa WWM optimized (512 tokens)'},
                {'model': 'markussagen/xlm-roberta-longformer-base-4096', 'reason': 'Multilingual Longformer supporting Chinese (4096 tokens)'},
                {'model': 'bert-base-chinese', 'reason': 'Chinese BERT baseline (512 tokens)'},
            ],
            'ja': [
                {'model': 'cl-tohoku/bert-base-japanese-whole-word-masking', 'reason': 'Japanese BERT WWM optimized (512 tokens)'},
                {'model': 'markussagen/xlm-roberta-longformer-base-4096', 'reason': 'Multilingual Longformer supporting Japanese (4096 tokens)'},
                {'model': 'cl-tohoku/bert-base-japanese', 'reason': 'Japanese BERT baseline (512 tokens)'},
            ],
            'ar': [
                {'model': 'aubmindlab/bert-base-arabert', 'reason': 'Arabic AraBERT optimized (512 tokens)'},
                {'model': 'markussagen/xlm-roberta-longformer-base-4096', 'reason': 'Multilingual Longformer supporting Arabic (4096 tokens)'},
                {'model': 'asafaya/bert-base-arabic', 'reason': 'Arabic BERT baseline (512 tokens)'},
            ],
            'multilingual': [
                {'model': 'markussagen/xlm-roberta-longformer-base-4096', 'reason': 'Multilingual Longformer (100+ languages, 4096 tokens)'},
                {'model': 'xlm-roberta-large', 'reason': 'Multilingual XLM-RoBERTa large (100+ languages, 512 tokens)'},
                {'model': 'xlm-roberta-base', 'reason': 'Multilingual XLM-RoBERTa base (100+ languages, 512 tokens)'},
                {'model': 'bert-base-multilingual-cased', 'reason': 'Multilingual BERT baseline (104 languages, 512 tokens)'},
            ],
        }

        # Return language-specific models or multilingual as fallback
        return LANG_LONG_MODELS.get(lang, LANG_LONG_MODELS.get('multilingual', []))

    def _get_model_recommendation_from_languages(self, confirmed_languages: set) -> Optional[str]:
        """
        Get model recommendations based on detected/confirmed languages.
        Returns selected model name or None.
        """
        if not confirmed_languages:
            return None

        recommendations = LanguageNormalizer.recommend_models(confirmed_languages, self.available_trainer_models)

        if not recommendations:
            return None

        self.console.print(f"\n[bold]🤖 Recommended Models for Your Languages:[/bold]")
        for i, rec in enumerate(recommendations[:5], 1):
            self.console.print(f"  {i}. [cyan]{rec['model']}[/cyan] - {rec['reason']}")

        # Interactive model selection
        self.console.print(f"\n[bold]Select a model:[/bold]")
        self.console.print("  [cyan]1-{num}[/cyan] - Select from recommendations above".format(num=min(5, len(recommendations))))
        self.console.print("  [cyan]manual[/cyan] - Enter model name manually")
        self.console.print("  [cyan]skip[/cyan] - Use default (bert-base-uncased)")

        model_choice = Prompt.ask("Your choice", default="1")

        if model_choice == "manual":
            return Prompt.ask("\nEnter model name", default="xlm-roberta-base")
        elif model_choice == "skip":
            return "bert-base-uncased"
        elif model_choice.isdigit():
            idx = int(model_choice) - 1
            if 0 <= idx < len(recommendations):
                model_to_use = recommendations[idx]['model']
                self.console.print(f"[green]✓ Selected: {model_to_use}[/green]")
                return model_to_use
            else:
                self.console.print("[yellow]Invalid selection, using first recommendation[/yellow]")
                return recommendations[0]['model']
        else:
            return recommendations[0]['model']

    def _detect_languages_and_analyze_text(
        self,
        df: 'pd.DataFrame',
        text_column: str,
        sample_size: int = 100
    ) -> Dict[str, Any]:
        """
        Universal function to detect languages and analyze text characteristics.
        Works for ANY dataset format and ANY training mode.

        Args:
            df: DataFrame containing the text data
            text_column: Name of the column containing text
            sample_size: Number of samples to analyze for language detection

        Returns:
            Dictionary with:
            - languages_detected: {lang: count} dictionary
            - text_length_stats: {avg_length, max_length, min_length, median_length}
            - long_document_percentage: Percentage of documents > 512 tokens
            - user_prefers_long_models: Boolean recommendation
        """
        from llm_tool.utils.language_detector import LanguageDetector

        # Initialize results
        results = {
            'languages_detected': {},
            'text_length_stats': {
                'avg_length': 0,
                'max_length': 0,
                'min_length': 0,
                'median_length': 0
            },
            'long_document_percentage': 0,
            'user_prefers_long_models': False
        }

        # Check if text column exists
        if text_column not in df.columns:
            self.logger.warning(f"Text column '{text_column}' not found in dataset")
            return results

        # Get text samples (filter out NaN values)
        text_samples = df[text_column].dropna()

        if len(text_samples) == 0:
            self.logger.warning("No text data found in dataset")
            return results

        # Sample for language detection (use up to sample_size rows)
        sample_texts = text_samples.head(sample_size).tolist()

        # Detect languages using LanguageDetector
        detector = LanguageDetector()
        language_counts = Counter()

        for text in sample_texts:
            if isinstance(text, str) and text.strip():
                detected_lang = detector.detect(text)
                if detected_lang:
                    # LanguageDetector returns dict like {'language': 'fr', 'confidence': 0.95}
                    if isinstance(detected_lang, dict):
                        lang = detected_lang.get('language')
                        if lang:
                            language_counts[lang] += 1
                    elif isinstance(detected_lang, str):
                        language_counts[detected_lang] += 1

        results['languages_detected'] = dict(language_counts)

        # Calculate text length statistics
        text_lengths = [len(str(text)) for text in text_samples if pd.notna(text)]

        if text_lengths:
            import statistics
            results['text_length_stats'] = {
                'avg_length': sum(text_lengths) / len(text_lengths),
                'max_length': max(text_lengths),
                'min_length': min(text_lengths),
                'median_length': statistics.median(text_lengths)
            }

            # Estimate long documents (assuming ~4 chars per token)
            long_docs = sum(1 for length in text_lengths if length > 2048)  # 512 tokens * 4 chars
            results['long_document_percentage'] = (long_docs / len(text_lengths)) * 100

            # Recommend long-document models if >20% of docs are long
            results['user_prefers_long_models'] = results['long_document_percentage'] > 20

        return results

    def _display_language_analysis_and_get_model(
        self,
        analysis_results: Dict[str, Any],
        interactive: bool = True
    ) -> Tuple[Set[str], Optional[str]]:
        """
        Display language analysis results and get model recommendation.
        Universal function that works for ANY dataset format and ANY training mode.

        Args:
            analysis_results: Results from _detect_languages_and_analyze_text
            interactive: If True, ask user to confirm languages and select model

        Returns:
            Tuple of (confirmed_languages, selected_model)
        """
        # LanguageNormalizer is defined at the module level, no need to import

        languages_found = set(analysis_results['languages_detected'].keys())
        text_stats = analysis_results['text_length_stats']
        confirmed_languages = set()
        model_to_use = None

        # Display language detection results
        if languages_found:
            self.console.print(f"\n[bold]🌍 Languages Detected:[/bold]")
            for lang, count in analysis_results['languages_detected'].items():
                self.console.print(f"  • {lang.upper()}: {count} samples")

            # Display text statistics
            self.console.print(f"\n[bold]📊 Text Statistics:[/bold]")
            self.console.print(f"  • Average length: {text_stats['avg_length']:.0f} characters")
            self.console.print(f"  • Max length: {text_stats['max_length']:.0f} characters")
            self.console.print(f"  • Median length: {text_stats['median_length']:.0f} characters")

            if analysis_results['long_document_percentage'] > 0:
                self.console.print(f"  • Long documents (>512 tokens): {analysis_results['long_document_percentage']:.1f}%")

                if analysis_results['user_prefers_long_models']:
                    self.console.print("\n[yellow]💡 Recommendation: Consider using long-document models (e.g., Longformer, BigBird)[/yellow]")

            if interactive:
                # Confirm languages with user
                lang_list = ', '.join([l.upper() for l in sorted(languages_found)])
                lang_confirmed = Confirm.ask(
                    f"\n[bold]Detected languages: {lang_list}. Is this correct?[/bold]",
                    default=True
                )

                if lang_confirmed:
                    confirmed_languages = languages_found
                    self.console.print("[green]✓ Languages confirmed[/green]")
                else:
                    # Ask user to specify languages manually
                    self.console.print("\n[yellow]Please specify languages manually[/yellow]")
                    manual_langs = Prompt.ask("Enter language codes (comma-separated, e.g., en,fr,de)")
                    confirmed_languages = set([l.strip().lower() for l in manual_langs.split(',') if l.strip()])
            else:
                confirmed_languages = languages_found
        else:
            # No languages detected - ask user
            self.console.print("\n[yellow]⚠️ No languages could be auto-detected[/yellow]")

            if interactive:
                manual_langs = Prompt.ask("Enter language codes (comma-separated, e.g., en,fr,de)", default="en")
                confirmed_languages = set([l.strip().lower() for l in manual_langs.split(',') if l.strip()])
            else:
                confirmed_languages = {'en'}  # Default to English

        # Get model recommendations based on confirmed languages
        if confirmed_languages and interactive:
            # Consider long-document models if needed
            if analysis_results.get('user_prefers_long_models'):
                self.console.print("\n[bold]🤖 Recommended Long-Document Models:[/bold]")

                # Get multilingual long-doc models - MULTILINGUAL FIRST
                long_doc_recs = [
                    {"model": "markussagen/xlm-roberta-longformer-base-4096", "reason": "Multilingual long-document support (100+ languages, 4096 tokens)"},
                    {"model": "google/long-t5-local-base", "reason": "Multilingual T5 for long documents (4096+ tokens)"},
                    {"model": "allenai/longformer-base-4096", "reason": "English-only, efficient for documents up to 4096 tokens"},
                    {"model": "google/bigbird-roberta-base", "reason": "English-only, sparse attention for very long documents"}
                ]

                for i, rec in enumerate(long_doc_recs, 1):
                    self.console.print(f"  {i}. [cyan]{rec['model']}[/cyan] - {rec['reason']}")

                use_long = Confirm.ask("\n[bold]Use long-document model?[/bold]", default=True)

                if use_long:
                    choice = IntPrompt.ask("Select model (1-4)", default=1)
                    if 1 <= choice <= len(long_doc_recs):
                        model_to_use = long_doc_recs[choice - 1]['model']
                        self.console.print(f"[green]✓ Selected: {model_to_use}[/green]")
                        return confirmed_languages, model_to_use

            # Get standard model recommendations
            recommendations = LanguageNormalizer.recommend_models(confirmed_languages, self.available_trainer_models)

            if recommendations:
                self.console.print(f"\n[bold]🤖 Recommended Models for Your Languages:[/bold]")
                for i, rec in enumerate(recommendations[:5], 1):
                    self.console.print(f"  {i}. [cyan]{rec['model']}[/cyan] - {rec['reason']}")

                # Store recommendations in bundle for later use, don't ask now
                self.console.print(f"\n[dim]ℹ️  Model selection will be done when choosing the training mode[/dim]")

                # Use first recommendation as default, but don't force it
                model_to_use = recommendations[0]['model'] if recommendations else "bert-base-uncased"

        return confirmed_languages, model_to_use

    def bert_annotation_studio(self):
        """BERT Annotation Studio - Advanced annotation with trained models"""
        # Display ASCII logo only
        self._display_ascii_logo()

        # Display mode-specific banner
        self._display_mode_banner('bert_studio')

        # Display personalized mode info
        self._display_section_header(
            "🤖 BERT Annotation Studio - High-Throughput Inference (Parallel GPU/CPU, 100+ Languages)",
            "Production-ready inference with trained BERT models: Parallel processing, language validation, confidence scoring",
            mode_info={
                'workflow': 'Select Model → Load Data (SQL/File) → Detect Language → Preprocessing → Parallel Inference → Export',
                'capabilities': ['50+ Trained Models', '100+ Languages', 'Multi-GPU/CPU Parallel', 'Confidence Scoring', 'Language Validation', 'Text Preprocessing'],
                'input': 'SQL (PostgreSQL/MySQL/SQLite/SQL Server) or Files (CSV/Excel/JSON/JSONL/Parquet/RData)',
                'output': 'Predictions with probabilities + Confidence intervals + Language tags + Multiple export formats',
                'best_for': 'High-throughput production inference, multilingual datasets, large-scale annotation with trained models',
                'duration': '~2-20 min (depends on dataset size, model complexity, and GPU availability)'
            }
        )

        if HAS_RICH and self.console:
            # Create mode menu
            self.console.print("\n[bold cyan]🎯 BERT Annotation Studio Options[/bold cyan]\n")

            studio_options_table = Table(show_header=False, box=None, padding=(0, 2))
            studio_options_table.add_column("Option", style="cyan", width=8)
            studio_options_table.add_column("Description")

            options = [
                ("1", "🚀 Start Annotation Workflow (8-step guided process)"),
                ("0", "⬅️  Back to main menu")
            ]

            for option, desc in options:
                studio_options_table.add_row(f"[bold cyan]{option}[/bold cyan]", desc)

            panel = Panel(
                studio_options_table,
                title="[bold]🤖 BERT Annotation Studio[/bold]",
                border_style="cyan"
            )

            self.console.print(panel)

            choice = Prompt.ask(
                "\n[bold yellow]Select an option[/bold yellow]",
                choices=["0", "1"],
                default="1"
            )

            if choice == "0":
                return

        from .bert_annotation_studio import BERTAnnotationStudio

        studio = BERTAnnotationStudio(
            console=self.console,
            settings=self.settings,
            logger=self.logger
        )
        studio.run()

    def validation_lab(self):
        """Validation lab for quality control and Doccano export"""
        # Display ASCII logo only
        self._display_ascii_logo()

        # Display mode-specific banner
        self._display_mode_banner('validation')

        # Display personalized mode info
        self._display_section_header(
            "🔍 Validation Lab - Quality Scoring, Stratified Sampling, Inter-Annotator Agreement",
            "Quality assurance tools: Validate LLM annotations, detect imbalances, prepare stratified samples for human review",
            mode_info={
                'workflow': 'Load Annotations → Quality Metrics (0-100 score) → Stratified Sampling → Export to Doccano/Label Studio',
                'capabilities': ['Quality Scoring', 'Label Distribution Analysis', 'Stratified Sampling', 'Inter-Annotator Agreement (Cohen\'s Kappa)', 'Schema Validation'],
                'input': 'Annotated JSON/JSONL files from LLM or BERT Studio',
                'output': 'Quality reports + Validation metrics + Doccano/Label Studio export files + Sample selection justification',
                'best_for': 'Quality assurance before training, human validation workflows, detecting annotation issues',
                'duration': '~2-5 min (analysis + sampling + export)'
            }
        )

        # ⚠️ DEVELOPMENT WARNING
        if HAS_RICH and self.console:
            warning_panel = Panel(
                Align.center(
                    "[bold yellow]⚠️  UNDER DEVELOPMENT ⚠️[/bold yellow]\n\n"
                    "[yellow]This mode is currently in active development and is NOT complete.[/yellow]\n"
                    "[dim]Some features may not work as expected or may be missing entirely.[/dim]\n"
                    "[dim]Use at your own risk for testing purposes only.[/dim]",
                    vertical="middle"
                ),
                title="[bold red]Development Status[/bold red]",
                border_style="red",
                box=box.DOUBLE,
                padding=(1, 2),
            )
            self.console.print()
            self.console.print(Align.center(warning_panel))
            self.console.print()

            # Ask user if they want to continue
            if not Confirm.ask("[yellow]Do you want to continue anyway?[/yellow]", default=False):
                self.console.print("[dim]Returning to main menu...[/dim]")
                return

        if HAS_RICH and self.console:
            # Create mode menu
            self.console.print("\n[bold cyan]🎯 Validation Lab Options[/bold cyan]\n")

            lab_options_table = Table(show_header=False, box=None, padding=(0, 2))
            lab_options_table.add_column("Option", style="cyan", width=8)
            lab_options_table.add_column("Description")

            options = [
                ("1", "🔍 Start Validation Workflow (Quality scoring + sampling + export)"),
                ("0", "⬅️  Back to main menu")
            ]

            for option, desc in options:
                lab_options_table.add_row(f"[bold cyan]{option}[/bold cyan]", desc)

            panel = Panel(
                lab_options_table,
                title="[bold]🔍 Validation Lab[/bold]",
                border_style="magenta"
            )

            self.console.print(panel)

            choice = Prompt.ask(
                "\n[bold yellow]Select an option[/bold yellow]",
                choices=["0", "1"],
                default="1"
            )

            if choice == "0":
                return

            # Select annotations file
            self.console.print("\n[bold]Select Annotations File:[/bold]")
            annotations_path = self._prompt_file_path("Annotations file path")

            # Load and analyze
            self.console.print("\n[cyan]Analyzing annotations...[/cyan]")

            from ..validators.annotation_validator import AnnotationValidator, ValidationConfig

            validator = AnnotationValidator()

            # Configuration
            self.console.print("\n[bold]Validation Configuration:[/bold]")

            sample_size = self._int_prompt_with_validation("Sample size for validation", default=100, min_value=10, max_value=1000)
            stratified = Confirm.ask("Use stratified sampling?", default=True)
            export_doccano = Confirm.ask("Export to Doccano format?", default=True)

            # Run validation
            with self.console.status("[bold green]Running validation...", spinner="dots"):
                config = ValidationConfig(
                    sample_size=sample_size,
                    stratified_sampling=stratified,
                    export_to_doccano=export_doccano,
                    export_format='both'
                )

                validator.config = config
                result = validator.validate(
                    input_file=annotations_path,
                    output_dir="./validation"
                )

            # Display results
            self.console.print("\n[bold]Validation Results:[/bold]\n")

            # Quality score with color coding
            score = result.quality_score
            if score >= 80:
                color = "green"
            elif score >= 60:
                color = "yellow"
            else:
                color = "red"

            self.console.print(f"[{color}]Quality Score: {score:.1f}/100[/{color}]")

            # Statistics table
            stats_table = Table(title="📊 Annotation Statistics", border_style="blue")
            stats_table.add_column("Metric", style="cyan")
            stats_table.add_column("Value", style="white")

            stats_table.add_row("Total Annotations", str(result.total_annotations))
            stats_table.add_row("Validated Samples", str(result.validated_samples))
            stats_table.add_row("Unique Labels", str(len(result.label_distribution)))

            if result.confidence_stats:
                stats_table.add_row("Avg Confidence", f"{result.confidence_stats.get('mean', 0):.3f}")
                stats_table.add_row("Low Confidence %", f"{result.confidence_stats.get('low_confidence_percentage', 0):.1f}%")

            self.console.print(stats_table)

            # Issues found
            if result.issues_found:
                self.console.print(f"\n[yellow]⚠ {len(result.issues_found)} issues found:[/yellow]")
                for issue in result.issues_found[:5]:
                    self.console.print(f"  - {issue['type']}: {issue.get('message', issue.get('column', ''))}")

            # Export paths
            self.console.print("\n[bold]Exports:[/bold]")
            if result.doccano_export_path:
                self.console.print(f"📁 Doccano: {result.doccano_export_path}")
            if result.export_path:
                self.console.print(f"📁 Data: {result.export_path}")

            self.console.print("\n[green]✅ Validation complete![/green]")

        else:
            print("\n=== Validation Lab ===")
            print("Quality control and validation\n")

            # ⚠️ DEVELOPMENT WARNING
            print("\n" + "="*60)
            print("⚠️  UNDER DEVELOPMENT - NOT COMPLETE ⚠️")
            print("="*60)
            print("This mode is currently in active development.")
            print("Some features may not work as expected or may be missing.")
            print("Use at your own risk for testing purposes only.")
            print("="*60 + "\n")

            continue_choice = input("Do you want to continue anyway? (y/N): ").strip().lower()
            if continue_choice not in ['y', 'yes']:
                print("Returning to main menu...")
                return

            annotations_path = input("\nAnnotations file path: ").strip()
            sample_size = int(input("Sample size (default 100): ").strip() or "100")

            print("\nRunning validation...")
            print("✅ Validation complete!")
            print(f"Exported to: ./validation/")

    def analytics_dashboard(self):
        """Analytics dashboard"""
        # Display simple section header
        self._display_section_header(
            "📊 Analytics Dashboard",
            "Performance analysis and insights (Coming Soon)"
        )

        if HAS_RICH and self.console:
            self.console.print("\n[yellow]This feature is under development[/yellow]")
        else:
            print("\nThis feature is under development")

    def profile_manager_ui(self):
        """Profile manager interface"""
        # Display ASCII logo only
        self._display_ascii_logo()

        # Display personalized mode info
        self._display_section_header(
            "💾 Profile Manager",
            "Save and reuse your favorite pipeline configurations",
            mode_info={
                'workflow': 'Browse Profiles → Select → Load Configuration → Execute',
                'capabilities': ['Save Configs', 'Quick Reload', 'Profile Sharing', 'Version Control'],
                'input': 'Saved profile from previous runs',
                'output': 'Executed pipeline with saved configuration',
                'best_for': 'Rerunning the same pipeline configuration multiple times',
                'duration': '~1 min + pipeline execution time'
            }
        )

        if HAS_RICH and self.console:
            profiles = self.profile_manager.list_profiles()

            if not profiles:
                self.console.print("[yellow]No saved profiles found[/yellow]")
                return

            table = Table(title="💾 Saved Profiles", border_style="blue")
            table.add_column("#", style="cyan", width=3)
            table.add_column("Name", style="white")
            table.add_column("Created", style="dim")
            table.add_column("Last Used", style="green")

            for i, profile in enumerate(profiles, 1):
                table.add_row(
                    str(i),
                    profile.name,
                    profile.created_at.strftime("%Y-%m-%d"),
                    profile.last_used.strftime("%Y-%m-%d %H:%M")
                )

            self.console.print(table)

            choice = IntPrompt.ask("Select profile to load (0 to cancel)", default=0)
            if choice > 0 and choice <= len(profiles):
                selected_profile = profiles[choice-1]
                self.console.print(f"[green]Loading profile: {selected_profile.name}[/green]")
                # Execute with loaded configuration
                self._execute_quick_start(selected_profile.configuration)
        else:
            print("Profile Manager - Coming Soon")

    def advanced_settings(self):
        """Advanced settings interface"""
        # Display ASCII logo only
        self._display_ascii_logo()

        # Display personalized mode info
        self._display_section_header(
            "⚙️ Advanced Settings",
            "Fine-tune system configuration and preferences",
            mode_info={
                'workflow': 'Browse Settings → Modify → Save → Apply',
                'capabilities': ['API Keys', 'Model Defaults', 'Path Configuration', 'Performance Tuning'],
                'input': 'Current system settings',
                'output': 'Updated configuration',
                'best_for': 'Customizing system behavior and defaults',
                'duration': '~2-5 min'
            }
        )

        if HAS_RICH and self.console:
            self.console.print("\n[yellow]This feature is under development[/yellow]")
        else:
            print("\nThis feature is under development")

    # ============================================================================
    # LLM ANNOTATION STUDIO - HELPER METHODS
    # ============================================================================

    def _detect_prompts_in_folder(self, folder_path: Optional[Path] = None) -> List[Dict[str, Any]]:
        """
        Auto-detect prompts from prompts/ folder with JSON key extraction.

        Returns:
            List of dicts with: {'path': Path, 'name': str, 'keys': List[str], 'content': str}
        """
        if folder_path is None:
            # Default to current directory prompts/ folder
            folder_path = Path.cwd() / "prompts"

        if not folder_path.exists():
            self.logger.warning(f"Prompts folder not found: {folder_path}")
            return []

        prompts = []
        for txt_file in sorted(folder_path.glob("*.txt")):
            try:
                content = txt_file.read_text(encoding='utf-8')

                # Extract JSON keys using the existing function
                from ..annotators.json_cleaner import extract_expected_keys
                keys = extract_expected_keys(content)

                prompts.append({
                    'path': txt_file,
                    'name': txt_file.stem,
                    'keys': keys,
                    'content': content
                })

                self.logger.info(f"Detected prompt: {txt_file.name} with {len(keys)} keys")
            except Exception as e:
                self.logger.warning(f"Failed to read prompt {txt_file.name}: {e}")

        return prompts

    def _detect_text_columns(self, file_path: Path) -> Dict[str, Any]:
        """
        Intelligently detect text columns in a dataset.

        Returns:
            Dict with: {
                'all_columns': List[str],
                'text_candidates': List[Dict],  # [{'name': str, 'confidence': str, 'sample': str}]
                'df': pd.DataFrame
            }
        """
        # Load data to analyze columns
        try:
            if file_path.suffix.lower() == '.csv':
                df = pd.read_csv(file_path, nrows=100)  # Sample first 100 rows
            elif file_path.suffix.lower() in ['.xlsx', '.xls']:
                df = pd.read_excel(file_path, nrows=100)
            elif file_path.suffix.lower() == '.parquet':
                df = pd.read_parquet(file_path)
                df = df.head(100)
            elif file_path.suffix.lower() == '.jsonl':
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = [json.loads(line) for line in f.readlines()[:100]]
                df = pd.DataFrame(data)
            else:
                # Fallback
                return {'all_columns': [], 'text_candidates': [], 'df': None}

            all_columns = df.columns.tolist()
            text_candidates = []

            # Analyze each column
            for col in all_columns:
                if col in df.columns and df[col].dtype == 'object':
                    # Get non-null sample
                    non_null = df[col].dropna()
                    if len(non_null) == 0:
                        continue

                    # Calculate average length
                    avg_length = non_null.astype(str).str.len().mean()
                    sample_value = str(non_null.iloc[0])[:100]  # First 100 chars

                    # Determine confidence
                    if avg_length > 100:
                        confidence = "high"
                    elif avg_length > 30:
                        confidence = "medium"
                    elif avg_length > 10:
                        confidence = "low"
                    else:
                        confidence = "very_low"

                    # Add to candidates if reasonable length
                    if avg_length > 10:
                        text_candidates.append({
                            'name': col,
                            'confidence': confidence,
                            'avg_length': avg_length,
                            'sample': sample_value
                        })

            # Sort by confidence and avg_length
            confidence_order = {"high": 0, "medium": 1, "low": 2, "very_low": 3}
            text_candidates.sort(key=lambda x: (confidence_order[x['confidence']], -x['avg_length']))

            return {
                'all_columns': all_columns,
                'text_candidates': text_candidates,
                'df': df
            }

        except Exception as e:
            self.logger.error(f"Failed to analyze columns: {e}")
            return {'all_columns': [], 'text_candidates': [], 'df': None}

    def _create_annotation_id(self, df: pd.DataFrame, id_column: str = "annotation_id") -> pd.DataFrame:
        """
        Create or verify annotation ID column for tracking.

        Args:
            df: DataFrame to process
            id_column: Name of ID column

        Returns:
            DataFrame with ID column added/verified
        """
        if id_column not in df.columns:
            df[id_column] = [f"ann_{i:06d}" for i in range(1, len(df) + 1)]
            self.console.print(f"[green]✓ Created {id_column} column with {len(df)} IDs[/green]")
        else:
            # Verify no nulls
            null_count = df[id_column].isna().sum()
            if null_count > 0:
                self.console.print(f"[yellow]⚠️  Found {null_count} null IDs, filling them...[/yellow]")
                next_id = len(df) + 1
                for idx in df[df[id_column].isna()].index:
                    df.at[idx, id_column] = f"ann_{next_id:06d}"
                    next_id += 1

        return df

    def _create_output_structure(self, base_name: str, data_format: str) -> Dict[str, Path]:
        """
        Create organized output folder structure.

        Structure:
            annotations_output/
                {timestamp}_{base_name}/
                    data/           # Annotated data files
                    logs/           # Annotation logs
                    prompts/        # Copy of prompts used
                    config.json     # Configuration used

        Returns:
            Dict with paths: {'root', 'data', 'logs', 'prompts', 'config'}
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        root = Path("annotations_output") / f"{timestamp}_{base_name}"

        paths = {
            'root': root,
            'data': root / 'data',
            'logs': root / 'logs',
            'prompts': root / 'prompts',
            'config': root / 'config.json'
        }

        # Create directories
        for key in ['data', 'logs', 'prompts']:
            paths[key].mkdir(parents=True, exist_ok=True)

        self.console.print(f"[green]✓ Created output structure: {root}[/green]")
        return paths

    def _save_incremental(
        self,
        df: pd.DataFrame,
        output_path: Path,
        data_format: str,
        batch_size: int = 50,
        db_config: Optional[Dict] = None
    ):
        """
        Save data incrementally based on format.

        Supports:
        - CSV: Line-by-line append
        - Excel: Batch save every N rows
        - Parquet: Batch save every N rows
        - PostgreSQL: Immediate UPDATE per row
        - RData: Batch save every N rows
        """
        if data_format == 'csv':
            # Append mode for CSV
            if not output_path.exists():
                df.head(0).to_csv(output_path, index=False)
            df.to_csv(output_path, mode='a', header=not output_path.exists(), index=False)

        elif data_format == 'excel':
            # Full save for Excel (cannot append)
            df.to_excel(output_path, index=False)

        elif data_format == 'parquet':
            # Full save for Parquet
            df.to_parquet(output_path, index=False)

        elif data_format == 'postgresql' and db_config:
            # Direct UPDATE in database
            from sqlalchemy import create_engine, text
            engine = create_engine(
                f"postgresql://{db_config['user']}:{db_config['password']}@"
                f"{db_config['host']}:{db_config.get('port', 5432)}/{db_config['database']}"
            )
            # Implementation would use UPDATE statements
            pass

        elif data_format in ['rdata', 'rds']:
            # Save using pyreadr
            try:
                import pyreadr
                pyreadr.write_rdata(str(output_path), df, df_name="annotated_data")
            except ImportError:
                self.logger.error("pyreadr not installed - cannot save RData format")

    # ============================================================================
    # LLM ANNOTATION STUDIO - WORKFLOW METHODS
    # ============================================================================

    def _quick_annotate(self):
        """Resume or relaunch annotation using saved parameters"""
        self.console.print("\n[bold cyan]🔄 Resume/Relaunch Annotation[/bold cyan]\n")
        self.console.print("[dim]Load saved parameters from previous annotations[/dim]\n")

        # ============================================================
        # DETECT METADATA FILES
        # ============================================================
        annotations_dir = self.settings.paths.data_dir / 'annotations'

        if not annotations_dir.exists():
            self.console.print("[yellow]No annotations directory found.[/yellow]")
            self.console.print("[dim]Run Smart Annotate first to create annotation sessions.[/dim]")
            self.console.print("\n[dim]Press Enter to continue...[/dim]")
            input()
            return

        # Find all metadata JSON files
        metadata_files = list(annotations_dir.glob("*_metadata_*.json"))

        if not metadata_files:
            self.console.print("[yellow]No saved annotation parameters found.[/yellow]")
            self.console.print("[dim]Run Smart Annotate and save parameters to use this feature.[/dim]")
            self.console.print("\n[dim]Press Enter to continue...[/dim]")
            input()
            return

        # Sort by modification time (most recent first)
        metadata_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)

        # Display available sessions
        self.console.print(f"[green]Found {len(metadata_files)} saved annotation session(s)[/green]\n")

        sessions_table = Table(border_style="cyan", show_header=True)
        sessions_table.add_column("#", style="cyan", width=3)
        sessions_table.add_column("Session", style="white")
        sessions_table.add_column("Date", style="yellow")
        sessions_table.add_column("Workflow", style="green")
        sessions_table.add_column("Model", style="magenta")

        import json
        from datetime import datetime

        # Load and display sessions
        valid_sessions = []
        for i, mf in enumerate(metadata_files[:20], 1):  # Show max 20 most recent
            try:
                with open(mf, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)

                session_info = metadata.get('annotation_session', {})
                model_config = metadata.get('model_configuration', {})

                timestamp_str = session_info.get('timestamp', '')
                try:
                    dt = datetime.strptime(timestamp_str, '%Y%m%d_%H%M%S')
                    date_str = dt.strftime('%Y-%m-%d %H:%M')
                except:
                    date_str = timestamp_str

                workflow = session_info.get('workflow', 'Unknown')
                model_name = model_config.get('model_name', 'Unknown')

                sessions_table.add_row(
                    str(i),
                    mf.stem[:40],
                    date_str,
                    workflow.split(' - ')[0] if ' - ' in workflow else workflow,
                    model_name
                )

                valid_sessions.append((mf, metadata))
            except Exception as e:
                self.logger.warning(f"Could not load metadata file {mf}: {e}")
                continue

        if not valid_sessions:
            self.console.print("[yellow]No valid metadata files found.[/yellow]")
            self.console.print("\n[dim]Press Enter to continue...[/dim]")
            input()
            return

        self.console.print(sessions_table)

        # Select session
        session_choice = self._int_prompt_with_validation(
            "\n[bold yellow]Select session to resume/relaunch[/bold yellow]",
            1, 1, len(valid_sessions)
        )

        selected_file, metadata = valid_sessions[session_choice - 1]

        self.console.print(f"\n[green]✓ Selected: {selected_file.name}[/green]")

        # ============================================================
        # DISPLAY ALL PARAMETERS
        # ============================================================
        self._display_metadata_parameters(metadata)

        # ============================================================
        # ASK: RESUME OR RELAUNCH?
        # ============================================================
        self.console.print("\n[bold cyan]📋 Action Mode[/bold cyan]\n")
        self.console.print("[yellow]What would you like to do?[/yellow]")
        self.console.print("  • [cyan]resume[/cyan]   - Continue an incomplete annotation (skip already annotated rows)")
        self.console.print("           [dim]Requires the output file with annotated rows[/dim]")
        self.console.print("  • [cyan]relaunch[/cyan] - Start a new annotation with same parameters")
        self.console.print("           [dim]Runs a fresh annotation session[/dim]")

        action_mode = Prompt.ask(
            "\n[bold yellow]Select action[/bold yellow]",
            choices=["resume", "relaunch"],
            default="relaunch"
        )

        # ============================================================
        # ASK: MODIFY PARAMETERS?
        # ============================================================
        self.console.print("\n[bold cyan]⚙️  Parameter Modification[/bold cyan]\n")

        modify_params = Confirm.ask(
            "Do you want to modify any parameters?",
            default=False
        )

        # Extract parameters from metadata
        modified_metadata = self._modify_parameters_if_requested(metadata, modify_params)

        # ============================================================
        # EXECUTE ANNOTATION
        # ============================================================
        self._execute_from_metadata(modified_metadata, action_mode, selected_file)

        self.console.print("\n[dim]Press Enter to return to menu...[/dim]")
        input()

    def _create_annotator_session_directories(self, session_id: str) -> dict:
        """
        Create organized directory structure for annotation session.

        Structure mirroring Training Arena (Mode 3):
        logs/annotator/{session_id}/
        ├── annotated_data/          # Annotated files (.csv, .xlsx, etc.)
        ├── metadata/                 # Metadata JSON files
        └── validation_exports/       # Validation tool exports
            ├── doccano/             # Doccano JSONL exports
            └── labelstudio/         # Label Studio JSONL exports

        Parameters
        ----------
        session_id : str
            Session identifier (e.g., 'sentiment_analysis_20251008_143022')

        Returns
        -------
        dict
            Dictionary with all directory paths
        """
        from pathlib import Path

        # Base directory for annotator logs (parallel to training_arena)
        base_dir = Path("logs") / "annotator" / session_id

        # Create subdirectories
        dirs = {
            'base': base_dir,
            'annotated_data': base_dir / 'annotated_data',
            'metadata': base_dir / 'metadata',
            'validation_exports': base_dir / 'validation_exports',
            'doccano': base_dir / 'validation_exports' / 'doccano',
            'labelstudio': base_dir / 'validation_exports' / 'labelstudio',
        }

        # Create all directories
        for dir_path in dirs.values():
            dir_path.mkdir(parents=True, exist_ok=True)

        return dirs

    def _create_annotator_factory_session_directories(self, session_id: str) -> dict:
        """
        Create organized directory structure for Annotator Factory session.

        Structure combining annotation + training (Mode 1 + Mode 3):
        logs/annotator_factory/{session_id}/
        ├── annotated_data/          # Annotated files (.csv, .xlsx, etc.)
        ├── metadata/                 # Metadata JSON files
        ├── validation_exports/       # Validation tool exports
        │   ├── doccano/
        │   └── labelstudio/
        ├── training_metrics/         # Training metrics (like Mode 3)
        └── training_data/           # Training data reports (like Mode 3)

        Parameters
        ----------
        session_id : str
            Session identifier (e.g., 'sentiment_pipeline_20251008_143022')

        Returns
        -------
        dict
            Dictionary with all directory paths
        """
        from pathlib import Path

        # Base directory for annotator_factory logs
        base_dir = Path("logs") / "annotator_factory" / session_id

        # Create subdirectories (annotation + training)
        dirs = {
            'base': base_dir,
            'annotated_data': base_dir / 'annotated_data',
            'metadata': base_dir / 'metadata',
            'validation_exports': base_dir / 'validation_exports',
            'doccano': base_dir / 'validation_exports' / 'doccano',
            'labelstudio': base_dir / 'validation_exports' / 'labelstudio',
            'training_metrics': base_dir / 'training_metrics',  # For model training results
            'training_data': base_dir / 'training_data',  # For data distribution reports
        }

        # Create all directories
        for dir_path in dirs.values():
            dir_path.mkdir(parents=True, exist_ok=True)

        return dirs

    def _smart_annotate(self, session_id: str = None):
        """Smart guided annotation wizard with all options

        Parameters
        ----------
        session_id : str, optional
            Session identifier for organizing outputs. If None, a timestamp-based ID is generated.
        """
        import pandas as pd
        from datetime import datetime

        # Generate session_id if not provided (for backward compatibility)
        if session_id is None:
            session_id = f"annotation_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        # Create session directories
        session_dirs = self._create_annotator_session_directories(session_id)

        self.console.print("\n[bold cyan]🎯 Smart Annotate - Guided Wizard[/bold cyan]\n")
    
        # Step 1: Data Selection
        self.console.print("[bold]Step 1/7: Data Selection[/bold]")

        if not self.detected_datasets:
            self.console.print("[yellow]No datasets auto-detected.[/yellow]")
            data_path = Path(self._prompt_file_path("Dataset path"))
        else:
            self.console.print(f"\n[bold cyan]📊 Found {len(self.detected_datasets)} dataset(s):[/bold cyan]\n")

            # Create table for datasets
            datasets_table = Table(border_style="cyan", show_header=True)
            datasets_table.add_column("#", style="bold yellow", width=4)
            datasets_table.add_column("Filename", style="white")
            datasets_table.add_column("Format", style="green", width=10)
            datasets_table.add_column("Size", style="magenta", width=10)
            datasets_table.add_column("Rows", style="cyan", width=10)
            datasets_table.add_column("Columns", style="blue", width=10)

            for i, ds in enumerate(self.detected_datasets[:20], 1):
                # Format size
                if ds.size_mb < 0.1:
                    size_str = f"{ds.size_mb * 1024:.1f} KB"
                else:
                    size_str = f"{ds.size_mb:.1f} MB"

                # Format rows and columns
                rows_str = f"{ds.rows:,}" if ds.rows else "?"
                cols_str = str(len(ds.columns)) if ds.columns else "?"

                datasets_table.add_row(
                    str(i),
                    ds.path.name,
                    ds.format.upper(),
                    size_str,
                    rows_str,
                    cols_str
                )

            self.console.print(datasets_table)
            self.console.print()

            use_detected = Confirm.ask("[bold yellow]Use detected dataset?[/bold yellow]", default=True)
            if use_detected:
                choice = self._int_prompt_with_validation("Select dataset", 1, 1, len(self.detected_datasets))
                data_path = self.detected_datasets[choice - 1].path
            else:
                data_path = Path(self._prompt_file_path("Dataset path"))
    
        # Detect format
        data_format = data_path.suffix[1:].lower()
        if data_format == 'xlsx':
            data_format = 'excel'
    
        self.console.print(f"[green]✓ Selected: {data_path.name} ({data_format})[/green]")
    
        # Step 2: Text column selection with intelligent detection
        self.console.print("\n[bold]Step 2/7: Text Column Selection[/bold]")
    
        # Detect text columns
        column_info = self._detect_text_columns(data_path)
    
        if column_info['text_candidates']:
            self.console.print("\n[dim]Detected text columns (sorted by confidence):[/dim]")
    
            # Create table for candidates
            col_table = Table(border_style="blue")
            col_table.add_column("#", style="cyan", width=3)
            col_table.add_column("Column", style="white")
            col_table.add_column("Confidence", style="yellow")
            col_table.add_column("Avg Length", style="green")
            col_table.add_column("Sample", style="dim")
    
            for i, candidate in enumerate(column_info['text_candidates'][:10], 1):
                # Color code confidence
                conf_color = {
                    "high": "[green]High[/green]",
                    "medium": "[yellow]Medium[/yellow]",
                    "low": "[orange1]Low[/orange1]",
                    "very_low": "[red]Very Low[/red]"
                }
                conf_display = conf_color.get(candidate['confidence'], candidate['confidence'])
    
                col_table.add_row(
                    str(i),
                    candidate['name'],
                    conf_display,
                    f"{candidate['avg_length']:.0f} chars",
                    candidate['sample'][:50] + "..." if len(candidate['sample']) > 50 else candidate['sample']
                )
    
            self.console.print(col_table)
    
            # Show all columns option
            self.console.print(f"\n[dim]All columns ({len(column_info['all_columns'])}): {', '.join(column_info['all_columns'])}[/dim]")
    
            # Ask user to select
            default_col = column_info['text_candidates'][0]['name'] if column_info['text_candidates'] else "text"
            text_column = Prompt.ask(
                "\n[bold yellow]Enter column name[/bold yellow] (or choose from above)",
                default=default_col
            )
        else:
            # No candidates detected, show all columns
            if column_info['all_columns']:
                self.console.print(f"\n[yellow]Could not auto-detect text columns.[/yellow]")
                self.console.print(f"[dim]Available columns: {', '.join(column_info['all_columns'])}[/dim]")
            text_column = Prompt.ask("Text column name", default="text")
    
        # Step 2b: ID Column Selection (FROM QUICK START)
        self.console.print("\n[bold]Step 2b/7: Identifier Column Selection[/bold]")
    
        # Get available columns
        available_columns = column_info['all_columns'] if column_info and column_info.get('all_columns') else []
    
        # Auto-detect potential ID columns
        suggested_id = None
        if available_columns:
            for col in available_columns:
                lowered = col.lower()
                if lowered == 'id' or lowered.endswith('_id') or 'identifier' in lowered:
                    suggested_id = col
                    break
    
        identifier_column = self._prompt_for_identifier_column(available_columns, suggested_id)
    
        self.console.print(f"[green]✓ Identifier strategy: {identifier_column}[/green]")
    
        # Check if user wants to return to menu
        if self._check_return_to_menu("with column configuration"):
            return
    
        # Step 3: Model Selection
        self.console.print("\n[bold]Step 3/7: Model Selection[/bold]")
        self.console.print("[dim]Tested API models: OpenAI & Anthropic[/dim]\n")
    
        selected_llm = self._select_llm_interactive()
        provider = selected_llm.provider
        model_name = selected_llm.name
    
        # Get API key if needed
        api_key = None
        if selected_llm.requires_api_key:
            api_key = self._get_or_prompt_api_key(provider, model_name)
    
        # Step 4: Prompt Configuration
        self.console.print("\n[bold]Step 4/7: Prompt Configuration[/bold]")
    
        # Auto-detect prompts
        detected_prompts = self._detect_prompts_in_folder()
    
        if detected_prompts:
            self.console.print(f"\n[green]✓ Found {len(detected_prompts)} prompts in prompts/ folder:[/green]")
            for i, p in enumerate(detected_prompts, 1):
                # Display ALL keys, not truncated
                keys_str = ', '.join(p['keys'])
                self.console.print(f"  {i}. [cyan]{p['name']}[/cyan]")
                self.console.print(f"     Keys ({len(p['keys'])}): {keys_str}")
    
            # Explain the options clearly
            self.console.print("\n[bold]Prompt Selection Options:[/bold]")
            self.console.print("  [cyan]all[/cyan]     - Use ALL detected prompts (multi-prompt mode)")
            self.console.print("           → Each text will be annotated with all prompts")
            self.console.print("           → Useful when you want complete annotations from all perspectives")
            self.console.print("\n  [cyan]select[/cyan]  - Choose SPECIFIC prompts by number (e.g., 1,3,5)")
            self.console.print("           → Only selected prompts will be used")
            self.console.print("           → Useful when testing or when you need only certain annotations")
            self.console.print("\n  [cyan]wizard[/cyan]  - 🧙‍♂️ Create NEW prompt using Social Science Wizard")
            self.console.print("           → Interactive guided prompt creation")
            self.console.print("           → Optional AI assistance for definitions")
            self.console.print("           → [bold green]Recommended for new research projects![/bold green]")
            self.console.print("\n  [cyan]custom[/cyan]  - Provide path to a prompt file NOT in prompts/ folder")
            self.console.print("           → Use a prompt from another location")
            self.console.print("           → Useful for testing new prompts or one-off annotations")
    
            prompt_choice = Prompt.ask(
                "\n[bold yellow]Prompt selection[/bold yellow]",
                choices=["all", "select", "wizard", "custom"],
                default="all"
            )
    
            selected_prompts = []
            if prompt_choice == "all":
                selected_prompts = detected_prompts
                self.console.print(f"[green]✓ Using all {len(selected_prompts)} prompts[/green]")
            elif prompt_choice == "select":
                indices = Prompt.ask("Enter prompt numbers (comma-separated, e.g., 1,3,5)")
                if indices.strip():  # Only process if not empty
                    for idx_str in indices.split(','):
                        idx_str = idx_str.strip()
                        if idx_str:  # Skip empty strings
                            try:
                                idx = int(idx_str) - 1
                                if 0 <= idx < len(detected_prompts):
                                    selected_prompts.append(detected_prompts[idx])
                            except ValueError:
                                self.console.print(f"[yellow]⚠️  Skipping invalid number: '{idx_str}'[/yellow]")
                if not selected_prompts:
                    self.console.print("[yellow]No valid prompts selected. Using all prompts.[/yellow]")
                    selected_prompts = detected_prompts
                else:
                    self.console.print(f"[green]✓ Selected {len(selected_prompts)} prompts[/green]")
            elif prompt_choice == "wizard":
                # Launch Social Science Wizard
                wizard_prompt = self._run_social_science_wizard()
                from ..annotators.json_cleaner import extract_expected_keys
                keys = extract_expected_keys(wizard_prompt)
                selected_prompts = [{
                    'path': None,  # Wizard-generated, not from file
                    'name': 'wizard_generated',
                    'keys': keys,
                    'content': wizard_prompt
                }]
                self.console.print(f"[green]✓ Using wizard-generated prompt with {len(keys)} keys[/green]")
            else:
                # Custom path
                custom_path = Path(self._prompt_file_path("Prompt file path (.txt)"))
                content = custom_path.read_text(encoding='utf-8')
                from ..annotators.json_cleaner import extract_expected_keys
                keys = extract_expected_keys(content)
                selected_prompts = [{
                    'path': custom_path,
                    'name': custom_path.stem,
                    'keys': keys,
                    'content': content
                }]
        else:
            self.console.print("[yellow]No prompts found in prompts/ folder[/yellow]")
    
            # Offer wizard or custom path
            self.console.print("\n[bold]Prompt Options:[/bold]")
            self.console.print("  [cyan]wizard[/cyan] - 🧙‍♂️ Create prompt using Social Science Wizard (Recommended)")
            self.console.print("  [cyan]custom[/cyan] - Provide path to existing prompt file")
    
            choice = Prompt.ask(
                "\n[bold yellow]Select option[/bold yellow]",
                choices=["wizard", "custom"],
                default="wizard"
            )
    
            if choice == "wizard":
                # Launch Social Science Wizard
                wizard_prompt = self._run_social_science_wizard()
                from ..annotators.json_cleaner import extract_expected_keys
                keys = extract_expected_keys(wizard_prompt)
                selected_prompts = [{
                    'path': None,  # Wizard-generated, not from file
                    'name': 'wizard_generated',
                    'keys': keys,
                    'content': wizard_prompt
                }]
                self.console.print(f"[green]✓ Using wizard-generated prompt with {len(keys)} keys[/green]")
            else:
                custom_path = Path(self._prompt_file_path("Prompt file path (.txt)"))
                content = custom_path.read_text(encoding='utf-8')
                from ..annotators.json_cleaner import extract_expected_keys
                keys = extract_expected_keys(content)
                selected_prompts = [{
                    'path': custom_path,
                    'name': custom_path.stem,
                    'keys': keys,
                    'content': content
                }]
    
    
        # Step 4b: Language Column Detection (FROM QUICK START)
        self.console.print("\n[bold]Step 4b/7: Language Column Detection[/bold]")

        lang_column = None
        available_columns = column_info.get('all_columns', []) if column_info else []
        if available_columns:
            # Detect potential language columns
            potential_lang_cols = [col for col in available_columns
                                  if col.lower() in ['lang', 'language', 'langue', 'lng', 'iso_lang']]
    
            if potential_lang_cols:
                self.console.print(f"\n[bold cyan]🌍 Found language column(s):[/bold cyan]")
                for col in potential_lang_cols:
                    self.console.print(f"  • [cyan]{col}[/cyan]")
    
                use_lang_col = Confirm.ask("Use a language column for training metadata?", default=True)
                if use_lang_col:
                    if len(potential_lang_cols) == 1:
                        lang_column = potential_lang_cols[0]
                        self.console.print(f"[green]✓ Using language column: {lang_column}[/green]")
                    else:
                        lang_column = Prompt.ask(
                            "Which language column to use?",
                            choices=potential_lang_cols,
                            default=potential_lang_cols[0]
                        )
                else:
                    # Ask if automatic language detection is needed
                    auto_detect = Confirm.ask(
                        "[yellow]⚠️  Language information is needed for training. Enable automatic language detection?[/yellow]",
                        default=True
                    )
                    if auto_detect:
                        self.console.print("[dim]Language will be automatically detected for each text during annotation.[/dim]")
                        lang_column = None  # Will trigger auto-detection later
                    else:
                        self.console.print("[yellow]⚠️  Warning: Proceeding without language information may affect training quality.[/yellow]")
                        lang_column = None
            else:
                # No language column detected
                has_lang = Confirm.ask("Does your dataset have a language column?", default=False)
                if has_lang:
                    lang_column = Prompt.ask(
                        "Language column name",
                        choices=available_columns,
                        default=available_columns[0] if available_columns else "language"
                    )
        # Step 5: Multi-prompt prefix configuration
        prompt_configs = []
        if len(selected_prompts) > 1:
            self.console.print("\n[bold]Multi-Prompt Mode:[/bold] Configure key prefixes")
            self.console.print("[dim]Prefixes help identify which prompt generated which keys[/dim]\n")
    
            for i, prompt in enumerate(selected_prompts, 1):
                self.console.print(f"\n[cyan]Prompt {i}: {prompt['name']}[/cyan]")
                self.console.print(f"  Keys: {', '.join(prompt['keys'])}")
    
                add_prefix = Confirm.ask(f"Add prefix to keys for this prompt?", default=True)
                prefix = ""
                if add_prefix:
                    default_prefix = prompt['name'].lower().replace(' ', '_')
                    prefix = Prompt.ask("Prefix", default=default_prefix)
                    self.console.print(f"  [green]Keys will become: {', '.join([f'{prefix}_{k}' for k in prompt['keys'][:3]])}[/green]")
    
                prompt_configs.append({
                    'prompt': prompt,
                    'prefix': prefix
                })
        else:
            # Single prompt - no prefix needed
            prompt_configs = [{'prompt': selected_prompts[0], 'prefix': ''}]
    
        # Step 6: Advanced Options
        self.console.print("\n[bold]Step 5/7: Advanced Options[/bold]")
    
        # ============================================================
        # DATASET SCOPE
        # ============================================================
        self.console.print("\n[bold cyan]📊 Dataset Scope[/bold cyan]")
        self.console.print("[dim]Determine how many rows to annotate from your dataset[/dim]\n")
    
        # Get total rows if possible
        total_rows = None
        if column_info.get('df') is not None:
            # We have a sample, extrapolate
            total_rows = len(pd.read_csv(data_path)) if data_format == 'csv' else None
    
        if total_rows:
            self.console.print(f"[green]✓ Dataset contains {total_rows:,} rows[/green]\n")
    
        # Option 1: Annotate all or limited
        self.console.print("[yellow]Option 1:[/yellow] Annotate ALL rows vs LIMIT to specific number")
        self.console.print("  • [cyan]all[/cyan]   - Annotate the entire dataset")
        self.console.print("           [dim]Use this for production annotations[/dim]")
        self.console.print("  • [cyan]limit[/cyan] - Specify exact number of rows to annotate")
        self.console.print("           [dim]Use this for testing or partial annotation[/dim]")

        scope_choice = Prompt.ask(
            "\nAnnotate entire dataset or limit rows?",
            choices=["all", "limit"],
            default="all"
        )

        annotation_limit = None
        use_sample = False
        sample_strategy = "head"
        recommended_sample = None

        if scope_choice == "limit":
            # Option 2: FIRST ask about representative sample calculation (before asking for number)
            if total_rows and total_rows > 1000:
                self.console.print("\n[yellow]Option 2:[/yellow] Representative Sample Calculation")
                self.console.print("  Calculate statistically representative sample size (95% confidence interval)")
                self.console.print("  [dim]This helps determine the minimum sample needed for statistical validity[/dim]")

                calculate_sample = Confirm.ask("Calculate representative sample size?", default=True)

                if calculate_sample:
                    # Formula: n = (Z² × p × (1-p)) / E²
                    # For 95% CI: Z=1.96, p=0.5 (max variance), E=0.05 (5% margin)
                    import math
                    z = 1.96
                    p = 0.5
                    e = 0.05
                    n_infinite = (z**2 * p * (1-p)) / (e**2)
                    n_adjusted = n_infinite / (1 + ((n_infinite - 1) / total_rows))
                    recommended_sample = int(math.ceil(n_adjusted))

                    self.console.print(f"\n[green]📈 Recommended sample size: {recommended_sample} rows[/green]")
                    self.console.print(f"[dim]   (95% confidence level, 5% margin of error)[/dim]")
                    self.console.print(f"[dim]   Population: {total_rows:,} rows[/dim]\n")

            # THEN ask for specific number (with recommendation as default if calculated)
            default_limit = recommended_sample if recommended_sample else 100
            annotation_limit = self._int_prompt_with_validation(
                f"How many rows to annotate?",
                default=default_limit,
                min_value=1,
                max_value=total_rows if total_rows else 1000000
            )

            # Check if user chose the recommended sample
            if recommended_sample and annotation_limit == recommended_sample:
                use_sample = True

            # Option 3: Random sampling
            self.console.print("\n[yellow]Option 3:[/yellow] Sampling Strategy")
            self.console.print("  Choose how to select the rows to annotate")
            self.console.print("  • [cyan]head[/cyan]   - Take first N rows (faster, sequential)")
            self.console.print("           [dim]Good for testing, preserves order[/dim]")
            self.console.print("  • [cyan]random[/cyan] - Random sample of N rows (representative)")
            self.console.print("           [dim]Better for statistical validity, unbiased[/dim]")
    
            sample_strategy = Prompt.ask(
                "\nSampling strategy",
                choices=["head", "random"],
                default="random" if use_sample else "head"
            )
    
        # ============================================================
        # PARALLEL PROCESSING
        # ============================================================
        self.console.print("\n[bold cyan]⚙️  Parallel Processing[/bold cyan]")
        self.console.print("[dim]Configure how many processes run simultaneously[/dim]\n")
    
        self.console.print("[yellow]Parallel Workers:[/yellow]")
        self.console.print("  Number of simultaneous annotation processes")
        self.console.print("\n  [red]⚠️  IMPORTANT:[/red]")
        self.console.print("  [dim]Most local machines can only handle 1 worker for LLM inference[/dim]")
        self.console.print("  [dim]Parallel processing is mainly useful for API models[/dim]")
        self.console.print("\n  • [cyan]1 worker[/cyan]  - Sequential processing")
        self.console.print("           [dim]Recommended for: Local models (Ollama), first time users, debugging[/dim]")
        self.console.print("  • [cyan]2-4 workers[/cyan] - Moderate parallelism")
        self.console.print("           [dim]Recommended for: API models (OpenAI, Claude) - avoid rate limits[/dim]")
        self.console.print("  • [cyan]4-8 workers[/cyan] - High parallelism")
        self.console.print("           [dim]Recommended for: API models only - requires high rate limits[/dim]")
    
        num_processes = self._int_prompt_with_validation("Parallel workers", 1, 1, 16)
    
        # ============================================================
        # INCREMENTAL SAVE
        # ============================================================
        self.console.print("\n[bold cyan]💾 Incremental Save[/bold cyan]")
        self.console.print("[dim]Configure how often results are saved during annotation[/dim]\n")
    
        self.console.print("[yellow]Enable incremental save?[/yellow]")
        self.console.print("  • [green]Yes[/green] - Save progress regularly during annotation (recommended)")
        self.console.print("           [dim]Protects against crashes, allows resuming, safer for long runs[/dim]")
        self.console.print("  • [red]No[/red]  - Save only at the end")
        self.console.print("           [dim]Faster but risky - you lose everything if process crashes[/dim]")
    
        save_incrementally = Confirm.ask("\n💿 Enable incremental save?", default=True)
    
        # Only ask for batch size if incremental save is enabled
        if save_incrementally:
            self.console.print("\n[yellow]Batch Size:[/yellow]")
            self.console.print("  Number of rows processed between each save")
            self.console.print("  • [cyan]Smaller (1-10)[/cyan]   - Very frequent saves, maximum safety")
            self.console.print("           [dim]Use for: Unstable systems, expensive APIs, testing[/dim]")
            self.console.print("  • [cyan]Medium (10-50)[/cyan]   - Balanced safety and performance")
            self.console.print("           [dim]Use for: Most production cases[/dim]")
            self.console.print("  • [cyan]Larger (50-200)[/cyan]  - Less frequent saves, better performance")
            self.console.print("           [dim]Use for: Stable systems, large datasets, local models[/dim]")
    
            batch_size = self._int_prompt_with_validation("Batch size", 1, 1, 1000)
        else:
            batch_size = None  # Not used when incremental save is disabled
    
        # ============================================================
        # MODEL PARAMETERS
        # ============================================================
        self.console.print("\n[bold cyan]🎛️  Model Parameters[/bold cyan]")
        self.console.print("[dim]Configure advanced model generation parameters[/dim]\n")
    
        # Check if model supports parameter tuning
        model_name_lower = model_name.lower()
        is_o_series = any(x in model_name_lower for x in ['o1', 'o3', 'o4'])
        supports_params = not is_o_series
    
        if not supports_params:
            self.console.print(f"[yellow]⚠️  Model '{model_name}' uses fixed parameters (reasoning model)[/yellow]")
            self.console.print("[dim]   Temperature and top_p are automatically set to 1.0[/dim]")
            configure_params = False
        else:
            self.console.print("[yellow]Configure model parameters?[/yellow]")
            self.console.print("  Adjust how the model generates responses")
            self.console.print("  [dim]• Default values work well for most cases[/dim]")
            self.console.print("  [dim]• Advanced users can fine-tune for specific needs[/dim]")
            configure_params = Confirm.ask("\nConfigure model parameters?", default=False)
    
        # Default values
        temperature = 0.7
        max_tokens = 1000
        top_p = 1.0
        top_k = 40
    
        if configure_params:
            self.console.print("\n[bold]Parameter Explanations:[/bold]\n")
    
            # Temperature
            self.console.print("[cyan]🌡️  Temperature (0.0 - 2.0):[/cyan]")
            self.console.print("  Controls randomness in responses")
            self.console.print("  • [green]Low (0.0-0.3)[/green]  - Deterministic, focused, consistent")
            self.console.print("           [dim]Use for: Structured tasks, factual extraction, classification[/dim]")
            self.console.print("  • [yellow]Medium (0.4-0.9)[/yellow] - Balanced creativity and consistency")
            self.console.print("           [dim]Use for: General annotation, most use cases[/dim]")
            self.console.print("  • [red]High (1.0-2.0)[/red]  - Creative, varied, unpredictable")
            self.console.print("           [dim]Use for: Brainstorming, diverse perspectives[/dim]")
            temperature = FloatPrompt.ask("Temperature", default=0.7)
    
            # Max tokens
            self.console.print("\n[cyan]📏 Max Tokens:[/cyan]")
            self.console.print("  Maximum length of the response")
            self.console.print("  • [green]Short (100-500)[/green]   - Brief responses, simple annotations")
            self.console.print("  • [yellow]Medium (500-2000)[/yellow]  - Standard responses, detailed annotations")
            self.console.print("  • [red]Long (2000+)[/red]     - Extensive responses, complex reasoning")
            self.console.print("  [dim]Note: More tokens = higher API costs[/dim]")
            max_tokens = self._int_prompt_with_validation("Max tokens", 1000, 50, 8000)
    
            # Top_p (nucleus sampling)
            self.console.print("\n[cyan]🎯 Top P (0.0 - 1.0):[/cyan]")
            self.console.print("  Nucleus sampling - alternative to temperature")
            self.console.print("  • [green]Low (0.1-0.5)[/green]  - Focused on most likely tokens")
            self.console.print("           [dim]More deterministic, safer outputs[/dim]")
            self.console.print("  • [yellow]High (0.9-1.0)[/yellow] - Consider broader token range")
            self.console.print("           [dim]More creative, diverse outputs[/dim]")
            self.console.print("  [dim]Tip: Use either temperature OR top_p, not both aggressively[/dim]")
            top_p = FloatPrompt.ask("Top P", default=1.0)
    
            # Top_k (only for some models)
            if provider in ['ollama', 'google']:
                self.console.print("\n[cyan]🔢 Top K:[/cyan]")
                self.console.print("  Limits vocabulary to K most likely next tokens")
                self.console.print("  • [green]Small (1-10)[/green]   - Very focused, repetitive")
                self.console.print("  • [yellow]Medium (20-50)[/yellow]  - Balanced diversity")
                self.console.print("  • [red]Large (50+)[/red]    - Maximum diversity")
                top_k = self._int_prompt_with_validation("Top K", 40, 1, 100)
    
        # Step 7: Execute
        self.console.print("\n[bold]Step 6/7: Review & Execute[/bold]")
    
        # Display comprehensive summary
        summary_table = Table(title="Configuration Summary", border_style="cyan", show_header=True)
        summary_table.add_column("Category", style="bold cyan", width=20)
        summary_table.add_column("Setting", style="yellow", width=25)
        summary_table.add_column("Value", style="white")
    
        # Data section
        summary_table.add_row("📁 Data", "Dataset", str(data_path.name))
        summary_table.add_row("", "Format", data_format.upper())
        summary_table.add_row("", "Text Column", text_column)
        if total_rows:
            summary_table.add_row("", "Total Rows", f"{total_rows:,}")
        if annotation_limit:
            summary_table.add_row("", "Rows to Annotate", f"{annotation_limit:,} ({sample_strategy})")
        else:
            summary_table.add_row("", "Rows to Annotate", "ALL")
    
        # Model section
        summary_table.add_row("🤖 Model", "Provider/Model", f"{provider}/{model_name}")
        summary_table.add_row("", "Temperature", f"{temperature}")
        summary_table.add_row("", "Max Tokens", f"{max_tokens}")
        if configure_params:
            summary_table.add_row("", "Top P", f"{top_p}")
            if provider in ['ollama', 'google']:
                summary_table.add_row("", "Top K", f"{top_k}")
    
        # Prompts section
        summary_table.add_row("📝 Prompts", "Count", f"{len(prompt_configs)}")
        for i, pc in enumerate(prompt_configs, 1):
            prefix_info = f" (prefix: {pc['prefix']}_)" if pc['prefix'] else " (no prefix)"
            summary_table.add_row("", f"  Prompt {i}", f"{pc['prompt']['name']}{prefix_info}")
    
        # Processing section
        summary_table.add_row("⚙️  Processing", "Parallel Workers", str(num_processes))
        summary_table.add_row("", "Batch Size", str(batch_size))
        summary_table.add_row("", "Incremental Save", "Yes" if save_incrementally else "No")
    
        self.console.print("\n")
        self.console.print(summary_table)
    
        if not Confirm.ask("\n[bold yellow]Start annotation?[/bold yellow]", default=True):
            return
    
        # ============================================================
        # REPRODUCIBILITY METADATA
        # ============================================================
        self.console.print("\n[bold cyan]📋 Reproducibility & Metadata[/bold cyan]")
        self.console.print("[yellow]⚠️  IMPORTANT: Save parameters for two critical purposes:[/yellow]\n")
    
        self.console.print("  [green]1. Resume Capability[/green]")
        self.console.print("     • Continue this annotation if it stops or crashes")
        self.console.print("     • Annotate additional rows later with same settings")
        self.console.print("     • Access via 'Resume/Relaunch Annotation' workflow\n")
    
        self.console.print("  [green]2. Scientific Reproducibility[/green]")
        self.console.print("     • Document exact parameters for research papers")
        self.console.print("     • Reproduce identical annotations in the future")
        self.console.print("     • Track model version, prompts, and all settings\n")
    
        self.console.print("  [red]⚠️  If you choose NO:[/red]")
        self.console.print("     • You CANNOT resume this annotation later")
        self.console.print("     • You CANNOT relaunch with same parameters")
        self.console.print("     • Parameters will be lost forever\n")
    
        save_metadata = Confirm.ask(
            "[bold yellow]Save annotation parameters to JSON file?[/bold yellow]",
            default=True
        )
    
        # ============================================================
        # VALIDATION TOOL EXPORT OPTION
        # ============================================================
        self.console.print("\n[bold cyan]📤 Validation Tool Export[/bold cyan]")
        self.console.print("[dim]Export annotations to JSONL format for human validation[/dim]\n")
    
        self.console.print("[yellow]Available validation tools:[/yellow]")
        self.console.print("  • [cyan]Doccano[/cyan] - Simple, lightweight NLP annotation tool")
        self.console.print("  • [cyan]Label Studio[/cyan] - Advanced, feature-rich annotation platform")
        self.console.print("  • Both are open-source and free\n")
    
        self.console.print("[green]Why validate with external tools?[/green]")
        self.console.print("  • Review and correct LLM annotations")
        self.console.print("  • Calculate inter-annotator agreement")
        self.console.print("  • Export validated data for metrics calculation\n")
    
        # Initialize export flags
        export_to_doccano = False
        export_to_labelstudio = False
        export_sample_size = None
    
        # Step 1: Ask if user wants to export
        export_confirm = Confirm.ask(
            "[bold yellow]Export to validation tool?[/bold yellow]",
            default=False
        )
    
        if export_confirm:
            # Step 2: Ask which tool to export to
            tool_choice = Prompt.ask(
                "[bold yellow]Which validation tool?[/bold yellow]",
                choices=["doccano", "labelstudio"],
                default="doccano"
            )
    
            # Set the appropriate export flag
            if tool_choice == "doccano":
                export_to_doccano = True
            else:  # labelstudio
                export_to_labelstudio = True
    
            # Step 2b: If Label Studio, ask export method
            labelstudio_direct_export = False
            labelstudio_api_url = None
            labelstudio_api_key = None
    
            if export_to_labelstudio:
                self.console.print("\n[yellow]Label Studio export method:[/yellow]")
                self.console.print("  • [cyan]jsonl[/cyan] - Export to JSONL file (manual import)")
                if HAS_REQUESTS:
                    self.console.print("  • [cyan]direct[/cyan] - Direct export to Label Studio via API\n")
                    export_choices = ["jsonl", "direct"]
                else:
                    self.console.print("  • [dim]direct[/dim] - Direct export via API [dim](requires 'requests' library)[/dim]\n")
                    export_choices = ["jsonl"]
    
                export_method = Prompt.ask(
                    "[bold yellow]Export method[/bold yellow]",
                    choices=export_choices,
                    default="jsonl"
                )
    
                if export_method == "direct":
                    labelstudio_direct_export = True
    
                    self.console.print("\n[cyan]Label Studio API Configuration:[/cyan]")
                    labelstudio_api_url = Prompt.ask(
                        "Label Studio URL",
                        default="http://localhost:8080"
                    )
    
                    labelstudio_api_key = Prompt.ask(
                        "API Key (from Label Studio Account & Settings)"
                    )
    
            # Step 3: Ask about LLM predictions inclusion
            self.console.print("\n[yellow]Include LLM predictions in export?[/yellow]")
            self.console.print("  • [cyan]with[/cyan] - Include LLM annotations as predictions (for review/correction)")
            self.console.print("  • [cyan]without[/cyan] - Export only data without predictions (for manual annotation)")
            self.console.print("  • [cyan]both[/cyan] - Create two files: one with and one without predictions\n")
    
            prediction_mode = Prompt.ask(
                "[bold yellow]Prediction mode[/bold yellow]",
                choices=["with", "without", "both"],
                default="with"
            )
    
            # Step 4: Ask how many sentences to export
            self.console.print("\n[yellow]How many annotated sentences to export?[/yellow]")
            self.console.print("  • [cyan]all[/cyan] - Export all annotated sentences")
            self.console.print("  • [cyan]representative[/cyan] - Representative sample (stratified by labels)")
            self.console.print("  • [cyan]number[/cyan] - Specify exact number\n")
    
            sample_choice = Prompt.ask(
                "[bold yellow]Export sample[/bold yellow]",
                choices=["all", "representative", "number"],
                default="all"
            )
    
            if sample_choice == "all":
                export_sample_size = "all"
            elif sample_choice == "representative":
                export_sample_size = "representative"
            else:  # number
                export_sample_size = self._int_prompt_with_validation(
                    "Number of sentences to export",
                    100,
                    1,
                    999999
                )
    
        # ============================================================
        # EXECUTE ANNOTATION
        # ============================================================
    
        # CRITICAL: Use new organized structure with dataset-specific subfolder
        # Structure: logs/annotator/{session_id}/annotated_data/{dataset_name}/
        safe_model_name = model_name.replace(':', '_').replace('/', '_')
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # Create dataset-specific subdirectory (like {category} in Training Arena)
        dataset_name = data_path.stem
        dataset_subdir = session_dirs['annotated_data'] / dataset_name
        dataset_subdir.mkdir(parents=True, exist_ok=True)

        output_filename = f"{data_path.stem}_{safe_model_name}_annotations_{timestamp}.{data_format}"
        default_output_path = dataset_subdir / output_filename
    
        self.console.print(f"\n[bold cyan]📁 Output Location:[/bold cyan]")
        self.console.print(f"   {default_output_path}")
        self.console.print()
    
        # Prepare prompts payload for pipeline
        prompts_payload = []
        for pc in prompt_configs:
            prompts_payload.append({
                'prompt': pc['prompt']['content'],
                'expected_keys': pc['prompt']['keys'],
                'prefix': pc['prefix']
            })
    
        # Determine annotation mode
        annotation_mode = 'api' if provider in {'openai', 'anthropic', 'google'} else 'local'
    
        # Build pipeline config
        pipeline_config = {
            'mode': 'file',
            'data_source': data_format,
            'data_format': data_format,
            'file_path': str(data_path),
            'text_column': text_column,
            'text_columns': [text_column],
            'annotation_column': 'annotation',
            'identifier_column': identifier_column,  # From Step 2b: User-selected ID strategy
            'run_annotation': True,
            'annotation_mode': annotation_mode,
            'annotation_provider': provider,
            'annotation_model': model_name,
            'api_key': api_key if api_key else None,
            'prompts': prompts_payload,
            'annotation_sample_size': annotation_limit,
            'annotation_sampling_strategy': sample_strategy if annotation_limit else 'head',
            'annotation_sample_seed': 42,
            'max_tokens': max_tokens,
            'temperature': temperature,
            'top_p': top_p,
            'top_k': top_k if provider in ['ollama', 'google'] else None,
            'max_workers': num_processes,
            'num_processes': num_processes,
            'use_parallel': num_processes > 1,
            'warmup': False,
            'disable_tqdm': True,  # Use Rich progress instead
            'output_format': data_format,
            'output_path': str(default_output_path),
            'save_incrementally': save_incrementally,
            'batch_size': batch_size,
            'run_validation': False,
            'run_training': False,
            'lang_column': lang_column,  # From Step 4b: Language column for training metadata
        }
    
        # Add model-specific options
        if provider == 'ollama':
            options = {
                'temperature': temperature,
                'num_predict': max_tokens,
                'top_p': top_p,
                'top_k': top_k
            }
            pipeline_config['options'] = options
    
        # ============================================================
        # SAVE REPRODUCIBILITY METADATA
        # ============================================================
        if save_metadata:
            import json
    
            # Build comprehensive metadata
            metadata = {
                'annotation_session': {
                    'timestamp': timestamp,
                    'tool_version': 'LLMTool v1.0',
                    'workflow': 'The Annotator - Smart Annotate'
                },
                'data_source': {
                    'file_path': str(data_path),
                    'file_name': data_path.name,
                    'data_format': data_format,
                    'text_column': text_column,
                    'total_rows': annotation_limit if annotation_limit else 'all',
                    'sampling_strategy': sample_strategy if annotation_limit else 'none (all rows)',
                    'sample_seed': 42 if sample_strategy == 'random' else None
                },
                'model_configuration': {
                    'provider': provider,
                    'model_name': model_name,
                    'annotation_mode': annotation_mode,
                    'temperature': temperature,
                    'max_tokens': max_tokens,
                    'top_p': top_p,
                    'top_k': top_k if provider in ['ollama', 'google'] else None
                },
                'prompts': [
                    {
                        'name': pc['prompt']['name'],
                        'file_path': str(pc['prompt']['path']) if 'path' in pc['prompt'] else None,
                        'expected_keys': pc['prompt']['keys'],
                        'prefix': pc['prefix'],
                        'prompt_content': pc['prompt']['content']
                    }
                    for pc in prompt_configs
                ],
                'processing_configuration': {
                    'parallel_workers': num_processes,
                    'batch_size': batch_size,
                    'incremental_save': save_incrementally,
                    'identifier_column': 'annotation_id'
                },
                'output': {
                    'output_path': str(default_output_path),
                    'output_format': data_format
                },
                'export_preferences': {
                    'export_to_doccano': export_to_doccano,
                    'export_to_labelstudio': export_to_labelstudio,
                    'export_sample_size': export_sample_size,
                    'prediction_mode': prediction_mode if (export_to_doccano or export_to_labelstudio) else 'with',
                    'labelstudio_direct_export': labelstudio_direct_export if export_to_labelstudio else False,
                    'labelstudio_api_url': labelstudio_api_url if export_to_labelstudio else None,
                    'labelstudio_api_key': labelstudio_api_key if export_to_labelstudio else None
                },
                'training_workflow': {
                    'enabled': False,  # Will be updated after training workflow
                    'training_params_file': None,  # Will be added after training
                    'note': 'Training parameters will be saved separately after annotation completes'
                }
            }

            # Save metadata JSON (PRE-ANNOTATION SAVE POINT 1)
            # Use dataset-specific subdirectory for metadata too
            metadata_subdir = session_dirs['metadata'] / dataset_name
            metadata_subdir.mkdir(parents=True, exist_ok=True)

            metadata_filename = f"{data_path.stem}_{safe_model_name}_metadata_{timestamp}.json"
            metadata_path = metadata_subdir / metadata_filename

            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)

            self.console.print(f"\n[bold green]✅ Metadata saved for reproducibility[/bold green]")
            self.console.print(f"[bold cyan]📋 Metadata File:[/bold cyan]")
            self.console.print(f"   {metadata_path}\n")
    
        # Execute pipeline with Rich progress
        try:
            self.console.print("\n[bold green]🚀 Starting annotation...[/bold green]\n")
    
            # Create pipeline controller
            from ..pipelines.pipeline_controller import PipelineController
            pipeline_with_progress = PipelineController(settings=self.settings)
    
            # Use RichProgressManager for elegant display
            from ..utils.rich_progress_manager import RichProgressManager
            from ..pipelines.enhanced_pipeline_wrapper import EnhancedPipelineWrapper
    
            with RichProgressManager(
                show_json_every=1,  # Show JSON sample for every annotation
                compact_mode=False   # Full preview panels
            ) as progress_manager:
                # Wrap pipeline for enhanced JSON tracking
                enhanced_pipeline = EnhancedPipelineWrapper(
                    pipeline_with_progress,
                    progress_manager
                )
    
                # Run pipeline
                state = enhanced_pipeline.run_pipeline(pipeline_config)
    
                # Check for errors
                if state.errors:
                    error_msg = state.errors[0]['error'] if state.errors else "Annotation failed"
                    self.console.print(f"\n[bold red]❌ Error:[/bold red] {error_msg}")
                    self.console.print("[dim]Press Enter to return to menu...[/dim]")
                    input()
                    return
    
            # Get results
            annotation_results = state.annotation_results or {}
            output_file = annotation_results.get('output_file', str(default_output_path))

            # Display success message
            self.console.print("\n[bold green]✅ Annotation completed successfully![/bold green]")
            self.console.print(f"\n[bold cyan]📄 Output File:[/bold cyan]")
            self.console.print(f"   {output_file}")

            # Display statistics if available
            total_annotated = annotation_results.get('total_annotated', 0)
            if total_annotated:
                self.console.print(f"\n[bold cyan]📊 Statistics:[/bold cyan]")
                self.console.print(f"   Rows annotated: {total_annotated:,}")

                success_count = annotation_results.get('success_count', 0)
                if success_count:
                    success_rate = (success_count / total_annotated * 100)
                    self.console.print(f"   Success rate: {success_rate:.1f}%")

            # ============================================================
            # AUTOMATIC LANGUAGE DETECTION (if no language column provided)
            # ============================================================
            if not lang_column:
                self.console.print("\n[bold cyan]🌍 Language Detection for Training[/bold cyan]")
                self.console.print("[yellow]No language column was provided. Detecting languages for training...[/yellow]\n")

                try:
                    import pandas as pd
                    from llm_tool.utils.language_detector import LanguageDetector

                    # Load annotated file
                    df_annotated = pd.read_csv(output_file)

                    # CRITICAL: Only detect languages for ANNOTATED rows
                    # The output file may contain ALL original rows, but we only want to detect
                    # languages for rows that were actually annotated
                    original_row_count = len(df_annotated)

                    # Try to identify annotated rows by checking for annotation columns
                    # Common annotation column names: 'label', 'category', 'annotation', 'labels'
                    annotation_cols = [col for col in df_annotated.columns if col in ['label', 'labels', 'category', 'annotation', 'predicted_label']]

                    if annotation_cols:
                        # Filter to only rows that have annotations (non-null in annotation column)
                        annotation_col = annotation_cols[0]
                        df_annotated = df_annotated[df_annotated[annotation_col].notna()].copy()
                        self.console.print(f"[dim]Filtering to {len(df_annotated):,} annotated rows (out of {original_row_count:,} total rows in file)[/dim]")
                    else:
                        self.console.print(f"[yellow]⚠️  Could not identify annotation column. Processing all {original_row_count:,} rows.[/yellow]")

                    if len(df_annotated) == 0:
                        self.console.print("[yellow]⚠️  No annotated rows found. Skipping language detection.[/yellow]")
                    elif text_column in df_annotated.columns:
                        # Get ALL texts (including NaN) to maintain index alignment
                        all_texts = df_annotated[text_column].tolist()

                        # Count non-empty texts for display
                        non_empty_texts = sum(1 for text in all_texts if pd.notna(text) and len(str(text).strip()) > 10)

                        if non_empty_texts > 0:
                            detector = LanguageDetector()
                            detected_languages = []

                            # Progress indicator
                            from tqdm import tqdm
                            self.console.print(f"[dim]Analyzing {non_empty_texts} texts...[/dim]")

                            for text in tqdm(all_texts, desc="Detecting languages", disable=not HAS_RICH):
                                # Handle NaN and empty texts
                                if pd.isna(text) or not text or len(str(text).strip()) <= 10:
                                    detected_languages.append('unknown')
                                else:
                                    try:
                                        detected = detector.detect(str(text))
                                        if detected and detected.get('language'):
                                            detected_languages.append(detected['language'])
                                        else:
                                            detected_languages.append('unknown')
                                    except Exception as e:
                                        self.logger.debug(f"Language detection failed for text: {e}")
                                        detected_languages.append('unknown')

                            # Add language column to the filtered dataframe
                            df_annotated['lang'] = detected_languages

                            # Reload the FULL original file and update only the annotated rows
                            df_full = pd.read_csv(output_file)

                            # Initialize lang column if it doesn't exist
                            if 'lang' not in df_full.columns:
                                df_full['lang'] = 'unknown'

                            # Update language for annotated rows only
                            # Match by index of df_annotated within df_full
                            df_full.loc[df_annotated.index, 'lang'] = df_annotated['lang'].values

                            # Save updated full file with language column
                            df_full.to_csv(output_file, index=False)

                            # Show distribution
                            lang_counts = {}
                            for lang in detected_languages:
                                if lang != 'unknown':
                                    lang_counts[lang] = lang_counts.get(lang, 0) + 1

                            if lang_counts:
                                total = sum(lang_counts.values())
                                self.console.print(f"\n[bold]🌍 Languages Detected ({total:,} texts):[/bold]")

                                lang_table = Table(border_style="cyan", show_header=True, header_style="bold")
                                lang_table.add_column("Language", style="cyan", width=12)
                                lang_table.add_column("Count", style="yellow", justify="right", width=12)
                                lang_table.add_column("Percentage", style="green", justify="right", width=12)

                                for lang, count in sorted(lang_counts.items(), key=lambda x: x[1], reverse=True):
                                    percentage = (count / total * 100) if total > 0 else 0
                                    lang_table.add_row(
                                        lang.upper(),
                                        f"{count:,}",
                                        f"{percentage:.1f}%"
                                    )

                                self.console.print(lang_table)
                                self.console.print(f"\n[green]✓ Language column 'lang' added to {output_file}[/green]")
                            else:
                                self.console.print("[yellow]⚠️  No languages detected successfully[/yellow]")

                except Exception as e:
                    self.console.print(f"[yellow]⚠️  Language detection failed: {e}[/yellow]")
                    self.logger.exception("Language detection failed")

            # ============================================================
            # INTELLIGENT TRAINING WORKFLOW (Post-Annotation)
            # ============================================================
            self._post_annotation_training_workflow(
                output_file=output_file,
                text_column=text_column,
                prompt_configs=prompt_configs
            )

            # Export to Doccano JSONL if requested
            if export_to_doccano:
                self._export_to_doccano_jsonl(
                    output_file=output_file,
                    text_column=text_column,
                    prompt_configs=prompt_configs,
                    data_path=data_path,
                    timestamp=timestamp,
                    sample_size=export_sample_size,
                    session_dirs=session_dirs
                )
    
            # Export to Label Studio if requested
            if export_to_labelstudio:
                if labelstudio_direct_export:
                    # Direct export to Label Studio via API
                    self._export_to_labelstudio_direct(
                        output_file=output_file,
                        text_column=text_column,
                        prompt_configs=prompt_configs,
                        data_path=data_path,
                        timestamp=timestamp,
                        sample_size=export_sample_size,
                        prediction_mode=prediction_mode,
                        api_url=labelstudio_api_url,
                        api_key=labelstudio_api_key
                    )
                else:
                    # Export to JSONL file
                    self._export_to_labelstudio_jsonl(
                        output_file=output_file,
                        text_column=text_column,
                        prompt_configs=prompt_configs,
                        data_path=data_path,
                        timestamp=timestamp,
                        sample_size=export_sample_size,
                        prediction_mode=prediction_mode,
                        session_dirs=session_dirs
                    )
    
            self.console.print("\n[dim]Press Enter to return to menu...[/dim]")
            input()
    
        except Exception as exc:
            self.console.print(f"\n[bold red]❌ Annotation failed:[/bold red] {exc}")
            self.logger.exception("Annotation execution failed")
            self.console.print("\n[dim]Press Enter to return to menu...[/dim]")
            input()

    def _display_metadata_parameters(self, metadata: dict):
        """Display all parameters from metadata in a formatted way"""
        self.console.print("\n[bold cyan]📋 Saved Parameters[/bold cyan]\n")

        # Create parameter display table
        params_table = Table(border_style="blue", show_header=False, box=None)
        params_table.add_column("Section", style="yellow bold", width=25)
        params_table.add_column("Details", style="white")

        # Session Info
        session = metadata.get('annotation_session', {})
        params_table.add_row("📅 Session", f"{session.get('workflow', 'N/A')}")
        params_table.add_row("", f"Date: {session.get('timestamp', 'N/A')}")

        # Data Source
        data_source = metadata.get('data_source', {})
        params_table.add_row("📁 Data", f"File: {data_source.get('file_name', 'N/A')}")
        params_table.add_row("", f"Format: {data_source.get('data_format', 'N/A')}")
        params_table.add_row("", f"Text Column: {data_source.get('text_column', 'N/A')}")
        params_table.add_row("", f"Rows: {data_source.get('total_rows', 'N/A')}")
        params_table.add_row("", f"Sampling: {data_source.get('sampling_strategy', 'N/A')}")

        # Model Configuration
        model_config = metadata.get('model_configuration', {})
        params_table.add_row("🤖 Model", f"{model_config.get('provider', 'N/A')}/{model_config.get('model_name', 'N/A')}")
        params_table.add_row("", f"Temperature: {model_config.get('temperature', 'N/A')}")
        params_table.add_row("", f"Max Tokens: {model_config.get('max_tokens', 'N/A')}")

        # Prompts
        prompts = metadata.get('prompts', [])
        params_table.add_row("📝 Prompts", f"Count: {len(prompts)}")
        for i, p in enumerate(prompts[:3], 1):  # Show first 3
            name = p.get('name', f"Prompt {i}")
            keys = p.get('expected_keys', [])
            prefix = p.get('prefix', '')
            keys_str = ', '.join(keys[:5])
            if len(keys) > 5:
                keys_str += f"... ({len(keys)} total)"
            prefix_str = f" [{prefix}_]" if prefix else ""
            params_table.add_row("", f"  {i}. {name}{prefix_str}: {keys_str}")

        # Processing
        proc_config = metadata.get('processing_configuration', {})
        params_table.add_row("⚙️  Processing", f"Workers: {proc_config.get('parallel_workers', 1)}")
        params_table.add_row("", f"Batch Size: {proc_config.get('batch_size', 'N/A')}")

        self.console.print(params_table)

    def _modify_parameters_if_requested(self, metadata: dict, modify: bool) -> dict:
        """Allow user to modify specific parameters"""
        if not modify:
            return metadata

        self.console.print("\n[bold]Select parameter to modify:[/bold]")
        self.console.print("  [cyan]1[/cyan] - Data source (file, text column)")
        self.console.print("  [cyan]2[/cyan] - Model (provider, model name)")
        self.console.print("  [cyan]3[/cyan] - Model parameters (temperature, max_tokens, etc.)")
        self.console.print("  [cyan]4[/cyan] - Prompts (add/remove/modify)")
        self.console.print("  [cyan]5[/cyan] - Sampling (rows to annotate, strategy)")
        self.console.print("  [cyan]6[/cyan] - Processing (workers, batch size)")
        self.console.print("  [cyan]0[/cyan] - Done modifying")

        modified = metadata.copy()

        while True:
            choice = Prompt.ask(
                "\n[bold yellow]Modify which parameter?[/bold yellow]",
                choices=["0", "1", "2", "3", "4", "5", "6"],
                default="0"
            )

            if choice == "0":
                break
            elif choice == "1":
                # Modify data source
                self.console.print("\n[yellow]Current data:[/yellow]")
                data_source = modified.get('data_source', {})
                self.console.print(f"  File: {data_source.get('file_path', 'N/A')}")
                self.console.print(f"  Text column: {data_source.get('text_column', 'N/A')}")

                if Confirm.ask("Change data file?", default=False):
                    new_file = self._prompt_file_path("New data file path")
                    modified['data_source']['file_path'] = new_file
                    modified['data_source']['file_name'] = Path(new_file).name

                if Confirm.ask("Change text column?", default=False):
                    new_col = Prompt.ask("New text column name")
                    modified['data_source']['text_column'] = new_col

            elif choice == "2":
                # Modify model
                self.console.print("\n[yellow]Current model:[/yellow]")
                model_config = modified.get('model_configuration', {})
                self.console.print(f"  Provider: {model_config.get('provider', 'N/A')}")
                self.console.print(f"  Model: {model_config.get('model_name', 'N/A')}")

                if Confirm.ask("Change model?", default=False):
                    # Reuse model selection from smart annotate
                    provider = Prompt.ask("Provider", choices=["ollama", "openai", "anthropic"], default="ollama")
                    model_name = Prompt.ask("Model name")
                    modified['model_configuration']['provider'] = provider
                    modified['model_configuration']['model_name'] = model_name

            elif choice == "3":
                # Modify model parameters
                model_config = modified.get('model_configuration', {})

                if Confirm.ask("Change temperature?", default=False):
                    temp = FloatPrompt.ask("Temperature (0.0-2.0)", default=0.7)
                    modified['model_configuration']['temperature'] = temp

                if Confirm.ask("Change max_tokens?", default=False):
                    tokens = self._int_prompt_with_validation("Max tokens", 1000, 50, 8000)
                    modified['model_configuration']['max_tokens'] = tokens

            elif choice == "4":
                # Modify prompts
                self.console.print("\n[yellow]Prompt modification not implemented in this version.[/yellow]")
                self.console.print("[dim]Use Smart Annotate to create new annotation with different prompts.[/dim]")

            elif choice == "5":
                # Modify sampling
                data_source = modified.get('data_source', {})
                current_rows = data_source.get('total_rows', 'all')

                self.console.print(f"\n[yellow]Current: {current_rows} rows[/yellow]")

                if Confirm.ask("Change number of rows to annotate?", default=False):
                    annotate_all = Confirm.ask("Annotate all rows?", default=True)
                    if annotate_all:
                        modified['data_source']['total_rows'] = 'all'
                        modified['data_source']['sampling_strategy'] = 'none'
                    else:
                        num_rows = self._int_prompt_with_validation("Number of rows", 100, 1, 1000000)
                        strategy = Prompt.ask("Sampling strategy", choices=["head", "random"], default="random")
                        modified['data_source']['total_rows'] = num_rows
                        modified['data_source']['sampling_strategy'] = strategy

            elif choice == "6":
                # Modify processing
                proc_config = modified.get('processing_configuration', {})

                if Confirm.ask("Change parallel workers?", default=False):
                    workers = self._int_prompt_with_validation("Parallel workers", 1, 1, 16)
                    modified['processing_configuration']['parallel_workers'] = workers

                if Confirm.ask("Change batch size?", default=False):
                    batch = self._int_prompt_with_validation("Batch size", 1, 1, 1000)
                    modified['processing_configuration']['batch_size'] = batch

        self.console.print("\n[green]✓ Parameters modified[/green]")
        return modified

    def _execute_from_metadata(self, metadata: dict, action_mode: str, metadata_file: Path):
        """Execute annotation based on loaded metadata"""
        import json
        from datetime import datetime

        # Extract all parameters from metadata
        data_source = metadata.get('data_source', {})
        model_config = metadata.get('model_configuration', {})
        prompts = metadata.get('prompts', [])
        proc_config = metadata.get('processing_configuration', {})
        output_config = metadata.get('output', {})
        export_prefs = metadata.get('export_preferences', {})

        # Get export preferences
        export_to_doccano = export_prefs.get('export_to_doccano', False)
        export_to_labelstudio = export_prefs.get('export_to_labelstudio', False)
        export_sample_size = export_prefs.get('export_sample_size', 'all')

        if export_to_doccano or export_to_labelstudio:
            export_tools = []
            if export_to_doccano:
                export_tools.append("Doccano")
            if export_to_labelstudio:
                export_tools.append("Label Studio")
            self.console.print(f"\n[cyan]ℹ️  Export enabled for: {', '.join(export_tools)} (from saved preferences)[/cyan]")
            if export_sample_size != 'all':
                self.console.print(f"[cyan]   Sample size: {export_sample_size}[/cyan]")

        # Prepare paths
        data_path = Path(data_source.get('file_path', ''))
        data_format = data_source.get('data_format', 'csv')

        # Check if resuming
        if action_mode == 'resume':
            # Try to find the output file
            original_output = Path(output_config.get('output_path', ''))

            if not original_output.exists():
                self.console.print(f"\n[yellow]⚠️  Output file not found: {original_output}[/yellow]")
                self.console.print("[yellow]Switching to relaunch mode (fresh annotation)[/yellow]")
                action_mode = 'relaunch'
            else:
                self.console.print(f"\n[green]✓ Found output file: {original_output.name}[/green]")

                # Count already annotated rows
                import pandas as pd
                try:
                    if data_format == 'csv':
                        df_output = pd.read_csv(original_output)
                    elif data_format in ['excel', 'xlsx']:
                        df_output = pd.read_excel(original_output)
                    elif data_format == 'parquet':
                        df_output = pd.read_parquet(original_output)

                    # Count rows with valid annotations (non-empty, non-null strings)
                    if 'annotation' in df_output.columns:
                        # Count only rows where annotation exists and is not empty/whitespace
                        annotated_mask = (
                            df_output['annotation'].notna() &
                            (df_output['annotation'].astype(str).str.strip() != '') &
                            (df_output['annotation'].astype(str) != 'nan')
                        )
                        annotated_count = annotated_mask.sum()
                    else:
                        annotated_count = 0

                    self.console.print(f"[cyan]  Rows already annotated: {annotated_count:,}[/cyan]")

                    # Get total available rows from source file
                    if data_path.exists():
                        if data_format == 'csv':
                            total_available = len(pd.read_csv(data_path))
                        elif data_format in ['excel', 'xlsx']:
                            total_available = len(pd.read_excel(data_path))
                        elif data_format == 'parquet':
                            total_available = len(pd.read_parquet(data_path))
                        else:
                            total_available = len(df_output)
                    else:
                        total_available = len(df_output)

                    # Calculate remaining based on original target
                    original_target = data_source.get('total_rows', 'all')

                    if original_target == 'all':
                        total_target = total_available
                    else:
                        total_target = original_target

                    remaining_from_target = total_target - annotated_count
                    remaining_from_source = total_available - annotated_count

                    self.console.print(f"[cyan]  Original target: {total_target:,} rows[/cyan]")
                    self.console.print(f"[cyan]  Remaining from target: {remaining_from_target:,}[/cyan]")
                    self.console.print(f"[cyan]  Total available in source: {total_available:,} rows[/cyan]")
                    self.console.print(f"[cyan]  Maximum you can annotate: {remaining_from_source:,}[/cyan]\n")

                    if remaining_from_source <= 0:
                        self.console.print("\n[yellow]All available rows are already annotated![/yellow]")
                        continue_anyway = Confirm.ask("Continue with relaunch mode?", default=False)
                        if not continue_anyway:
                            return
                        action_mode = 'relaunch'
                    else:
                        self.console.print("[yellow]You can annotate:[/yellow]")
                        self.console.print(f"  • Up to [cyan]{remaining_from_target:,}[/cyan] more rows to complete original target")
                        self.console.print(f"  • Or up to [cyan]{remaining_from_source:,}[/cyan] total to use all available data\n")

                        resume_count = self._int_prompt_with_validation(
                            f"How many more rows to annotate? (max: {remaining_from_source:,})",
                            min(100, remaining_from_target) if remaining_from_target > 0 else 100,
                            1,
                            remaining_from_source
                        )

                        # Update metadata for resume
                        metadata['data_source']['total_rows'] = resume_count
                        metadata['resume_mode'] = True
                        metadata['resume_from_file'] = str(original_output)
                        metadata['already_annotated'] = int(annotated_count)

                except Exception as e:
                    self.console.print(f"\n[red]Error reading output file: {e}[/red]")
                    self.console.print("[yellow]Switching to relaunch mode[/yellow]")
                    action_mode = 'relaunch'

        # Prepare output path
        annotations_dir = self.settings.paths.data_dir / 'annotations'
        annotations_dir.mkdir(parents=True, exist_ok=True)
        safe_model_name = model_config.get('model_name', 'unknown').replace(':', '_').replace('/', '_')
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        if action_mode == 'resume':
            output_filename = original_output.name  # Keep same filename
            default_output_path = original_output
        else:
            output_filename = f"{data_path.stem}_{safe_model_name}_annotations_{timestamp}.{data_format}"
            default_output_path = annotations_dir / output_filename

        self.console.print(f"\n[bold cyan]📁 Output Location:[/bold cyan]")
        self.console.print(f"   {default_output_path}")

        # Prepare prompts payload
        prompts_payload = []
        for p in prompts:
            prompts_payload.append({
                'prompt': p.get('prompt_content', p.get('prompt', '')),
                'expected_keys': p.get('expected_keys', []),
                'prefix': p.get('prefix', '')
            })

        # Get parameters
        provider = model_config.get('provider', 'ollama')
        model_name = model_config.get('model_name', 'llama2')
        annotation_mode = model_config.get('annotation_mode', 'local')
        temperature = model_config.get('temperature', 0.7)
        max_tokens = model_config.get('max_tokens', 1000)
        top_p = model_config.get('top_p', 1.0)
        top_k = model_config.get('top_k', 40)

        num_processes = proc_config.get('parallel_workers', 1)
        batch_size = proc_config.get('batch_size', 1)

        total_rows = data_source.get('total_rows')
        annotation_limit = None if total_rows == 'all' else total_rows
        sample_strategy = data_source.get('sampling_strategy', 'head')

        # IMPORTANT: In resume mode, always use 'head' strategy to continue sequentially
        # This ensures we pick up exactly where we left off, not random new rows
        if action_mode == 'resume':
            sample_strategy = 'head'
            self.console.print(f"\n[cyan]ℹ️  Resume mode: Using sequential (head) strategy to continue where you left off[/cyan]")

        # Get API key if needed
        api_key = None
        if provider in ['openai', 'anthropic', 'google']:
            api_key = self._get_api_key(provider)
            if not api_key:
                self.console.print(f"[red]API key required for {provider}[/red]")
                return

        # Build pipeline config
        pipeline_config = {
            'mode': 'file',
            'data_source': data_format,
            'data_format': data_format,
            'file_path': str(data_path),
            'text_column': data_source.get('text_column', 'text'),
            'text_columns': [data_source.get('text_column', 'text')],
            'annotation_column': 'annotation',
            'identifier_column': 'annotation_id',
            'run_annotation': True,
            'annotation_mode': annotation_mode,
            'annotation_provider': provider,
            'annotation_model': model_name,
            'api_key': api_key,
            'prompts': prompts_payload,
            'annotation_sample_size': annotation_limit,
            'annotation_sampling_strategy': sample_strategy if annotation_limit else 'head',
            'annotation_sample_seed': 42,
            'max_tokens': max_tokens,
            'temperature': temperature,
            'top_p': top_p,
            'top_k': top_k if provider in ['ollama', 'google'] else None,
            'max_workers': num_processes,
            'num_processes': num_processes,
            'use_parallel': num_processes > 1,
            'warmup': False,
            'disable_tqdm': True,
            'output_format': data_format,
            'output_path': str(default_output_path),
            'save_incrementally': True,
            'batch_size': batch_size,
            'run_validation': False,
            'run_training': False,
        }

        # Add resume information if resuming
        if action_mode == 'resume' and metadata.get('resume_mode'):
            pipeline_config['resume_mode'] = True
            pipeline_config['resume_from_file'] = metadata.get('resume_from_file')
            pipeline_config['skip_annotated'] = True

            # Load already annotated IDs to skip them
            try:
                import pandas as pd
                resume_file = Path(metadata.get('resume_from_file'))
                if resume_file.exists():
                    if data_format == 'csv':
                        df_resume = pd.read_csv(resume_file)
                    elif data_format in ['excel', 'xlsx']:
                        df_resume = pd.read_excel(resume_file)
                    elif data_format == 'parquet':
                        df_resume = pd.read_parquet(resume_file)

                    # Get IDs of rows that have valid annotations
                    if 'annotation' in df_resume.columns and 'annotation_id' in df_resume.columns:
                        annotated_mask = (
                            df_resume['annotation'].notna() &
                            (df_resume['annotation'].astype(str).str.strip() != '') &
                            (df_resume['annotation'].astype(str) != 'nan')
                        )
                        already_annotated_ids = df_resume.loc[annotated_mask, 'annotation_id'].tolist()
                        pipeline_config['skip_annotation_ids'] = already_annotated_ids

                        self.console.print(f"[cyan]  Will skip {len(already_annotated_ids)} already annotated row(s)[/cyan]")
            except Exception as e:
                self.logger.warning(f"Could not load annotated IDs from resume file: {e}")
                self.console.print(f"[yellow]⚠️  Warning: Could not load annotated IDs - may re-annotate some rows[/yellow]")

        # Add model-specific options
        if provider == 'ollama':
            options = {
                'temperature': temperature,
                'num_predict': max_tokens,
                'top_p': top_p,
                'top_k': top_k
            }
            pipeline_config['options'] = options

        # Save new metadata for this execution
        if action_mode == 'relaunch':
            new_metadata = metadata.copy()
            new_metadata['annotation_session']['timestamp'] = timestamp
            new_metadata['annotation_session']['relaunch_from'] = str(metadata_file.name)
            new_metadata['output']['output_path'] = str(default_output_path)

            new_metadata_filename = f"{data_path.stem}_{safe_model_name}_metadata_{timestamp}.json"
            new_metadata_path = session_dirs['metadata'] / new_metadata_filename

            with open(new_metadata_path, 'w', encoding='utf-8') as f:
                json.dump(new_metadata, f, indent=2, ensure_ascii=False)

            self.console.print(f"\n[green]✅ New session metadata saved[/green]")
            self.console.print(f"[cyan]📋 Metadata File:[/cyan]")
            self.console.print(f"   {new_metadata_path}\n")

        # Execute pipeline
        try:
            self.console.print("\n[bold green]🚀 Starting annotation...[/bold green]\n")

            from ..pipelines.pipeline_controller import PipelineController
            pipeline_with_progress = PipelineController(settings=self.settings)

            from ..utils.rich_progress_manager import RichProgressManager
            from ..pipelines.enhanced_pipeline_wrapper import EnhancedPipelineWrapper

            with RichProgressManager(
                show_json_every=1,
                compact_mode=False
            ) as progress_manager:
                enhanced_pipeline = EnhancedPipelineWrapper(
                    pipeline_with_progress,
                    progress_manager
                )

                state = enhanced_pipeline.run_pipeline(pipeline_config)

                if state.errors:
                    error_msg = state.errors[0]['error'] if state.errors else "Annotation failed"
                    self.console.print(f"\n[bold red]❌ Error:[/bold red] {error_msg}")
                    return

            # Display results
            annotation_results = state.annotation_results or {}
            output_file = annotation_results.get('output_file', str(default_output_path))

            self.console.print("\n[bold green]✅ Annotation completed successfully![/bold green]")
            self.console.print(f"\n[bold cyan]📄 Output File:[/bold cyan]")
            self.console.print(f"   {output_file}")

            total_annotated = annotation_results.get('total_annotated', 0)
            if total_annotated:
                self.console.print(f"\n[bold cyan]📊 Statistics:[/bold cyan]")
                self.console.print(f"   Rows annotated: {total_annotated:,}")

                success_count = annotation_results.get('success_count', 0)
                if success_count:
                    success_rate = (success_count / total_annotated * 100)
                    self.console.print(f"   Success rate: {success_rate:.1f}%")

            # ============================================================
            # INTELLIGENT TRAINING WORKFLOW (Post-Annotation)
            # ============================================================
            # Build prompt_configs for training workflow
            prompt_configs_for_training = []
            for p in prompts:
                prompt_configs_for_training.append({
                    'prompt': {
                        'keys': p.get('expected_keys', []),
                        'content': p.get('prompt_content', p.get('prompt', '')),
                        'name': p.get('name', 'prompt')
                    },
                    'prefix': p.get('prefix', '')
                })

            self._post_annotation_training_workflow(
                output_file=output_file,
                text_column=data_source.get('text_column', 'text'),
                prompt_configs=prompt_configs_for_training
            )

            # Export to Doccano JSONL if enabled in preferences
            if export_to_doccano:
                # Build prompt_configs for export
                prompt_configs_for_export = []
                for p in prompts:
                    prompt_configs_for_export.append({
                        'prompt': {
                            'keys': p.get('expected_keys', []),
                            'content': p.get('prompt_content', p.get('prompt', '')),
                            'name': p.get('name', 'prompt')
                        },
                        'prefix': p.get('prefix', '')
                    })

                self._export_to_doccano_jsonl(
                    output_file=output_file,
                    text_column=data_source.get('text_column', 'text'),
                    prompt_configs=prompt_configs_for_export,
                    data_path=data_path,
                    timestamp=datetime.now().strftime('%Y%m%d_%H%M%S'),
                    sample_size=export_sample_size
                )

            # Export to Label Studio JSONL if enabled in preferences
            if export_to_labelstudio:
                # Build prompt_configs for export
                prompt_configs_for_export = []
                for p in prompts:
                    prompt_configs_for_export.append({
                        'prompt': {
                            'keys': p.get('expected_keys', []),
                            'content': p.get('prompt_content', p.get('prompt', '')),
                            'name': p.get('name', 'prompt')
                        },
                        'prefix': p.get('prefix', '')
                    })

                self._export_to_labelstudio_jsonl(
                    output_file=output_file,
                    text_column=data_source.get('text_column', 'text'),
                    prompt_configs=prompt_configs_for_export,
                    data_path=data_path,
                    timestamp=datetime.now().strftime('%Y%m%d_%H%M%S'),
                    sample_size=export_sample_size
                )

        except Exception as exc:
            self.console.print(f"\n[bold red]❌ Annotation failed:[/bold red] {exc}")
            self.logger.exception("Resume/Relaunch annotation failed")

    def _clean_metadata(self):
        """Clean old metadata files"""
        self.console.print("\n[bold cyan]🗑️  Clean Old Metadata[/bold cyan]\n")
        self.console.print("[dim]Delete saved annotation parameters to free space[/dim]\n")

        annotations_dir = self.settings.paths.data_dir / 'annotations'

        if not annotations_dir.exists():
            self.console.print("[yellow]No annotations directory found.[/yellow]")
            self.console.print("\n[dim]Press Enter to continue...[/dim]")
            input()
            return

        # Find all metadata JSON files
        metadata_files = list(annotations_dir.glob("*_metadata_*.json"))

        if not metadata_files:
            self.console.print("[yellow]No metadata files found.[/yellow]")
            self.console.print("\n[dim]Press Enter to continue...[/dim]")
            input()
            return

        # Sort by modification time (oldest first for cleaning)
        metadata_files.sort(key=lambda x: x.stat().st_mtime)

        self.console.print(f"[green]Found {len(metadata_files)} metadata file(s)[/green]\n")

        # Display cleaning options
        self.console.print("[bold]Cleaning Options:[/bold]")
        self.console.print("  [cyan]1[/cyan] - Delete ALL metadata files")
        self.console.print("  [cyan]2[/cyan] - Delete metadata older than X days")
        self.console.print("  [cyan]3[/cyan] - Select specific files to delete")
        self.console.print("  [cyan]0[/cyan] - Cancel")

        clean_choice = Prompt.ask(
            "\n[bold yellow]Select cleaning option[/bold yellow]",
            choices=["0", "1", "2", "3"],
            default="0"
        )

        if clean_choice == "0":
            self.console.print("[yellow]Cleaning cancelled[/yellow]")
            self.console.print("\n[dim]Press Enter to continue...[/dim]")
            input()
            return

        files_to_delete = []

        if clean_choice == "1":
            # Delete ALL
            self.console.print(f"\n[red]⚠️  Warning: This will delete ALL {len(metadata_files)} metadata files![/red]")
            confirm = Confirm.ask("Are you sure?", default=False)

            if confirm:
                files_to_delete = metadata_files
            else:
                self.console.print("[yellow]Deletion cancelled[/yellow]")
                self.console.print("\n[dim]Press Enter to continue...[/dim]")
                input()
                return

        elif clean_choice == "2":
            # Delete older than X days
            days = self._int_prompt_with_validation(
                "Delete files older than how many days?",
                30, 1, 365
            )

            from datetime import datetime, timedelta
            cutoff_time = datetime.now() - timedelta(days=days)

            for mf in metadata_files:
                file_time = datetime.fromtimestamp(mf.stat().st_mtime)
                if file_time < cutoff_time:
                    files_to_delete.append(mf)

            if not files_to_delete:
                self.console.print(f"\n[yellow]No files older than {days} days found[/yellow]")
                self.console.print("\n[dim]Press Enter to continue...[/dim]")
                input()
                return

            self.console.print(f"\n[yellow]Found {len(files_to_delete)} file(s) older than {days} days[/yellow]")
            confirm = Confirm.ask("Delete these files?", default=False)

            if not confirm:
                self.console.print("[yellow]Deletion cancelled[/yellow]")
                self.console.print("\n[dim]Press Enter to continue...[/dim]")
                input()
                return

        elif clean_choice == "3":
            # Select specific files
            import json
            from datetime import datetime

            # Display files with details
            files_table = Table(border_style="cyan", show_header=True)
            files_table.add_column("#", style="cyan", width=4)
            files_table.add_column("Filename", style="white", width=50)
            files_table.add_column("Date", style="yellow", width=16)
            files_table.add_column("Size", style="green", width=10)

            valid_files = []
            for i, mf in enumerate(metadata_files, 1):
                try:
                    size_kb = mf.stat().st_size / 1024
                    mtime = datetime.fromtimestamp(mf.stat().st_mtime)
                    date_str = mtime.strftime('%Y-%m-%d %H:%M')

                    files_table.add_row(
                        str(i),
                        mf.name[:50],
                        date_str,
                        f"{size_kb:.1f} KB"
                    )
                    valid_files.append(mf)
                except Exception as e:
                    continue

            self.console.print("\n")
            self.console.print(files_table)

            self.console.print("\n[yellow]Select files to delete:[/yellow]")
            self.console.print("[dim]Enter comma-separated numbers (e.g., 1,3,5) or 'all' for all files[/dim]")

            selection = Prompt.ask("Files to delete")

            if selection.lower() == 'all':
                files_to_delete = valid_files
            else:
                try:
                    indices = [int(x.strip()) for x in selection.split(',')]
                    for idx in indices:
                        if 1 <= idx <= len(valid_files):
                            files_to_delete.append(valid_files[idx - 1])
                except ValueError:
                    self.console.print("[red]Invalid selection[/red]")
                    self.console.print("\n[dim]Press Enter to continue...[/dim]")
                    input()
                    return

            if not files_to_delete:
                self.console.print("[yellow]No files selected[/yellow]")
                self.console.print("\n[dim]Press Enter to continue...[/dim]")
                input()
                return

            self.console.print(f"\n[yellow]Selected {len(files_to_delete)} file(s) for deletion[/yellow]")
            confirm = Confirm.ask("Delete these files?", default=False)

            if not confirm:
                self.console.print("[yellow]Deletion cancelled[/yellow]")
                self.console.print("\n[dim]Press Enter to continue...[/dim]")
                input()
                return

        # Perform deletion
        deleted_count = 0
        failed_count = 0

        self.console.print("\n[bold]Deleting files...[/bold]")

        for mf in files_to_delete:
            try:
                mf.unlink()
                deleted_count += 1
                self.console.print(f"  [green]✓[/green] Deleted: {mf.name}")
            except Exception as e:
                failed_count += 1
                self.console.print(f"  [red]✗[/red] Failed: {mf.name} - {e}")

        # Summary
        self.console.print(f"\n[bold green]✅ Deletion complete[/bold green]")
        self.console.print(f"   Deleted: {deleted_count} file(s)")
        if failed_count > 0:
            self.console.print(f"   [red]Failed: {failed_count} file(s)[/red]")

        self.console.print("\n[dim]Press Enter to continue...[/dim]")
        input()

    def _export_to_doccano_jsonl(self, output_file: str, text_column: str,
                                  prompt_configs: list, data_path: Path, timestamp: str,
                                  sample_size=None, session_dirs=None):
        """Export annotations to Doccano JSONL format

        Parameters
        ----------
        sample_size : str or int, optional
            Number of samples to export. Can be:
            - 'all': export all annotations
            - 'representative': export 10% (minimum 100)
            - int: export specific number
        """
        import json
        import pandas as pd

        try:
            self.console.print("\n[bold cyan]📤 Exporting to Doccano JSONL...[/bold cyan]")

            # Load the annotated file
            output_path = Path(output_file)
            if output_path.suffix.lower() == '.csv':
                df = pd.read_csv(output_path)
            elif output_path.suffix.lower() in ['.xlsx', '.xls']:
                df = pd.read_excel(output_path)
            elif output_path.suffix.lower() == '.parquet':
                df = pd.read_parquet(output_path)
            else:
                self.console.print(f"[yellow]⚠️  Unsupported format for Doccano export: {output_path.suffix}[/yellow]")
                return

            # Filter only annotated rows
            if 'annotation' not in df.columns:
                self.console.print("[yellow]⚠️  No annotation column found[/yellow]")
                return

            annotated_mask = (
                df['annotation'].notna() &
                (df['annotation'].astype(str).str.strip() != '') &
                (df['annotation'].astype(str) != 'nan')
            )
            df_annotated = df[annotated_mask].copy()

            if len(df_annotated) == 0:
                self.console.print("[yellow]⚠️  No valid annotations to export[/yellow]")
                return

            total_annotated = len(df_annotated)
            self.console.print(f"[cyan]  Found {total_annotated:,} annotated rows[/cyan]")

            # Apply sampling if specified
            if sample_size is not None and sample_size != 'all':
                if sample_size == 'representative':
                    # Stratified sampling: 10% from each label class (minimum 100 total)
                    n_samples = max(100, int(total_annotated * 0.1))

                    # Don't sample more than available
                    n_samples = min(n_samples, total_annotated)

                    if n_samples < total_annotated:
                        self.console.print(f"[cyan]  Using stratified sampling: {n_samples:,} rows (proportional by labels)[/cyan]")

                        # Parse annotations to get label distribution
                        label_counts = {}
                        for idx, row in df_annotated.iterrows():
                            try:
                                annotation = json.loads(row['annotation'])
                                # Get first label key as stratification key
                                for key in annotation.keys():
                                    if key != 'text':
                                        label_val = str(annotation[key])
                                        label_counts[label_val] = label_counts.get(label_val, 0) + 1
                                        break
                            except:
                                pass

                        # Stratified sampling
                        df_annotated = df_annotated.sample(n=n_samples, random_state=42).copy()
                    else:
                        self.console.print(f"[cyan]  Exporting all {total_annotated:,} rows (sample size >= total)[/cyan]")
                else:
                    # Custom number - random sampling
                    n_samples = int(sample_size)
                    n_samples = min(n_samples, total_annotated)

                    if n_samples < total_annotated:
                        self.console.print(f"[cyan]  Random sampling: {n_samples:,} rows for export[/cyan]")
                        df_annotated = df_annotated.sample(n=n_samples, random_state=42).copy()
                    else:
                        self.console.print(f"[cyan]  Exporting all {total_annotated:,} rows[/cyan]")
            else:
                self.console.print(f"[cyan]  Exporting all {total_annotated:,} rows[/cyan]")

            # Prepare JSONL output - Use organized structure if session_dirs provided
            if session_dirs:
                # Create dataset-specific subdirectory for exports
                dataset_name = data_path.stem
                doccano_dir = session_dirs['doccano'] / dataset_name
                doccano_dir.mkdir(parents=True, exist_ok=True)
            else:
                # Fallback to old structure for backward compatibility
                doccano_dir = self.settings.paths.data_dir / 'doccano_exports'
                doccano_dir.mkdir(parents=True, exist_ok=True)

            jsonl_filename = f"{data_path.stem}_doccano_{timestamp}.jsonl"
            jsonl_path = doccano_dir / jsonl_filename

            # Get all label keys from prompts
            all_label_keys = set()
            for pc in prompt_configs:
                prefix = pc.get('prefix', '')
                for key in pc['prompt']['keys']:
                    if prefix:
                        all_label_keys.add(f"{prefix}_{key}")
                    else:
                        all_label_keys.add(key)

            # Extract labels from annotations (JSON strings)
            exported_count = 0
            with open(jsonl_path, 'w', encoding='utf-8') as f:
                for idx, row in df_annotated.iterrows():
                    try:
                        # Parse annotation JSON
                        annotation_str = row['annotation']
                        if pd.isna(annotation_str) or str(annotation_str).strip() == '':
                            continue

                        annotation_data = json.loads(annotation_str)

                        # Build Doccano entry
                        doccano_entry = {
                            'text': str(row[text_column]),
                            'labels': []
                        }

                        # Extract labels from annotation
                        for label_key in all_label_keys:
                            if label_key in annotation_data:
                                label_value = annotation_data[label_key]
                                # Handle different label formats
                                if isinstance(label_value, list):
                                    doccano_entry['labels'].extend(label_value)
                                elif isinstance(label_value, str) and label_value.strip():
                                    doccano_entry['labels'].append(label_value)

                        # Add metadata (everything except text and annotation columns)
                        metadata = {}
                        for col in df.columns:
                            if col not in [text_column, 'annotation'] and col not in all_label_keys:
                                val = row[col]
                                # Convert to JSON-serializable format
                                if pd.notna(val):
                                    if isinstance(val, (pd.Timestamp, pd.DatetimeTZDtype)):
                                        metadata[col] = str(val)
                                    elif isinstance(val, (int, float, str, bool)):
                                        metadata[col] = val
                                    else:
                                        metadata[col] = str(val)

                        doccano_entry['meta'] = metadata

                        # Write to JSONL
                        f.write(json.dumps(doccano_entry, ensure_ascii=False) + '\n')
                        exported_count += 1

                    except json.JSONDecodeError as e:
                        self.logger.warning(f"Could not parse annotation at row {idx}: {e}")
                        continue
                    except Exception as e:
                        self.logger.warning(f"Error processing row {idx}: {e}")
                        continue

            # Display success
            self.console.print(f"\n[bold green]✅ Doccano JSONL export completed![/bold green]")
            self.console.print(f"[bold cyan]📄 JSONL File:[/bold cyan]")
            self.console.print(f"   {jsonl_path}")
            self.console.print(f"[cyan]   Exported: {exported_count:,} entries[/cyan]\n")

            self.console.print("[yellow]📌 Next Steps:[/yellow]")
            self.console.print("  1. Import this JSONL file into Doccano for validation")
            self.console.print("  2. Review and correct annotations in Doccano")
            self.console.print("  3. Export validated annotations from Doccano")
            self.console.print("  4. Use LLM Tool to calculate metrics on validated data\n")

        except Exception as e:
            self.console.print(f"\n[red]❌ Doccano export failed: {e}[/red]")
            self.logger.exception("Doccano JSONL export failed")

    def _export_to_labelstudio_jsonl(self, output_file: str, text_column: str,
                                      prompt_configs: list, data_path: Path, timestamp: str,
                                      sample_size=None, prediction_mode='with', session_dirs=None):
        """Export annotations to Label Studio JSONL format

        Parameters
        ----------
        sample_size : str or int, optional
            Number of samples to export. Can be:
            - 'all': export all annotations
            - 'representative': export 10% (minimum 100)
            - int: export specific number
        prediction_mode : str, optional
            How to include LLM predictions:
            - 'with': Include predictions (default)
            - 'without': Export without predictions
            - 'both': Create two files (with and without)
        """
        import json
        import pandas as pd

        try:
            self.console.print("\n[bold cyan]📤 Exporting to Label Studio JSONL...[/bold cyan]")

            # Load the annotated file
            output_path = Path(output_file)
            if output_path.suffix.lower() == '.csv':
                df = pd.read_csv(output_path)
            elif output_path.suffix.lower() in ['.xlsx', '.xls']:
                df = pd.read_excel(output_path)
            elif output_path.suffix.lower() == '.parquet':
                df = pd.read_parquet(output_path)
            else:
                self.console.print(f"[yellow]⚠️  Unsupported format for Label Studio export: {output_path.suffix}[/yellow]")
                return

            # Filter only annotated rows
            if 'annotation' not in df.columns:
                self.console.print("[yellow]⚠️  No annotation column found[/yellow]")
                return

            annotated_mask = (
                df['annotation'].notna() &
                (df['annotation'].astype(str).str.strip() != '') &
                (df['annotation'].astype(str) != 'nan')
            )
            df_annotated = df[annotated_mask].copy()

            if len(df_annotated) == 0:
                self.console.print("[yellow]⚠️  No valid annotations to export[/yellow]")
                return

            total_annotated = len(df_annotated)
            self.console.print(f"[cyan]  Found {total_annotated:,} annotated rows[/cyan]")

            # Apply sampling if specified
            if sample_size is not None and sample_size != 'all':
                if sample_size == 'representative':
                    # Stratified sampling: 10% from each label class (minimum 100 total)
                    n_samples = max(100, int(total_annotated * 0.1))

                    # Don't sample more than available
                    n_samples = min(n_samples, total_annotated)

                    if n_samples < total_annotated:
                        self.console.print(f"[cyan]  Using stratified sampling: {n_samples:,} rows (proportional by labels)[/cyan]")

                        # Parse annotations to get label distribution
                        label_counts = {}
                        for idx, row in df_annotated.iterrows():
                            try:
                                annotation = json.loads(row['annotation'])
                                # Get first label key as stratification key
                                for key in annotation.keys():
                                    if key != 'text':
                                        label_val = str(annotation[key])
                                        label_counts[label_val] = label_counts.get(label_val, 0) + 1
                                        break
                            except:
                                pass

                        # Stratified sampling
                        df_annotated = df_annotated.sample(n=n_samples, random_state=42).copy()
                    else:
                        self.console.print(f"[cyan]  Exporting all {total_annotated:,} rows (sample size >= total)[/cyan]")
                else:
                    # Custom number - random sampling
                    n_samples = int(sample_size)
                    n_samples = min(n_samples, total_annotated)

                    if n_samples < total_annotated:
                        self.console.print(f"[cyan]  Random sampling: {n_samples:,} rows for export[/cyan]")
                        df_annotated = df_annotated.sample(n=n_samples, random_state=42).copy()
                    else:
                        self.console.print(f"[cyan]  Exporting all {total_annotated:,} rows[/cyan]")
            else:
                self.console.print(f"[cyan]  Exporting all {total_annotated:,} rows[/cyan]")

            # Prepare JSONL output - Use organized structure if session_dirs provided
            if session_dirs:
                # Create dataset-specific subdirectory for exports
                dataset_name = data_path.stem
                labelstudio_dir = session_dirs['labelstudio'] / dataset_name
                labelstudio_dir.mkdir(parents=True, exist_ok=True)
            else:
                # Fallback to old structure for backward compatibility
                labelstudio_dir = self.settings.paths.data_dir / 'labelstudio_exports'
                labelstudio_dir.mkdir(parents=True, exist_ok=True)

            jsonl_filename = f"{data_path.stem}_labelstudio_{timestamp}.jsonl"
            jsonl_path = labelstudio_dir / jsonl_filename

            # Get all label keys from prompts
            all_label_keys = set()
            for pc in prompt_configs:
                prefix = pc.get('prefix', '')
                for key in pc['prompt']['keys']:
                    if prefix:
                        all_label_keys.add(f"{prefix}_{key}")
                    else:
                        all_label_keys.add(key)

            # Export to Label Studio format
            exported_count = 0
            with open(jsonl_path, 'w', encoding='utf-8') as f:
                for idx, row in df_annotated.iterrows():
                    try:
                        # Parse annotation JSON
                        annotation_str = row['annotation']
                        if pd.isna(annotation_str) or str(annotation_str).strip() == '':
                            continue

                        annotation_data = json.loads(annotation_str)

                        # Build Label Studio entry
                        # Data section
                        data_entry = {
                            'text': str(row[text_column])
                        }

                        # Add metadata
                        for col in df.columns:
                            if col not in [text_column, 'annotation'] and col not in all_label_keys:
                                val = row[col]
                                if pd.notna(val):
                                    if isinstance(val, (pd.Timestamp, pd.DatetimeTZDtype)):
                                        data_entry[col] = str(val)
                                    elif isinstance(val, (int, float, str, bool)):
                                        data_entry[col] = val
                                    else:
                                        data_entry[col] = str(val)

                        # Predictions section (LLM annotations as predictions)
                        predictions_result = []

                        for label_key in all_label_keys:
                            if label_key in annotation_data:
                                label_value = annotation_data[label_key]

                                # Handle list of labels
                                if isinstance(label_value, list):
                                    for lv in label_value:
                                        if lv and str(lv).strip():
                                            predictions_result.append({
                                                "value": {
                                                    "choices": [str(lv)]
                                                },
                                                "from_name": label_key,
                                                "to_name": "text",
                                                "type": "choices"
                                            })
                                # Handle single label
                                elif isinstance(label_value, str) and label_value.strip():
                                    predictions_result.append({
                                        "value": {
                                            "choices": [label_value]
                                        },
                                        "from_name": label_key,
                                        "to_name": "text",
                                        "type": "choices"
                                    })

                        # Build entry based on prediction mode
                        if prediction_mode == 'without':
                            # Export without predictions - just data
                            labelstudio_entry = {"data": data_entry}
                        else:
                            # Export with predictions (default)
                            labelstudio_entry = {
                                "data": data_entry,
                                "predictions": [{
                                    "result": predictions_result,
                                    "model_version": "llm_annotation"
                                }]
                            }

                        # Write to JSONL
                        f.write(json.dumps(labelstudio_entry, ensure_ascii=False) + '\n')
                        exported_count += 1

                    except json.JSONDecodeError as e:
                        self.logger.warning(f"Could not parse annotation at row {idx}: {e}")
                        continue
                    except Exception as e:
                        self.logger.warning(f"Error processing row {idx}: {e}")
                        continue

            # Create Label Studio config XML file
            config_path = jsonl_path.with_suffix('.xml')
            label_config = self._build_labelstudio_config(all_label_keys, prompt_configs)

            with open(config_path, 'w', encoding='utf-8') as f:
                f.write(label_config)

            # Also create a JSON file (non-line-delimited) for easier import
            suffix = "_with_predictions" if prediction_mode == 'with' else "_without_predictions"
            json_path = jsonl_path.parent / f"{jsonl_path.stem}{suffix}.json"
            jsonl_final = jsonl_path.parent / f"{jsonl_path.stem}{suffix}.jsonl"

            # Rename jsonl if needed
            if prediction_mode != 'with':
                jsonl_path.rename(jsonl_final)
                jsonl_path = jsonl_final

            # Read the JSONL and convert to JSON array
            tasks_array = []
            with open(jsonl_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        tasks_array.append(json.loads(line))

            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(tasks_array, f, indent=2, ensure_ascii=False)

            # If mode is 'both', create second set without predictions
            if prediction_mode == 'both':
                self.console.print(f"[cyan]  Creating second file without predictions...[/cyan]")

                # Call recursively with 'without' mode
                self._export_to_labelstudio_jsonl(
                    output_file=output_file,
                    text_column=text_column,
                    prompt_configs=prompt_configs,
                    data_path=data_path,
                    timestamp=timestamp,
                    sample_size='all',  # Use all already sampled data
                    prediction_mode='without'
                )

                self.console.print(f"\n[bold green]✅ Label Studio export completed (both modes)![/bold green]")
            else:
                self.console.print(f"\n[bold green]✅ Label Studio export completed![/bold green]")

            # Display files created
            mode_desc = {
                'with': 'with LLM predictions',
                'without': 'without predictions (for manual annotation)',
                'both': 'with predictions'
            }.get(prediction_mode, '')

            self.console.print(f"[bold cyan]📄 Files created ({mode_desc}):[/bold cyan]")
            self.console.print(f"   {json_path} [dim](JSON array - use this for import)[/dim]")
            self.console.print(f"   {jsonl_path} [dim](JSONL - alternative format)[/dim]")
            self.console.print(f"   {config_path} [dim](labeling config XML)[/dim]")
            self.console.print(f"[cyan]   Exported: {exported_count:,} entries[/cyan]\n")

            self.console.print("[yellow]📌 Import Instructions:[/yellow]")
            self.console.print("  [bold]Recommended: Use the JSON file[/bold]")
            self.console.print("  1. In Label Studio, click 'Create Project'")
            self.console.print("  2. Name your project and click 'Save'")
            self.console.print("  3. Go to 'Settings' → 'Labeling Interface'")
            self.console.print(f"  4. Click 'Code' and paste contents from: {config_path.name}")
            self.console.print("  5. Save the configuration")
            self.console.print(f"  6. Go to project, click 'Import' and upload: {json_path.name}\n")

            self.console.print("  [dim]Alternative: Use direct API export for automatic setup[/dim]\n")

        except Exception as e:
            self.console.print(f"\n[red]❌ Label Studio export failed: {e}[/red]")
            self.logger.exception("Label Studio JSONL export failed")

    def _export_to_labelstudio_direct(self, output_file: str, text_column: str,
                                        prompt_configs: list, data_path: Path, timestamp: str,
                                        sample_size=None, prediction_mode='with', api_url=None, api_key=None):
        """Export annotations directly to Label Studio via API

        Parameters
        ----------
        api_url : str
            Label Studio API URL (e.g., http://localhost:8080)
        api_key : str
            Label Studio API key from Account & Settings
        prediction_mode : str, optional
            How to include LLM predictions:
            - 'with': Include predictions (default)
            - 'without': Export without predictions
            - 'both': Create two projects (with and without)
        """
        import json
        import pandas as pd

        # Check if requests is available
        if not HAS_REQUESTS:
            self.console.print("\n[yellow]⚠️  Direct export to Label Studio requires the 'requests' library[/yellow]")
            self.console.print("[cyan]This library is not currently installed.[/cyan]\n")

            install_requests = Confirm.ask(
                "Would you like to install 'requests' now?",
                default=True
            )

            if install_requests:
                try:
                    self.console.print("\n[cyan]Installing requests...[/cyan]")
                    import subprocess
                    result = subprocess.run(
                        [sys.executable, "-m", "pip", "install", "requests"],
                        capture_output=True,
                        text=True
                    )

                    if result.returncode == 0:
                        self.console.print("[green]✅ Successfully installed 'requests'[/green]")
                        # Import it now
                        import requests as req_module
                        # Note: requests is now available for the rest of this function
                        globals()['requests'] = req_module
                        globals()['HAS_REQUESTS'] = True
                    else:
                        self.console.print(f"[red]❌ Installation failed: {result.stderr}[/red]")
                        self.console.print("\n[yellow]Please install manually:[/yellow]")
                        self.console.print("  pip install requests")
                        return

                except Exception as e:
                    self.console.print(f"[red]❌ Installation error: {e}[/red]")
                    self.console.print("\n[yellow]Please install manually:[/yellow]")
                    self.console.print("  pip install requests")
                    return
            else:
                self.console.print("\n[yellow]Skipping direct export. You can:[/yellow]")
                self.console.print("  1. Install requests: pip install requests")
                self.console.print("  2. Use JSONL export instead (no extra dependencies)")
                return

        # Import requests locally for use in this function
        import requests

        try:
            self.console.print("\n[bold cyan]📤 Exporting directly to Label Studio...[/bold cyan]")

            # Load the annotated file
            output_path = Path(output_file)
            if output_path.suffix.lower() == '.csv':
                df = pd.read_csv(output_path)
            elif output_path.suffix.lower() in ['.xlsx', '.xls']:
                df = pd.read_excel(output_path)
            elif output_path.suffix.lower() == '.parquet':
                df = pd.read_parquet(output_path)
            else:
                self.console.print(f"[yellow]⚠️  Unsupported format: {output_path.suffix}[/yellow]")
                return

            # Filter only annotated rows
            if 'annotation' not in df.columns:
                self.console.print("[yellow]⚠️  No annotation column found[/yellow]")
                return

            annotated_mask = (
                df['annotation'].notna() &
                (df['annotation'].astype(str).str.strip() != '') &
                (df['annotation'].astype(str) != 'nan')
            )
            df_annotated = df[annotated_mask].copy()

            if len(df_annotated) == 0:
                self.console.print("[yellow]⚠️  No valid annotations to export[/yellow]")
                return

            total_annotated = len(df_annotated)
            self.console.print(f"[cyan]  Found {total_annotated:,} annotated rows[/cyan]")

            # Apply sampling (same logic as JSONL export)
            if sample_size is not None and sample_size != 'all':
                if sample_size == 'representative':
                    n_samples = max(100, int(total_annotated * 0.1))
                    n_samples = min(n_samples, total_annotated)
                    if n_samples < total_annotated:
                        self.console.print(f"[cyan]  Using stratified sampling: {n_samples:,} rows[/cyan]")
                        df_annotated = df_annotated.sample(n=n_samples, random_state=42).copy()
                else:
                    n_samples = int(sample_size)
                    n_samples = min(n_samples, total_annotated)
                    if n_samples < total_annotated:
                        self.console.print(f"[cyan]  Random sampling: {n_samples:,} rows[/cyan]")
                        df_annotated = df_annotated.sample(n=n_samples, random_state=42).copy()

            # Get all label keys from prompts
            all_label_keys = set()
            for pc in prompt_configs:
                if 'keys' in pc['prompt']:
                    for key in pc['prompt']['keys']:
                        all_label_keys.add(key)

            # Create Label Studio project
            mode_suffix = "_with_predictions" if prediction_mode == 'with' else "_no_predictions"
            self.console.print(f"\n[cyan]  Creating Label Studio project{mode_suffix}...[/cyan]")

            # Title must be max 50 chars for Label Studio
            # Format: LLM_{short_name}_{mode} where mode is 'pred' or 'nopred'
            mode_short = "pred" if prediction_mode == 'with' else "nopred"
            base_name = data_path.stem[:30]  # Truncate base name if needed
            project_title = f"LLM_{base_name}_{mode_short}"[:50]

            # Build labeling config
            label_config = self._build_labelstudio_config(all_label_keys, prompt_configs)

            # Create project via API
            # Label Studio Personal Access Tokens (JWT refresh tokens) must be exchanged for access tokens
            # Try to get an access token first if using JWT format
            access_token = api_key

            if api_key.startswith('eyJ'):
                # This is a JWT refresh token - exchange for access token
                try:
                    token_response = requests.post(
                        f'{api_url}/api/token/refresh',
                        headers={'Content-Type': 'application/json'},
                        json={'refresh': api_key}
                    )
                    if token_response.status_code == 200:
                        token_data = token_response.json()
                        access_token = token_data.get('access', api_key)
                        self.console.print(f"[dim cyan]  ✓ Obtained access token from refresh token[/dim cyan]")
                    else:
                        self.console.print(f"[dim yellow]  Note: Token refresh returned {token_response.status_code}, trying direct use[/dim yellow]")
                except Exception as e:
                    # If exchange fails, try using the token directly
                    self.console.print(f"[dim yellow]  Note: Could not exchange refresh token ({e}), trying direct use[/dim yellow]")
                    pass

            # Try different auth formats
            # For JWT access tokens, Bearer is the correct format
            auth_formats = [
                f'Bearer {access_token}',     # JWT format (1.20+) - try first
                f'Token {access_token}',      # Legacy format (pre-1.20)
                access_token                   # Raw token (fallback)
            ]

            project_data = {
                'title': project_title,
                'label_config': label_config,
                'description': f'LLM annotations exported from {data_path.name}'
            }

            response = None
            for i, auth_format in enumerate(auth_formats):
                headers = {
                    'Authorization': auth_format,
                    'Content-Type': 'application/json'
                }

                response = requests.post(
                    f'{api_url}/api/projects',
                    headers=headers,
                    json=project_data
                )

                if response.status_code in [200, 201]:
                    break
                elif i < len(auth_formats) - 1:
                    self.console.print(f"[yellow]  Trying alternative auth format ({i+2}/{len(auth_formats)})...[/yellow]")

            if response.status_code not in [200, 201]:
                self.console.print(f"[red]❌ Failed to create project: {response.text}[/red]")
                return

            project = response.json()
            project_id = project['id']

            self.console.print(f"[green]✅ Created project: {project_title} (ID: {project_id})[/green]")

            # Import tasks
            self.console.print(f"\n[cyan]  Importing {len(df_annotated):,} tasks...[/cyan]")

            tasks = []
            for idx, row in df_annotated.iterrows():
                try:
                    annotation_str = row['annotation']
                    annotation_data = json.loads(annotation_str)

                    # Build task data
                    task_data = {'text': str(row[text_column])}

                    # Add metadata
                    for col in df.columns:
                        if col not in [text_column, 'annotation'] and col not in all_label_keys:
                            val = row[col]
                            if pd.notna(val):
                                task_data[col] = str(val)

                    # Build task based on prediction mode
                    if prediction_mode == 'without':
                        # Export without predictions - just data
                        task = {'data': task_data}
                    else:
                        # Build predictions (LLM annotations)
                        predictions_result = []
                        for label_key in all_label_keys:
                            if label_key in annotation_data:
                                label_value = annotation_data[label_key]
                                if isinstance(label_value, list):
                                    for lv in label_value:
                                        if lv and str(lv).strip():
                                            predictions_result.append({
                                                "value": {"choices": [str(lv)]},
                                                "from_name": label_key,
                                                "to_name": "text",
                                                "type": "choices"
                                            })
                                elif isinstance(label_value, str) and label_value.strip():
                                    predictions_result.append({
                                        "value": {"choices": [label_value]},
                                        "from_name": label_key,
                                        "to_name": "text",
                                        "type": "choices"
                                    })

                        task = {
                            'data': task_data,
                            'predictions': [{
                                'result': predictions_result,
                                'model_version': 'llm_annotation'
                            }]
                        }

                    tasks.append(task)

                except Exception as e:
                    self.console.print(f"[yellow]⚠️  Skipped row {idx}: {e}[/yellow]")
                    continue

            # Import tasks to project
            response = requests.post(
                f'{api_url}/api/projects/{project_id}/import',
                headers=headers,
                json=tasks
            )

            if response.status_code not in [200, 201]:
                self.console.print(f"[red]❌ Failed to import tasks: {response.text}[/red]")
                return

            self.console.print(f"\n[bold green]✅ Successfully exported {len(tasks):,} tasks to Label Studio[/bold green]")
            self.console.print(f"[cyan]🔗 Project URL: {api_url}/projects/{project_id}/[/cyan]\n")

            # If mode is 'both', create second project without predictions
            if prediction_mode == 'both':
                self.console.print(f"\n[cyan]Creating second project without predictions...[/cyan]")

                # Call recursively with 'without' mode
                self._export_to_labelstudio_direct(
                    output_file=output_file,
                    text_column=text_column,
                    prompt_configs=prompt_configs,
                    data_path=data_path,
                    timestamp=timestamp,
                    sample_size='all',  # Use all already sampled data
                    prediction_mode='without',
                    api_url=api_url,
                    api_key=api_key
                )

            self.console.print("[yellow]Next steps:[/yellow]")
            self.console.print("  1. Open Label Studio and navigate to your project(s)")
            self.console.print("  2. Review and correct the LLM predictions (if applicable)")
            self.console.print("  3. Export validated annotations")
            self.console.print("  4. Use LLM Tool to calculate metrics\n")

        except requests.exceptions.ConnectionError:
            self.console.print(f"\n[red]❌ Connection error: Could not connect to {api_url}[/red]")
            self.console.print("[yellow]Make sure Label Studio is running:[/yellow]")
            self.console.print("  label-studio start")
        except Exception as e:
            self.console.print(f"\n[red]❌ Export failed: {e}[/red]")
            self.logger.exception("Label Studio direct export failed")

    def _build_labelstudio_config(self, label_keys, prompt_configs=None):
        """Build Label Studio labeling configuration

        Parameters
        ----------
        label_keys : set
            Set of label keys (e.g., {'theme', 'party'})
        prompt_configs : list, optional
            List of prompt configurations containing the actual values for each key
        """
        config_parts = ['<View>']
        config_parts.append('  <Text name="text" value="$text"/>')

        # Extract actual values from prompt_configs if available
        key_values_map = {}
        if prompt_configs:
            for pc in prompt_configs:
                # Try new format first
                if 'prompt' in pc and 'keys' in pc['prompt']:
                    for key_def in pc['prompt']['keys']:
                        if isinstance(key_def, dict):
                            # New format: {'name': 'theme', 'values': ['env', 'health']}
                            key_name = key_def.get('name')
                            values = key_def.get('values', [])
                            if key_name:
                                key_values_map[key_name] = values

                # Try extracting from prompt_content (old wizard format)
                if 'prompt_content' in pc and not key_values_map:
                    import re
                    prompt_text = pc['prompt_content']

                    # Pattern: - "theme": "value1" si ..., "value2" si ..., "null" si ...
                    # or: - "theme" (can be multiple values): "value1" si ..., "value2" si ...
                    for label_key in label_keys:
                        # Find lines starting with - "key"
                        pattern = rf'- "{label_key}"[^:]*:\s*(.+?)(?=\n-|\n\*\*|\Z)'
                        match = re.search(pattern, prompt_text, re.DOTALL)

                        if match:
                            values_text = match.group(1)
                            # Extract all quoted values except "null"
                            value_pattern = r'"([^"]+)"\s+si'
                            values = re.findall(value_pattern, values_text)
                            # Filter out 'null'
                            values = [v for v in values if v != 'null']
                            if values:
                                key_values_map[label_key] = values

        for label_key in sorted(label_keys):
            config_parts.append(f'  <Choices name="{label_key}" toName="text" choice="single">')

            # Use actual values if available, otherwise placeholder
            if label_key in key_values_map and key_values_map[label_key]:
                for value in key_values_map[label_key]:
                    config_parts.append(f'    <Choice value="{value}"/>')
            else:
                # Fallback to placeholder
                config_parts.append(f'    <Choice value="placeholder_{label_key}"/>')

            config_parts.append('  </Choices>')

        config_parts.append('</View>')
        return '\n'.join(config_parts)

    def _get_common_annotation_options(self, total_rows=None, provider=None, model_name=None):
        """
        Common annotation options workflow used by both CSV and SQL annotation modes.

        Returns a dict with all configuration options:
        - annotation_limit, sample_strategy, num_processes, save_incrementally, batch_size
        - temperature, max_tokens, top_p, top_k
        - save_metadata, export_to_doccano, export_to_labelstudio, etc.
        """
        config = {}

        # ============================================================
        # DATASET SCOPE
        # ============================================================
        self.console.print("\n[bold cyan]📊 Dataset Scope[/bold cyan]")
        self.console.print("[dim]Determine how many rows to annotate from your dataset[/dim]\n")

        if total_rows:
            self.console.print(f"[green]✓ Dataset contains {total_rows:,} rows[/green]\n")

        # Option 1: Annotate all or limited
        self.console.print("[yellow]Option 1:[/yellow] Annotate ALL rows vs LIMIT to specific number")
        self.console.print("  • [cyan]all[/cyan]   - Annotate the entire dataset")
        self.console.print("           [dim]Use this for production annotations[/dim]")
        self.console.print("  • [cyan]limit[/cyan] - Specify exact number of rows to annotate")
        self.console.print("           [dim]Use this for testing or partial annotation[/dim]")

        scope_choice = Prompt.ask(
            "\nAnnotate entire dataset or limit rows?",
            choices=["all", "limit"],
            default="all"
        )

        annotation_limit = None
        use_sample = False
        sample_strategy = "head"

        if scope_choice == "limit":
            # Ask for specific number
            annotation_limit = self._int_prompt_with_validation(
                "How many rows to annotate?",
                default=100,
                min_value=1,
                max_value=total_rows if total_rows else 1000000
            )

            # Option 2: Calculate representative sample
            if total_rows and total_rows > 1000:
                self.console.print("\n[yellow]Option 2:[/yellow] Representative Sample Calculation")
                self.console.print("  Calculate statistically representative sample size (95% confidence interval)")
                self.console.print(f"  [dim]• Current selection: {annotation_limit} rows[/dim]")

                calculate_sample = Confirm.ask("Calculate representative sample size?", default=False)

                if calculate_sample:
                    # Formula: n = (Z² × p × (1-p)) / E²
                    # For 95% CI: Z=1.96, p=0.5 (max variance), E=0.05 (5% margin)
                    import math
                    z = 1.96
                    p = 0.5
                    e = 0.05
                    n_infinite = (z**2 * p * (1-p)) / (e**2)
                    n_adjusted = n_infinite / (1 + ((n_infinite - 1) / total_rows))
                    recommended_sample = int(math.ceil(n_adjusted))

                    self.console.print(f"\n[green]📈 Recommended sample size: {recommended_sample} rows[/green]")
                    self.console.print(f"[dim]   (95% confidence level, 5% margin of error)[/dim]")

                    use_recommended = Confirm.ask(f"Use recommended sample size ({recommended_sample} rows)?", default=True)
                    if use_recommended:
                        annotation_limit = recommended_sample
                        use_sample = True

            # Option 3: Random sampling
            self.console.print("\n[yellow]Option 3:[/yellow] Sampling Strategy")
            self.console.print("  Choose how to select the rows to annotate")
            self.console.print("  • [cyan]head[/cyan]   - Take first N rows (faster, sequential)")
            self.console.print("           [dim]Good for testing, preserves order[/dim]")
            self.console.print("  • [cyan]random[/cyan] - Random sample of N rows (representative)")
            self.console.print("           [dim]Better for statistical validity, unbiased[/dim]")

            sample_strategy = Prompt.ask(
                "\nSampling strategy",
                choices=["head", "random"],
                default="random" if use_sample else "head"
            )

        config['annotation_limit'] = annotation_limit
        config['sample_strategy'] = sample_strategy

        # ============================================================
        # PARALLEL PROCESSING
        # ============================================================
        self.console.print("\n[bold cyan]⚙️  Parallel Processing[/bold cyan]")
        self.console.print("[dim]Configure how many processes run simultaneously[/dim]\n")

        self.console.print("[yellow]Parallel Workers:[/yellow]")
        self.console.print("  Number of simultaneous annotation processes")
        self.console.print("\n  [red]⚠️  IMPORTANT:[/red]")
        self.console.print("  [dim]Most local machines can only handle 1 worker for LLM inference[/dim]")
        self.console.print("  [dim]Parallel processing is mainly useful for API models[/dim]")
        self.console.print("\n  • [cyan]1 worker[/cyan]  - Sequential processing")
        self.console.print("           [dim]Recommended for: Local models (Ollama), first time users, debugging[/dim]")
        self.console.print("  • [cyan]2-4 workers[/cyan] - Moderate parallelism")
        self.console.print("           [dim]Recommended for: API models (OpenAI, Claude) - avoid rate limits[/dim]")
        self.console.print("  • [cyan]4-8 workers[/cyan] - High parallelism")
        self.console.print("           [dim]Recommended for: API models only - requires high rate limits[/dim]")

        num_processes = self._int_prompt_with_validation("Parallel workers", 1, 1, 16)
        config['num_processes'] = num_processes

        # ============================================================
        # INCREMENTAL SAVE
        # ============================================================
        self.console.print("\n[bold cyan]💾 Incremental Save[/bold cyan]")
        self.console.print("[dim]Configure how often results are saved during annotation[/dim]\n")

        self.console.print("[yellow]Enable incremental save?[/yellow]")
        self.console.print("  • [green]Yes[/green] - Save progress regularly during annotation (recommended)")
        self.console.print("           [dim]Protects against crashes, allows resuming, safer for long runs[/dim]")
        self.console.print("  • [red]No[/red]  - Save only at the end")
        self.console.print("           [dim]Faster but risky - you lose everything if process crashes[/dim]")

        save_incrementally = Confirm.ask("\n💿 Enable incremental save?", default=True)
        config['save_incrementally'] = save_incrementally

        # Only ask for batch size if incremental save is enabled
        if save_incrementally:
            self.console.print("\n[yellow]Batch Size:[/yellow]")
            self.console.print("  Number of rows processed between each save")
            self.console.print("  • [cyan]Smaller (1-10)[/cyan]   - Very frequent saves, maximum safety")
            self.console.print("           [dim]Use for: Unstable systems, expensive APIs, testing[/dim]")
            self.console.print("  • [cyan]Medium (10-50)[/cyan]   - Balanced safety and performance")
            self.console.print("           [dim]Use for: Most production cases[/dim]")
            self.console.print("  • [cyan]Larger (50-200)[/cyan]  - Less frequent saves, better performance")
            self.console.print("           [dim]Use for: Stable systems, large datasets, local models[/dim]")

            batch_size = self._int_prompt_with_validation("Batch size", 10, 1, 1000)
        else:
            batch_size = None  # Not used when incremental save is disabled

        config['batch_size'] = batch_size

        # ============================================================
        # MODEL PARAMETERS
        # ============================================================
        self.console.print("\n[bold cyan]🎛️  Model Parameters[/bold cyan]")
        self.console.print("[dim]Configure advanced model generation parameters[/dim]\n")

        # Check if model supports parameter tuning
        model_name_lower = model_name.lower() if model_name else ""
        is_o_series = any(x in model_name_lower for x in ['o1', 'o3', 'o4'])
        supports_params = not is_o_series

        if not supports_params:
            self.console.print(f"[yellow]⚠️  Model '{model_name}' uses fixed parameters (reasoning model)[/yellow]")
            self.console.print("[dim]   Temperature and top_p are automatically set to 1.0[/dim]")
            configure_params = False
        else:
            self.console.print("[yellow]Configure model parameters?[/yellow]")
            self.console.print("  Adjust how the model generates responses")
            self.console.print("  [dim]• Default values work well for most cases[/dim]")
            self.console.print("  [dim]• Advanced users can fine-tune for specific needs[/dim]")
            configure_params = Confirm.ask("\nConfigure model parameters?", default=False)

        # Default values
        temperature = 0.7
        max_tokens = 1000
        top_p = 1.0
        top_k = 40

        if configure_params:
            self.console.print("\n[bold]Parameter Explanations:[/bold]\n")

            # Temperature
            self.console.print("[cyan]🌡️  Temperature (0.0 - 2.0):[/cyan]")
            self.console.print("  Controls randomness in responses")
            self.console.print("  • [green]Low (0.0-0.3)[/green]  - Deterministic, focused, consistent")
            self.console.print("           [dim]Use for: Structured tasks, factual extraction, classification[/dim]")
            self.console.print("  • [yellow]Medium (0.4-0.9)[/yellow] - Balanced creativity and consistency")
            self.console.print("           [dim]Use for: General annotation, most use cases[/dim]")
            self.console.print("  • [red]High (1.0-2.0)[/red]  - Creative, varied, unpredictable")
            self.console.print("           [dim]Use for: Brainstorming, diverse perspectives[/dim]")
            temperature = FloatPrompt.ask("Temperature", default=0.7)

            # Max tokens
            self.console.print("\n[cyan]📏 Max Tokens:[/cyan]")
            self.console.print("  Maximum length of the response")
            self.console.print("  • [green]Short (100-500)[/green]   - Brief responses, simple annotations")
            self.console.print("  • [yellow]Medium (500-2000)[/yellow]  - Standard responses, detailed annotations")
            self.console.print("  • [red]Long (2000+)[/red]     - Extensive responses, complex reasoning")
            self.console.print("  [dim]Note: More tokens = higher API costs[/dim]")
            max_tokens = self._int_prompt_with_validation("Max tokens", 1000, 50, 8000)

            # Top_p (nucleus sampling)
            self.console.print("\n[cyan]🎯 Top P (0.0 - 1.0):[/cyan]")
            self.console.print("  Nucleus sampling - alternative to temperature")
            self.console.print("  • [green]Low (0.1-0.5)[/green]  - Focused on most likely tokens")
            self.console.print("           [dim]More deterministic, safer outputs[/dim]")
            self.console.print("  • [yellow]High (0.9-1.0)[/yellow] - Consider broader token range")
            self.console.print("           [dim]More creative, diverse outputs[/dim]")
            self.console.print("  [dim]Tip: Use either temperature OR top_p, not both aggressively[/dim]")
            top_p = FloatPrompt.ask("Top P", default=1.0)

            # Top_k (only for some models)
            if provider and provider in ['ollama', 'google']:
                self.console.print("\n[cyan]🔢 Top K:[/cyan]")
                self.console.print("  Limits vocabulary to K most likely next tokens")
                self.console.print("  • [green]Small (1-10)[/green]   - Very focused, repetitive")
                self.console.print("  • [yellow]Medium (20-50)[/yellow]  - Balanced diversity")
                self.console.print("  • [red]Large (50+)[/red]    - Maximum diversity")
                top_k = self._int_prompt_with_validation("Top K", 40, 1, 100)

        config['temperature'] = temperature
        config['max_tokens'] = max_tokens
        config['top_p'] = top_p
        config['top_k'] = top_k

        # ============================================================
        # REPRODUCIBILITY METADATA
        # ============================================================
        self.console.print("\n[bold cyan]📋 Reproducibility & Metadata[/bold cyan]")
        self.console.print("[yellow]⚠️  IMPORTANT: Save parameters for two critical purposes:[/yellow]\n")

        self.console.print("  [green]1. Resume Capability[/green]")
        self.console.print("     • Continue this annotation if it stops or crashes")
        self.console.print("     • Annotate additional rows later with same settings")
        self.console.print("     • Access via 'Resume/Relaunch Annotation' workflow\n")

        self.console.print("  [green]2. Scientific Reproducibility[/green]")
        self.console.print("     • Document exact parameters for research papers")
        self.console.print("     • Reproduce identical annotations in the future")
        self.console.print("     • Track model version, prompts, and all settings\n")

        self.console.print("  [red]⚠️  If you choose NO:[/red]")
        self.console.print("     • You CANNOT resume this annotation later")
        self.console.print("     • You CANNOT relaunch with same parameters")
        self.console.print("     • Parameters will be lost forever\n")

        save_metadata = Confirm.ask(
            "[bold yellow]Save annotation parameters to JSON file?[/bold yellow]",
            default=True
        )
        config['save_metadata'] = save_metadata

        # ============================================================
        # VALIDATION TOOL EXPORT OPTION
        # ============================================================
        self.console.print("\n[bold cyan]📤 Validation Tool Export[/bold cyan]")
        self.console.print("[dim]Export annotations to JSONL format for human validation[/dim]\n")

        self.console.print("[yellow]Available validation tools:[/yellow]")
        self.console.print("  • [cyan]Doccano[/cyan] - Simple, lightweight NLP annotation tool")
        self.console.print("  • [cyan]Label Studio[/cyan] - Advanced, feature-rich annotation platform")
        self.console.print("  • Both are open-source and free\n")

        self.console.print("[green]Why validate with external tools?[/green]")
        self.console.print("  • Review and correct LLM annotations")
        self.console.print("  • Calculate inter-annotator agreement")
        self.console.print("  • Export validated data for metrics calculation\n")

        # Initialize export flags
        export_to_doccano = False
        export_to_labelstudio = False
        export_sample_size = None
        labelstudio_direct_export = False
        labelstudio_api_url = None
        labelstudio_api_key = None
        prediction_mode = "with"

        # Step 1: Ask if user wants to export
        export_confirm = Confirm.ask(
            "[bold yellow]Export to validation tool?[/bold yellow]",
            default=False
        )

        if export_confirm:
            # Step 2: Ask which tool to export to
            tool_choice = Prompt.ask(
                "[bold yellow]Which validation tool?[/bold yellow]",
                choices=["doccano", "labelstudio"],
                default="doccano"
            )

            # Set the appropriate export flag
            if tool_choice == "doccano":
                export_to_doccano = True
            else:  # labelstudio
                export_to_labelstudio = True

            # Step 2b: If Label Studio, ask export method
            if export_to_labelstudio:
                self.console.print("\n[yellow]Label Studio export method:[/yellow]")
                self.console.print("  • [cyan]jsonl[/cyan] - Export to JSONL file (manual import)")
                if HAS_REQUESTS:
                    self.console.print("  • [cyan]direct[/cyan] - Direct export to Label Studio via API\n")
                    export_choices = ["jsonl", "direct"]
                else:
                    self.console.print("  • [dim]direct[/dim] - Direct export via API [dim](requires 'requests' library)[/dim]\n")
                    export_choices = ["jsonl"]

                export_method = Prompt.ask(
                    "[bold yellow]Export method[/bold yellow]",
                    choices=export_choices,
                    default="jsonl"
                )

                if export_method == "direct":
                    labelstudio_direct_export = True

                    self.console.print("\n[cyan]Label Studio API Configuration:[/cyan]")
                    labelstudio_api_url = Prompt.ask(
                        "Label Studio URL",
                        default="http://localhost:8080"
                    )

                    labelstudio_api_key = Prompt.ask(
                        "API Key (from Label Studio Account & Settings)"
                    )

            # Step 3: Ask about LLM predictions inclusion
            self.console.print("\n[yellow]Include LLM predictions in export?[/yellow]")
            self.console.print("  • [cyan]with[/cyan] - Include LLM annotations as predictions (for review/correction)")
            self.console.print("  • [cyan]without[/cyan] - Export only data without predictions (for manual annotation)")
            self.console.print("  • [cyan]both[/cyan] - Create two files: one with and one without predictions\n")

            prediction_mode = Prompt.ask(
                "[bold yellow]Prediction mode[/bold yellow]",
                choices=["with", "without", "both"],
                default="with"
            )

            # Step 4: Ask how many sentences to export
            self.console.print("\n[yellow]How many annotated sentences to export?[/yellow]")
            self.console.print("  • [cyan]all[/cyan] - Export all annotated sentences")
            self.console.print("  • [cyan]number[/cyan] - Specify exact number\n")

            sample_choice = Prompt.ask(
                "[bold yellow]Export sample[/bold yellow]",
                choices=["all", "number"],
                default="all"
            )

            if sample_choice == "number":
                export_sample_size = IntPrompt.ask(
                    "Number of items to export",
                    default=100
                )
            else:
                export_sample_size = "all"

        config['export_to_doccano'] = export_to_doccano
        config['export_to_labelstudio'] = export_to_labelstudio
        config['labelstudio_direct_export'] = labelstudio_direct_export
        config['labelstudio_api_url'] = labelstudio_api_url
        config['labelstudio_api_key'] = labelstudio_api_key
        config['prediction_mode'] = prediction_mode
        config['export_sample_size'] = export_sample_size

        return config

    def _database_annotator(self):
        """
        Comprehensive SQL Database Annotator

        Supports multiple database systems with intelligent sampling and flexible output options.
        """
        self.console.print("\n[bold cyan]🗄️  SQL Database Annotator[/bold cyan]\n")

        # ========================================
        # STEP 1/9: Database Connection
        # ========================================
        self.console.print("[bold cyan]Step 1/9: Database Connection[/bold cyan]\n")

        # Database type selection
        db_choices = [
            "PostgreSQL",
            "MySQL",
            "SQLite",
            "Microsoft SQL Server",
            "← Back to main menu"
        ]

        # Display database type choices
        db_table = Table(title="Database Types", box=box.ROUNDED)
        db_table.add_column("#", style="cyan", justify="right", width=4)
        db_table.add_column("Database Type", style="green", width=30)

        for idx, db_choice in enumerate(db_choices, 1):
            db_table.add_row(str(idx), db_choice)

        self.console.print(db_table)

        db_type = Prompt.ask(
            "\n[cyan]Select database type[/cyan]",
            choices=[str(i) for i in range(1, len(db_choices) + 1)],
            default="1"
        )

        db_type_idx = int(db_type) - 1
        if db_type_idx == len(db_choices) - 1:
            return

        db_type_name = db_choices[db_type_idx]

        # Build connection string based on database type
        if db_type_name == "SQLite":
            db_path = Prompt.ask("\n[cyan]Enter SQLite database file path[/cyan]")
            db_path = os.path.expanduser(db_path)
            if not os.path.exists(db_path):
                self.console.print(f"[red]✗ Database file not found: {db_path}[/red]")
                input("\nPress Enter to continue...")
                return
            connection_string = f"sqlite:///{db_path}"
        else:
            # For server-based databases
            host = Prompt.ask("\n[cyan]Database host[/cyan]", default="localhost")
            port_defaults = {
                "PostgreSQL": "5432",
                "MySQL": "3306",
                "Microsoft SQL Server": "1433"
            }
            port = Prompt.ask("[cyan]Port[/cyan]", default=port_defaults.get(db_type_name, "5432"))
            username = Prompt.ask("[cyan]Username[/cyan]", default="postgres" if db_type_name == "PostgreSQL" else "root")

            # Password (hidden input)
            import getpass
            self.console.print("[cyan]Password:[/cyan] ", end="")
            password = getpass.getpass("")

            database = Prompt.ask("[cyan]Database name[/cyan]")

            # Build connection string
            if db_type_name == "PostgreSQL":
                connection_string = f"postgresql://{username}:{password}@{host}:{port}/{database}"
            elif db_type_name == "MySQL":
                connection_string = f"mysql+pymysql://{username}:{password}@{host}:{port}/{database}"
            elif db_type_name == "Microsoft SQL Server":
                connection_string = f"mssql+pyodbc://{username}:{password}@{host}:{port}/{database}?driver=ODBC+Driver+17+for+SQL+Server"

        # Test connection
        self.console.print("\n[cyan]Testing database connection...[/cyan]")
        try:
            from sqlalchemy import create_engine, inspect, text
            engine = create_engine(connection_string)
            with engine.connect() as conn:
                self.console.print("[green]✓ Connection successful![/green]")
        except Exception as e:
            self.console.print(f"[red]✗ Connection failed: {str(e)}[/red]")
            input("\nPress Enter to continue...")
            return

        # ========================================
        # STEP 2/9: Table Selection
        # ========================================
        self.console.print("\n[bold cyan]Step 2/9: Table Selection[/bold cyan]\n")

        try:
            inspector = inspect(engine)
            tables = inspector.get_table_names()

            if not tables:
                self.console.print("[yellow]No tables found in database[/yellow]")
                input("\nPress Enter to continue...")
                return

            # Display tables with row counts
            table_info = Table(title="Available Tables", box=box.ROUNDED)
            table_info.add_column("#", style="cyan", justify="right")
            table_info.add_column("Table Name", style="green")
            table_info.add_column("Rows", style="yellow", justify="right")

            table_row_counts = {}
            for idx, table_name in enumerate(tables, 1):
                try:
                    with engine.connect() as conn:
                        result = conn.execute(text(f"SELECT COUNT(*) FROM {table_name}"))
                        row_count = result.scalar()
                        table_row_counts[table_name] = row_count
                        table_info.add_row(str(idx), table_name, f"{row_count:,}")
                except:
                    table_info.add_row(str(idx), table_name, "N/A")

            self.console.print(table_info)

            table_choice = Prompt.ask(
                "\n[cyan]Select table[/cyan]",
                choices=[str(i) for i in range(1, len(tables) + 1)]
            )

            selected_table = tables[int(table_choice) - 1]
            total_rows = table_row_counts.get(selected_table, 0)

            self.console.print(f"\n[green]✓ Selected table: {selected_table} ({total_rows:,} rows)[/green]")

        except Exception as e:
            self.console.print(f"[red]✗ Error accessing tables: {str(e)}[/red]")
            input("\nPress Enter to continue...")
            return

        # ========================================
        # STEP 3/9: Column Selection (Intelligent Detection)
        # ========================================
        self.console.print("\n[bold cyan]Step 3/9: Column Selection with Intelligent Detection[/bold cyan]\n")

        try:
            columns = inspector.get_columns(selected_table)

            # Load sample data for intelligent detection
            self.console.print("[cyan]Analyzing columns...[/cyan]")
            sample_query = f"SELECT * FROM {selected_table} LIMIT 100"
            df_sample = pd.read_sql(sample_query, engine)

            # Detect text columns intelligently
            text_candidates = []
            id_candidates = []

            for col_info in columns:
                col_name = col_info['name']
                if col_name not in df_sample.columns:
                    continue

                # Check if it's a potential ID column
                col_lower = col_name.lower()
                if any(id_keyword in col_lower for id_keyword in ['id', 'key', 'index', 'number', 'num', 'pk']):
                    # Check if values are unique
                    is_unique = df_sample[col_name].nunique() == len(df_sample[col_name].dropna())
                    if is_unique:
                        id_candidates.append({
                            'name': col_name,
                            'type': str(col_info['type']),
                            'confidence': 'high' if 'id' in col_lower else 'medium'
                        })

                # Check if it's a text column (object/string type)
                if df_sample[col_name].dtype == 'object':
                    # Get non-null samples
                    non_null = df_sample[col_name].dropna()
                    if len(non_null) == 0:
                        continue

                    # Calculate average length
                    avg_length = non_null.astype(str).str.len().mean()
                    sample_value = str(non_null.iloc[0])[:80] if len(non_null) > 0 else ""

                    # Determine confidence based on average length
                    if avg_length > 100:
                        confidence = "high"
                    elif avg_length > 50:
                        confidence = "medium"
                    elif avg_length > 20:
                        confidence = "low"
                    else:
                        continue  # Skip very short text

                    text_candidates.append({
                        'name': col_name,
                        'confidence': confidence,
                        'avg_length': avg_length,
                        'sample': sample_value
                    })

            # Sort candidates
            confidence_order = {"high": 0, "medium": 1, "low": 2}
            text_candidates.sort(key=lambda x: (confidence_order[x['confidence']], -x['avg_length']))
            id_candidates.sort(key=lambda x: (confidence_order.get(x['confidence'], 3)))

            # Display columns with intelligent suggestions
            col_table = Table(title=f"Columns in '{selected_table}'", box=box.ROUNDED)
            col_table.add_column("#", style="cyan", justify="right", width=4)
            col_table.add_column("Column Name", style="green", width=25)
            col_table.add_column("Type", style="yellow", width=20)
            col_table.add_column("Detection", style="magenta", width=30)

            detected_text_col = None
            detected_id_col = None

            for idx, col in enumerate(columns, 1):
                col_name = col['name']
                col_type = str(col['type'])
                detection = ""

                # Check if it's a suggested text column
                text_match = next((tc for tc in text_candidates if tc['name'] == col_name), None)
                if text_match:
                    if text_match['confidence'] == 'high':
                        detection = "📝 Text (High confidence)"
                        if detected_text_col is None:
                            detected_text_col = idx
                    elif text_match['confidence'] == 'medium':
                        detection = "📝 Text (Medium)"
                    else:
                        detection = "📝 Text (Low)"

                # Check if it's a suggested ID column
                id_match = next((ic for ic in id_candidates if ic['name'] == col_name), None)
                if id_match:
                    if id_match['confidence'] == 'high':
                        detection = "🔑 ID (Recommended)"
                        if detected_id_col is None:
                            detected_id_col = idx
                    else:
                        detection = "🔑 ID (Possible)"

                col_table.add_row(str(idx), col_name, col_type, detection)

            self.console.print(col_table)

            # Select text column with intelligent default
            if detected_text_col:
                self.console.print(f"\n[cyan]💡 Suggested text column: '{columns[detected_text_col-1]['name']}' (detected automatically)[/cyan]")

            text_col_choice = Prompt.ask(
                "\n[cyan]Select TEXT column (to annotate)[/cyan]",
                choices=[str(i) for i in range(1, len(columns) + 1)],
                default=str(detected_text_col) if detected_text_col else "1"
            )
            text_column = columns[int(text_col_choice) - 1]['name']

            # Select ID column with intelligent default
            if Confirm.ask("\n[cyan]Do you want to select an ID column?[/cyan]", default=True):
                if detected_id_col:
                    self.console.print(f"\n[cyan]💡 Suggested ID column: '{columns[detected_id_col-1]['name']}' (unique values detected)[/cyan]")

                id_col_choice = Prompt.ask(
                    "\n[cyan]Select ID column[/cyan]",
                    choices=[str(i) for i in range(1, len(columns) + 1)],
                    default=str(detected_id_col) if detected_id_col else "1"
                )
                id_column = columns[int(id_col_choice) - 1]['name']
            else:
                id_column = None

            self.console.print(f"\n[green]✓ Text column: {text_column}[/green]")
            if id_column:
                self.console.print(f"[green]✓ ID column: {id_column}[/green]")

        except Exception as e:
            self.console.print(f"[red]✗ Error accessing columns: {str(e)}[/red]")
            import traceback
            self.console.print(f"[dim]{traceback.format_exc()}[/dim]")
            input("\nPress Enter to continue...")
            return

        # ========================================
        # STEP 4/9: Sampling Strategy
        # ========================================
        self.console.print("\n[bold cyan]Step 4/9: Sampling Strategy[/bold cyan]\n")

        # Calculate representative sample sizes using Cochran's formula
        def calculate_representative_sample(population_size: int, confidence_level: float = 0.95, margin_error: float = 0.05) -> int:
            """
            Calculate representative sample size using Cochran's formula.

            Args:
                population_size: Total population size
                confidence_level: Confidence level (0.90, 0.95, 0.99)
                margin_error: Margin of error (0.03, 0.05, 0.10)

            Returns:
                Required sample size
            """
            import math

            # Z-scores for common confidence levels
            z_scores = {0.90: 1.645, 0.95: 1.96, 0.99: 2.576}
            z = z_scores.get(confidence_level, 1.96)

            # Assume maximum variability (p = 0.5)
            p = 0.5

            # Cochran's formula for infinite population
            n0 = (z ** 2 * p * (1 - p)) / (margin_error ** 2)

            # Adjust for finite population
            n = n0 / (1 + (n0 - 1) / population_size)

            return math.ceil(n)

        # Calculate sample sizes for different confidence levels
        sample_sizes = {
            "95% confidence, ±5% margin": calculate_representative_sample(total_rows, 0.95, 0.05),
            "95% confidence, ±3% margin": calculate_representative_sample(total_rows, 0.95, 0.03),
            "99% confidence, ±5% margin": calculate_representative_sample(total_rows, 0.99, 0.05),
            "99% confidence, ±3% margin": calculate_representative_sample(total_rows, 0.99, 0.03)
        }

        # Display sampling options
        self.console.print(f"[cyan]Total rows in table: {total_rows:,}[/cyan]\n")

        sampling_choices = [
            f"Representative sample - {sample_sizes['95% confidence, ±5% margin']:,} rows (95% confidence, ±5% margin)",
            f"Representative sample - {sample_sizes['95% confidence, ±3% margin']:,} rows (95% confidence, ±3% margin)",
            f"Representative sample - {sample_sizes['99% confidence, ±5% margin']:,} rows (99% confidence, ±5% margin)",
            f"Representative sample - {sample_sizes['99% confidence, ±3% margin']:,} rows (99% confidence, ±3% margin)",
            "Exact number of rows (I'll specify)",
            "Percentage of total rows",
            "All rows (no sampling)"
        ]

        # Display sampling choices table
        sampling_table = Table(title="Sampling Strategy Options", box=box.ROUNDED)
        sampling_table.add_column("#", style="cyan", justify="right", width=4)
        sampling_table.add_column("Strategy", style="green", width=70)

        for idx, choice in enumerate(sampling_choices, 1):
            sampling_table.add_row(str(idx), choice)

        self.console.print(sampling_table)

        sampling_choice = Prompt.ask(
            "\n[cyan]Select sampling strategy[/cyan]",
            choices=[str(i) for i in range(1, len(sampling_choices) + 1)],
            default="1"
        )

        sampling_idx = int(sampling_choice) - 1

        if sampling_idx < 4:
            # Representative sample
            sample_key = list(sample_sizes.keys())[sampling_idx]
            num_rows = sample_sizes[sample_key]
        elif sampling_idx == 4:
            # Exact number
            num_rows = IntPrompt.ask("\n[cyan]Enter exact number of rows to annotate[/cyan]", default=min(1000, total_rows))
        elif sampling_idx == 5:
            # Percentage
            percentage = FloatPrompt.ask("\n[cyan]Enter percentage (0-100)[/cyan]", default=10.0)
            num_rows = int(total_rows * percentage / 100)
        else:
            # All rows
            num_rows = total_rows

        # Cap at total rows
        num_rows = min(num_rows, total_rows)

        # Sampling method
        if num_rows < total_rows:
            sampling_method_choices = [
                "Random sampling (recommended)",
                "First N rows",
                "Last N rows"
            ]

            # Display sampling method choices
            method_table = Table(title="Sampling Method", box=box.ROUNDED)
            method_table.add_column("#", style="cyan", justify="right", width=4)
            method_table.add_column("Method", style="green", width=40)

            for idx, method in enumerate(sampling_method_choices, 1):
                method_table.add_row(str(idx), method)

            self.console.print()
            self.console.print(method_table)

            sampling_method = Prompt.ask(
                "\n[cyan]Select sampling method[/cyan]",
                choices=[str(i) for i in range(1, len(sampling_method_choices) + 1)],
                default="1"
            )

            sampling_method_idx = int(sampling_method) - 1
            sampling_method_name = ["random", "first", "last"][sampling_method_idx]
        else:
            sampling_method_name = "all"

        self.console.print(f"\n[green]✓ Will annotate {num_rows:,} rows using '{sampling_method_name}' sampling[/green]")

        # ========================================
        # STEP 5/9: Output Destination
        # ========================================
        self.console.print("\n[bold cyan]Step 5/9: Output Destination[/bold cyan]\n")

        output_choices = [
            f"Write back to same table ('{selected_table}')",
            "Create new table in database",
            "Export to CSV file",
            "Export to JSON file",
            "Export to JSONL file",
            "Export to Excel file",
            "Export to Parquet file",
            "Export to RData file"
        ]

        # Display output destination choices
        output_table = Table(title="Output Destination Options", box=box.ROUNDED)
        output_table.add_column("#", style="cyan", justify="right", width=4)
        output_table.add_column("Destination", style="green", width=50)

        for idx, choice in enumerate(output_choices, 1):
            output_table.add_row(str(idx), choice)

        self.console.print(output_table)

        output_choice = Prompt.ask(
            "\n[cyan]Select output destination[/cyan]",
            choices=[str(i) for i in range(1, len(output_choices) + 1)],
            default="2"
        )

        output_idx = int(output_choice) - 1

        if output_idx == 0:
            # Same table - need annotation column name
            annotation_column = Prompt.ask(
                "\n[cyan]Enter name for annotation column[/cyan]",
                default="llm_annotation"
            )
            output_dest = {
                'type': 'same_table',
                'table': selected_table,
                'column': annotation_column
            }
        elif output_idx == 1:
            # New table
            new_table_name = Prompt.ask(
                "\n[cyan]Enter new table name[/cyan]",
                default=f"{selected_table}_annotated"
            )
            output_dest = {
                'type': 'new_table',
                'table': new_table_name
            }
        else:
            # File export
            file_extensions = {
                2: 'csv',
                3: 'json',
                4: 'jsonl',
                5: 'xlsx',
                6: 'parquet',
                7: 'rdata'
            }

            ext = file_extensions[output_idx]
            default_filename = f"{selected_table}_annotated.{ext}"

            output_file = Prompt.ask(
                f"\n[cyan]Enter output file path[/cyan]",
                default=default_filename
            )

            output_dest = {
                'type': 'file',
                'path': output_file,
                'format': ext
            }

        # ========================================
        # STEP 6/9: LLM Selection
        # ========================================
        self.console.print("\n[bold cyan]Step 6/9: LLM Selection[/bold cyan]\n")

        # Reuse existing LLM selection logic
        llm_config = self._select_llm_interactive()
        if llm_config is None:
            self.console.print("[yellow]LLM selection cancelled[/yellow]")
            input("\nPress Enter to continue...")
            return

        # ========================================
        # STEP 7/9: Prompt Configuration
        # ========================================
        self.console.print("\n[bold cyan]Step 7/9: Prompt Configuration[/bold cyan]\n")

        # Detect prompts from prompts directory (same as other modes)
        detected_prompts = self._detect_prompts_in_folder()

        selected_prompts = []

        if detected_prompts:
            self.console.print("[bold green]✓ Detected prompts in prompts directory:[/bold green]")
            for i, p in enumerate(detected_prompts, 1):
                keys_str = ', '.join(p['keys'][:3]) + ('...' if len(p['keys']) > 3 else '')
                self.console.print(f"  {i}. [cyan]{p['name']}[/cyan]")
                self.console.print(f"     Keys ({len(p['keys'])}): {keys_str}")

            # Prompt selection options
            self.console.print("\n[bold]Prompt Selection Options:[/bold]")
            self.console.print("  [cyan]all[/cyan]     - Use ALL detected prompts")
            self.console.print("  [cyan]select[/cyan]  - Choose SPECIFIC prompts by number")
            self.console.print("  [cyan]wizard[/cyan]  - 🧙‍♂️ Create NEW prompt using Social Science Wizard")
            self.console.print("  [cyan]custom[/cyan]  - Provide path to a prompt file")

            prompt_choice = Prompt.ask(
                "\n[bold yellow]Prompt selection[/bold yellow]",
                choices=["all", "select", "wizard", "custom"],
                default="all"
            )

            if prompt_choice == "all":
                selected_prompts = detected_prompts
                self.console.print(f"[green]✓ Using all {len(selected_prompts)} prompts[/green]")
            elif prompt_choice == "select":
                indices = Prompt.ask("Enter prompt numbers (comma-separated, e.g., 1,3,5)")
                if indices.strip():  # Only process if not empty
                    for idx_str in indices.split(','):
                        idx_str = idx_str.strip()
                        if idx_str:  # Skip empty strings
                            try:
                                idx = int(idx_str) - 1
                                if 0 <= idx < len(detected_prompts):
                                    selected_prompts.append(detected_prompts[idx])
                            except ValueError:
                                self.console.print(f"[yellow]⚠️  Skipping invalid number: '{idx_str}'[/yellow]")
                if not selected_prompts:
                    self.console.print("[yellow]No valid prompts selected. Using all prompts.[/yellow]")
                    selected_prompts = detected_prompts
                else:
                    self.console.print(f"[green]✓ Selected {len(selected_prompts)} prompts[/green]")
            elif prompt_choice == "wizard":
                wizard_prompt = self._run_social_science_wizard()
                from ..annotators.json_cleaner import extract_expected_keys
                keys = extract_expected_keys(wizard_prompt)
                selected_prompts = [{
                    'path': None,
                    'name': 'wizard_generated',
                    'keys': keys,
                    'content': wizard_prompt
                }]
                self.console.print(f"[green]✓ Using wizard-generated prompt with {len(keys)} keys[/green]")
            else:
                custom_path = Path(self._prompt_file_path("Prompt file path (.txt)"))
                content = custom_path.read_text(encoding='utf-8')
                from ..annotators.json_cleaner import extract_expected_keys
                keys = extract_expected_keys(content)
                selected_prompts = [{
                    'path': custom_path,
                    'name': custom_path.stem,
                    'keys': keys,
                    'content': content
                }]
        else:
            self.console.print("[yellow]No prompts found in prompts/ folder[/yellow]")

            # Offer wizard or custom path
            self.console.print("\n[bold]Prompt Options:[/bold]")
            self.console.print("  [cyan]wizard[/cyan] - 🧙‍♂️ Create prompt using Social Science Wizard (Recommended)")
            self.console.print("  [cyan]custom[/cyan] - Provide path to existing prompt file")

            choice = Prompt.ask(
                "\n[bold yellow]Select option[/bold yellow]",
                choices=["wizard", "custom"],
                default="wizard"
            )

            if choice == "wizard":
                wizard_prompt = self._run_social_science_wizard()
                from ..annotators.json_cleaner import extract_expected_keys
                keys = extract_expected_keys(wizard_prompt)
                selected_prompts = [{
                    'path': None,
                    'name': 'wizard_generated',
                    'keys': keys,
                    'content': wizard_prompt
                }]
                self.console.print(f"[green]✓ Using wizard-generated prompt with {len(keys)} keys[/green]")
            else:
                custom_path = Path(self._prompt_file_path("Prompt file path (.txt)"))
                content = custom_path.read_text(encoding='utf-8')
                from ..annotators.json_cleaner import extract_expected_keys
                keys = extract_expected_keys(content)
                selected_prompts = [{
                    'path': custom_path,
                    'name': custom_path.stem,
                    'keys': keys,
                    'content': content
                }]

        if not selected_prompts:
            self.console.print("[red]✗ No prompts selected[/red]")
            input("\nPress Enter to continue...")
            return

        # Build prompt configs (same format as other modes)
        prompt_configs = []
        if len(selected_prompts) > 1:
            self.console.print("\n[bold]Multi-Prompt Mode:[/bold] Configure key prefixes")
            self.console.print("[dim]Prefixes help identify which prompt generated which keys[/dim]\n")

            for i, p in enumerate(selected_prompts, 1):
                default_prefix = p['name'].replace('_', '').replace('-', '')[:10]
                prefix = Prompt.ask(
                    f"  Prefix for '{p['name']}'",
                    default=default_prefix
                )
                prompt_configs.append({
                    'prompt': p,
                    'prefix': prefix
                })
        else:
            prompt_configs = [{'prompt': selected_prompts[0], 'prefix': ''}]

        # ========================================
        # STEP 8/9: Common Annotation Options
        # ========================================
        # Use the shared function for all advanced options (same as CSV annotation)
        common_options = self._get_common_annotation_options(
            total_rows=total_rows,
            provider=llm_config.provider,
            model_name=llm_config.name
        )

        # Extract values from common options
        annotation_limit = common_options.get('annotation_limit')
        sample_strategy = common_options.get('sample_strategy', 'head')
        num_processes = common_options.get('num_processes', 1)
        save_incrementally = common_options.get('save_incrementally', True)
        batch_size = common_options.get('batch_size', 10)
        temperature = common_options.get('temperature', 0.7)
        max_tokens = common_options.get('max_tokens', 1000)
        top_p = common_options.get('top_p', 1.0)
        top_k = common_options.get('top_k', 40)
        save_metadata = common_options.get('save_metadata', True)
        export_to_doccano = common_options.get('export_to_doccano', False)
        export_to_labelstudio = common_options.get('export_to_labelstudio', False)
        labelstudio_direct_export = common_options.get('labelstudio_direct_export', False)
        labelstudio_api_url = common_options.get('labelstudio_api_url')
        labelstudio_api_key = common_options.get('labelstudio_api_key')
        prediction_mode = common_options.get('prediction_mode', 'with')
        export_sample_size = common_options.get('export_sample_size', 'all')

        # Apply annotation_limit to num_rows if specified
        if annotation_limit:
            num_rows = annotation_limit
            # Update sampling_method_name based on sample_strategy
            if sample_strategy == 'random':
                sampling_method_name = 'random'
            else:
                sampling_method_name = 'first'

        # Max retries parameter (not in common options, specific to annotation)
        max_retries = IntPrompt.ask(
            "[cyan]Max retries on failure[/cyan]",
            default=3
        )

        # ========================================
        # STEP 9/9: Confirmation and Execution
        # ========================================
        self.console.print("\n[bold cyan]Step 9/9: Review and Execute[/bold cyan]\n")

        # Summary table
        summary = Table(title="Annotation Configuration Summary", box=box.ROUNDED)
        summary.add_column("Parameter", style="cyan")
        summary.add_column("Value", style="green")

        summary.add_row("Database Type", db_type_name)
        summary.add_row("Table", selected_table)
        summary.add_row("Text Column", text_column)
        if id_column:
            summary.add_row("ID Column", id_column)
        summary.add_row("Total Rows", f"{total_rows:,}")
        summary.add_row("Rows to Annotate", f"{num_rows:,}")
        summary.add_row("Sampling Method", sampling_method_name)

        if output_dest['type'] == 'same_table':
            summary.add_row("Output", f"Same table, column '{output_dest['column']}'")
        elif output_dest['type'] == 'new_table':
            summary.add_row("Output", f"New table '{output_dest['table']}'")
        else:
            summary.add_row("Output", f"File: {output_dest['path']}")

        summary.add_row("LLM Provider", llm_config.provider if llm_config else 'N/A')
        summary.add_row("LLM Model", llm_config.name if llm_config else 'N/A')
        summary.add_row("Prompts", f"{len(prompt_configs)} configured")

        # Add all model parameters
        summary.add_row("Temperature", str(temperature))
        summary.add_row("Max Tokens", str(max_tokens))
        summary.add_row("Top P", str(top_p))
        if llm_config and llm_config.provider in ['ollama', 'google']:
            summary.add_row("Top K", str(top_k))

        # Add processing parameters
        summary.add_row("Parallel Workers", str(num_processes))
        summary.add_row("Batch Size", str(batch_size))
        summary.add_row("Incremental Save", "Yes" if save_incrementally else "No")
        summary.add_row("Max Retries", str(max_retries))

        # Add export options if configured
        if export_to_doccano or export_to_labelstudio:
            export_tool = "Doccano" if export_to_doccano else "Label Studio"
            summary.add_row("Export Tool", export_tool)
            summary.add_row("Prediction Mode", prediction_mode)
            if export_sample_size != "all":
                summary.add_row("Export Sample", str(export_sample_size))

        self.console.print(summary)

        if not Confirm.ask("\n[cyan]Start annotation?[/cyan]", default=True):
            self.console.print("[yellow]Annotation cancelled[/yellow]")
            input("\nPress Enter to continue...")
            return

        # ========================================
        # EXECUTION
        # ========================================
        self.console.print("\n[bold green]Starting annotation...[/bold green]\n")

        try:
            # Load data with sampling
            self.console.print("[cyan]Loading data from database...[/cyan]")

            if sampling_method_name == "all":
                query = f"SELECT * FROM {selected_table}"
            elif sampling_method_name == "random":
                if db_type_name == "PostgreSQL":
                    query = f"SELECT * FROM {selected_table} ORDER BY RANDOM() LIMIT {num_rows}"
                elif db_type_name == "MySQL":
                    query = f"SELECT * FROM {selected_table} ORDER BY RAND() LIMIT {num_rows}"
                elif db_type_name == "SQLite":
                    query = f"SELECT * FROM {selected_table} ORDER BY RANDOM() LIMIT {num_rows}"
                else:
                    query = f"SELECT TOP {num_rows} * FROM {selected_table} ORDER BY NEWID()"
            elif sampling_method_name == "first":
                query = f"SELECT * FROM {selected_table} LIMIT {num_rows}"
            else:  # last
                if db_type_name in ["PostgreSQL", "MySQL", "SQLite"]:
                    query = f"SELECT * FROM (SELECT * FROM {selected_table} ORDER BY {id_column or '1'} DESC LIMIT {num_rows}) sub ORDER BY {id_column or '1'} ASC"
                else:
                    query = f"SELECT TOP {num_rows} * FROM {selected_table} ORDER BY {id_column or '1'} DESC"

            df = pd.read_sql(query, engine)
            self.console.print(f"[green]✓ Loaded {len(df):,} rows[/green]\n")

            # Save DataFrame to file in data/annotations directory (same as Smart Annotate)
            annotations_dir = self.settings.paths.data_dir / 'annotations'
            annotations_dir.mkdir(parents=True, exist_ok=True)

            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            safe_model_name = llm_config.name.replace(':', '_').replace('/', '_')

            # Input file for pipeline
            temp_input_filename = f"db_{selected_table}_{timestamp}.csv"
            temp_input_file = annotations_dir / temp_input_filename
            df.to_csv(temp_input_file, index=False)
            self.console.print(f"[dim]Saved {len(df)} rows to: {temp_input_file}[/dim]\n")

            # Output file for annotations (will be created by pipeline)
            output_filename = f"db_{selected_table}_{safe_model_name}_annotations_{timestamp}.csv"
            pipeline_output_file = annotations_dir / output_filename

            # Build prompts payload (same format as Smart Annotate)
            prompts_payload = []
            for pc in prompt_configs:
                prompt_dict = pc['prompt']
                prefix = pc['prefix']
                prompts_payload.append({
                    'prompt': prompt_dict['content'],
                    'expected_keys': prompt_dict['keys'],
                    'prefix': prefix
                })

            # Get API key if needed
            api_key = None
            if llm_config.provider in ['openai', 'anthropic', 'google']:
                api_key = self._get_api_key(llm_config.provider)
                if not api_key:
                    self.console.print(f"[red]API key required for {llm_config.provider}[/red]")
                    return

            # Build pipeline config (SAME as Smart Annotate)
            pipeline_config = {
                'mode': 'file',
                'data_source': 'csv',
                'data_format': 'csv',
                'file_path': str(temp_input_file),
                'text_column': text_column,
                'text_columns': [text_column],
                'annotation_column': 'annotation',
                'identifier_column': id_column if id_column else 'annotation_id',
                'run_annotation': True,
                'annotation_mode': 'local' if llm_config.provider == 'ollama' else 'api',
                'annotation_provider': llm_config.provider,
                'annotation_model': llm_config.name,
                'api_key': api_key,
                'prompts': prompts_payload,
                'annotation_sample_size': annotation_limit,
                'annotation_sampling_strategy': sample_strategy if annotation_limit else 'head',
                'annotation_sample_seed': 42,
                'max_tokens': max_tokens,
                'temperature': temperature,
                'top_p': top_p,
                'top_k': top_k if llm_config.provider in ['ollama', 'google'] else None,
                'max_workers': num_processes,
                'num_processes': num_processes,
                'use_parallel': num_processes > 1,
                'warmup': False,
                'disable_tqdm': False,  # Enable tqdm for same display as Smart Annotate
                'output_format': 'csv',
                'output_path': str(pipeline_output_file),
                'save_incrementally': save_incrementally,
                'batch_size': batch_size,
                'run_validation': False,
                'run_training': False,
            }

            # Add model-specific options (same as Smart Annotate)
            if llm_config.provider == 'ollama':
                options = {
                    'temperature': temperature,
                    'num_predict': max_tokens,
                    'top_p': top_p,
                    'top_k': top_k
                }
                pipeline_config['options'] = options

            # Execute annotation using SAME pipeline as Smart Annotate
            try:
                self.console.print("[bold green]🚀 Starting annotation...[/bold green]\n")

                from ..pipelines.pipeline_controller import PipelineController
                from ..utils.rich_progress_manager import RichProgressManager
                from ..pipelines.enhanced_pipeline_wrapper import EnhancedPipelineWrapper

                pipeline_with_progress = PipelineController(settings=self.settings)

                with RichProgressManager(
                    show_json_every=1,
                    compact_mode=False
                ) as progress_manager:
                    enhanced_pipeline = EnhancedPipelineWrapper(
                        pipeline_with_progress,
                        progress_manager
                    )

                    state = enhanced_pipeline.run_pipeline(pipeline_config)

                # Check if annotation was successful
                if not state or not state.annotation_results:
                    self.console.print("\n[red]✗ Annotation failed or returned no results[/red]")
                    input("\nPress Enter to continue...")
                    return

                # Load annotated data
                if not pipeline_output_file.exists():
                    self.console.print(f"\n[red]✗ Output file not found: {pipeline_output_file}[/red]")
                    input("\nPress Enter to continue...")
                    return

                df_annotated = pd.read_csv(pipeline_output_file)
                self.console.print(f"\n[green]✓ Annotation complete: {len(df_annotated)} rows annotated[/green]")

            except Exception as e:
                self.console.print(f"\n[red]✗ Error during annotation: {str(e)}[/red]")
                import traceback
                self.console.print(f"[dim]{traceback.format_exc()}[/dim]")
                input("\nPress Enter to continue...")
                return

            # ========================================
            # Save results to final destination
            # ========================================
            self.console.print("\n[bold cyan]📁 Saving to final destination...[/bold cyan]\n")

            if output_dest['type'] == 'same_table':
                # Update same table in database
                from sqlalchemy import Table as SQLTable, MetaData, Column, JSON

                metadata_obj = MetaData()
                metadata_obj.reflect(bind=engine)
                table = metadata_obj.tables[selected_table]

                # Add annotation column if doesn't exist
                annotation_col_name = output_dest['column']
                if annotation_col_name not in [c.name for c in table.columns]:
                    with engine.connect() as conn:
                        if db_type_name == "PostgreSQL":
                            conn.execute(text(f"ALTER TABLE {selected_table} ADD COLUMN {annotation_col_name} JSONB"))
                        else:
                            conn.execute(text(f"ALTER TABLE {selected_table} ADD COLUMN {annotation_col_name} TEXT"))
                        conn.commit()
                    self.console.print(f"[green]✓ Added column '{annotation_col_name}' to table[/green]")

                # Update rows with annotations
                with engine.connect() as conn:
                    updated_count = 0
                    for idx, row in df_annotated.iterrows():
                        if id_column and id_column in row:
                            id_value = row[id_column]
                            annotation_value = row.get('annotation', '')
                            update_query = text(f"UPDATE {selected_table} SET {annotation_col_name} = :annotation WHERE {id_column} = :id_val")
                            conn.execute(update_query, {'annotation': str(annotation_value), 'id_val': id_value})
                            updated_count += 1
                        else:
                            self.console.print("[yellow]⚠️  Warning: No ID column specified, cannot update specific rows[/yellow]")
                            break
                    conn.commit()

                self.console.print(f"[green]✓ Updated {updated_count:,} rows in table '{selected_table}'[/green]")
                self.console.print(f"[dim]Annotations saved in data/annotations: {pipeline_output_file.name}[/dim]")

                # Clean up temporary input file
                if temp_input_file.exists():
                    temp_input_file.unlink()

            elif output_dest['type'] == 'new_table':
                # Create new table in database
                df_annotated.to_sql(
                    output_dest['table'],
                    engine,
                    if_exists='replace',
                    index=False
                )
                self.console.print(f"[green]✓ Created new table '{output_dest['table']}' with {len(df_annotated):,} rows[/green]")
                self.console.print(f"[dim]Annotations also saved in data/annotations: {pipeline_output_file.name}[/dim]")

                # Clean up temporary input file
                if temp_input_file.exists():
                    temp_input_file.unlink()

            else:
                # Export to file - either copy to user path or keep in data/annotations
                output_path = Path(output_dest['path'])
                format_type = output_dest['format']

                # If user specified a custom path, copy/convert there
                if str(output_path) != str(pipeline_output_file):
                    if format_type == 'csv':
                        # Already CSV, just copy
                        import shutil
                        shutil.copy(pipeline_output_file, output_path)
                    elif format_type == 'json':
                        df_annotated.to_json(output_path, orient='records', indent=2)
                    elif format_type == 'jsonl':
                        df_annotated.to_json(output_path, orient='records', lines=True)
                    elif format_type == 'xlsx':
                        df_annotated.to_excel(output_path, index=False)
                    elif format_type == 'parquet':
                        df_annotated.to_parquet(output_path, index=False)
                    elif format_type == 'rdata':
                        import pyreadr
                        pyreadr.write_rdata(str(output_path), df_annotated, df_name='annotated_data')

                    self.console.print(f"[green]✓ Exported {len(df_annotated):,} rows to: {output_path}[/green]")
                    self.console.print(f"[dim]Annotations also saved in data/annotations: {pipeline_output_file.name}[/dim]")
                else:
                    # Using default location in data/annotations
                    self.console.print(f"[green]✓ Annotations saved: {pipeline_output_file}[/green]")

                # Clean up temporary input file
                if temp_input_file.exists():
                    temp_input_file.unlink()

            self.console.print("\n[bold green]✅ Annotation completed successfully![/bold green]")

            # ============================================================
            # INTELLIGENT TRAINING WORKFLOW (Post-Annotation)
            # ============================================================
            self._post_annotation_training_workflow(
                output_file=str(pipeline_output_file),
                text_column=text_column,
                prompt_configs=prompt_configs
            )

            # Export to Doccano JSONL if requested
            if export_to_doccano:
                self._export_to_doccano_jsonl(
                    output_file=str(pipeline_output_file),
                    text_column=text_column,
                    prompt_configs=prompt_configs,
                    data_path=None,
                    timestamp=timestamp,
                    sample_size=export_sample_size
                )

            # Export to Label Studio if requested
            if export_to_labelstudio:
                if labelstudio_direct_export:
                    # Direct export to Label Studio via API
                    self._export_to_labelstudio_direct(
                        output_file=str(pipeline_output_file),
                        text_column=text_column,
                        prompt_configs=prompt_configs,
                        data_path=None,
                        timestamp=timestamp,
                        sample_size=export_sample_size,
                        prediction_mode=prediction_mode,
                        api_url=labelstudio_api_url,
                        api_key=labelstudio_api_key
                    )
                else:
                    # Export to JSONL file
                    self._export_to_labelstudio_jsonl(
                        output_file=str(pipeline_output_file),
                        text_column=text_column,
                        prompt_configs=prompt_configs,
                        data_path=None,
                        timestamp=timestamp,
                        sample_size=export_sample_size,
                        prediction_mode=prediction_mode
                    )

        except Exception as e:
            self.console.print(f"\n[bold red]✗ Error during annotation: {str(e)}[/bold red]")
            import traceback
            self.console.print(f"[dim]{traceback.format_exc()}[/dim]")

        input("\nPress Enter to continue...")


    def show_documentation(self):
        """Show documentation"""
        # Display ASCII logo only
        self._display_ascii_logo()

        if HAS_RICH and self.console:
            doc_text = """
# LLMTool Documentation

## Quick Start
1. Ensure you have models available (Ollama or API keys)
2. Prepare your dataset (CSV, JSON, etc.)
3. Use Quick Start Wizard for automatic configuration

## Features
- **Auto-detection**: Automatically finds models and datasets
- **Smart defaults**: Intelligent configuration suggestions
- **Profile system**: Save and reuse configurations
- **Benchmarking**: Compare multiple models automatically

## Support
- GitHub: https://github.com/antoine-lemor/LLMTool
- Email: support@llmtool.ai
            """

            md = Markdown(doc_text)
            self.console.print(Panel(md, title="📚 Documentation", border_style="blue"))
        else:
            print("\n=== Documentation ===")
            print("Visit: https://github.com/antoine-lemor/LLMTool")


def main():
    """Entry point for the advanced CLI"""
    cli = AdvancedCLI()
    cli.run()


if __name__ == "__main__":
    main()
